

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Mzzf">
  <meta name="keywords" content="">
  
    <meta name="description" content="引言：Kubernetes 网络的核心挑战与解决方案Kubernetes 极大地简化了容器化应用的部署和管理，但在网络层面，它引入了新的挑战。Pods 是短暂的，其 IP 地址是动态分配的，这使得直接访问 Pod 变得不可靠。为了向集群内部和外部的客户端提供稳定、可靠的服务访问入口，并有效地将流量分发到健康的后端 Pod 实例，Kubernetes 提供了一系列网络抽象和机制。 本文将深入探讨 K">
<meta property="og:type" content="article">
<meta property="og:title" content="Kubernetes 网络服务暴露：从 Service 到 Ingress 与 BGP&#x2F;DSR">
<meta property="og:url" content="https://mfzzf.github.io/2025/04/22/Ingress/index.html">
<meta property="og:site_name" content="Mzzf&#39;s Blog">
<meta property="og:description" content="引言：Kubernetes 网络的核心挑战与解决方案Kubernetes 极大地简化了容器化应用的部署和管理，但在网络层面，它引入了新的挑战。Pods 是短暂的，其 IP 地址是动态分配的，这使得直接访问 Pod 变得不可靠。为了向集群内部和外部的客户端提供稳定、可靠的服务访问入口，并有效地将流量分发到健康的后端 Pod 实例，Kubernetes 提供了一系列网络抽象和机制。 本文将深入探讨 K">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/22/Ingress/image-20250423165931807.png">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/22/Ingress/image-20250423161359858.png">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/22/Ingress/image-20250423172643606.png">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/22/Ingress/image-20250423190233621.png">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/22/Ingress/image-20250423190256226.png">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/22/Ingress/image-20250423201824385.png">
<meta property="article:published_time" content="2025-04-22T14:19:47.000Z">
<meta property="article:modified_time" content="2025-04-27T09:41:59.691Z">
<meta property="article:author" content="Mzzf">
<meta property="article:tag" content="kubernetes">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://mfzzf.github.io/2025/04/22/Ingress/image-20250423165931807.png">
  
  
  
  <title>Kubernetes 网络服务暴露：从 Service 到 Ingress 与 BGP/DSR - Mzzf&#39;s Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"mfzzf.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Mzzf&#39;s blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="Kubernetes 网络服务暴露：从 Service 到 Ingress 与 BGP/DSR"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-04-22 22:19" pubdate>
          2025年4月22日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          10k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          84 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">Kubernetes 网络服务暴露：从 Service 到 Ingress 与 BGP/DSR</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="引言：Kubernetes-网络的核心挑战与解决方案"><a href="#引言：Kubernetes-网络的核心挑战与解决方案" class="headerlink" title="引言：Kubernetes 网络的核心挑战与解决方案"></a>引言：Kubernetes 网络的核心挑战与解决方案</h1><p>Kubernetes 极大地简化了容器化应用的部署和管理，但在网络层面，它引入了新的挑战。Pods 是短暂的，其 IP 地址是动态分配的，这使得直接访问 Pod 变得不可靠。为了向集群内部和外部的客户端提供稳定、可靠的服务访问入口，并有效地将流量分发到健康的后端 Pod 实例，Kubernetes 提供了一系列网络抽象和机制。</p>
<p>本文将深入探讨 Kubernetes 中实现网络负载均衡和服务暴露的关键技术，从基础的 L4 Service 到更高级的 L7 Ingress，再到适用于特定场景（如裸金属环境或高性能需求）的 BGP 和 DSR 技术。我们将剖析它们的工作原理、解决的问题以及如何在实际场景中应用。</p>
<h1 id="Kubernetes-网络负载均衡基础"><a href="#Kubernetes-网络负载均衡基础" class="headerlink" title="Kubernetes 网络负载均衡基础"></a>Kubernetes 网络负载均衡基础</h1><h2 id="1-为何需要负载均衡？"><a href="#1-为何需要负载均衡？" class="headerlink" title="1. 为何需要负载均衡？"></a>1. 为何需要负载均衡？</h2><ul>
<li><strong>Pod 的动态性</strong>: Pod IP 地址是动态分配的，且 Pod 实例可能随时被创建或销毁。直接依赖 Pod IP 进行访问是不可靠的。</li>
<li><strong>服务发现与稳定性</strong>: 需要一个稳定的访问点（IP 地址和端口），客户端可以通过这个固定入口访问服务，而无需关心后端 Pod 的具体变化。</li>
<li><strong>流量分发</strong>: 需要将传入的请求有效地分发到多个健康的后端 Pod 实例上，以实现负载均衡和高可用。</li>
<li><strong>自动化</strong>: 后端 Pod 实例的变化（增减、健康状态）需要被自动感知，并实时更新路由规则。</li>
</ul>
<h2 id="2-L4-负载均衡-Service"><a href="#2-L4-负载均衡-Service" class="headerlink" title="2. L4 负载均衡: Service"></a>2. L4 负载均衡: Service</h2><p><strong>核心对象</strong>: <code>Service</code> (Kubernetes API Object)</p>
<p><strong>作用</strong>: Service 是 Kubernetes 中实现 L4 (TCP&#x2F;UDP) 负载均衡和集群内部服务发现的基础。它为一组功能相同的 Pod（通过标签选择器 <code>selector</code> 关联）提供了一个统一、稳定的虚拟 IP 地址（<code>ClusterIP</code>）和端口。客户端（集群内的其他 Pod 或进程）只需访问 Service 的 <code>ClusterIP:Port</code>，流量就会被自动转发到后端某个健康的 Pod 上。</p>
<p><strong>实现者</strong>: <code>kube-proxy</code></p>
<p><code>kube-proxy</code> 是一个运行在 Kubernetes 集群中每个 Node 上的网络代理和负载均衡器。它并不是用户流量的直接代理，而是通过修改节点上的网络规则（iptables 或 IPVS）来实现 Service 的虚拟 IP 转发逻辑。</p>
<ul>
<li><strong>工作模式</strong>:<ol>
<li><strong>监视 API Server</strong>: <code>kube-proxy</code> 持续监听（Watch）API Server 上 <code>Service</code> 和 <code>EndpointSlice</code>（或旧版的 <code>Endpoints</code>）资源的变化。<code>EndpointSlice</code> 包含了 Service 关联的所有健康 Pod 的实际 IP 地址和端口。</li>
<li><strong>更新网络规则</strong>: 当检测到变化时，<code>kube-proxy</code> 会根据其配置的模式（iptables 或 IPVS）在当前节点上更新相应的网络规则。</li>
</ol>
</li>
</ul>
<h3 id="kube-proxy-模式详解"><a href="#kube-proxy-模式详解" class="headerlink" title="kube-proxy 模式详解"></a><code>kube-proxy</code> 模式详解</h3><ol>
<li><p><strong><code>iptables</code> 模式 (曾经的默认，现在 IPVS 更常见)</strong></p>
<ul>
<li><strong>原理</strong>: 利用 Linux 内核的 <code>netfilter</code> 框架和 <code>iptables</code> 工具。为每个 Service 创建一系列 <code>iptables</code> 规则。当数据包到达节点，目标是 Service 的 <code>ClusterIP:Port</code> 时，<code>iptables</code> 规则（通常在 <code>nat</code> 表的 <code>PREROUTING</code> 或 <code>OUTPUT</code> 链触发，跳转到 <code>KUBE-SERVICES</code> 链，再到具体的 <code>KUBE-SVC-*</code> 链，最后到 <code>KUBE-SEP-*</code> 链）会执行 <strong>DNAT (Destination Network Address Translation)</strong>，将数据包的目标 IP 和端口修改为选中的后端 Pod 的 IP 和端口。同时，<code>iptables</code> 也会进行连接跟踪 (<code>conntrack</code>) 来确保同一连接的后续包被转发到同一个 Pod。负载均衡策略通常是<strong>随机选择</strong>一个后端 Pod。</li>
<li><strong>优点</strong>: 成熟稳定，几乎所有 Linux 发行版都支持。</li>
<li><strong>缺点</strong>:<ul>
<li><strong>性能</strong>: 当 Service 和 Endpoints 数量巨大时，<code>iptables</code> 规则链会变得非常长，内核需要线性查找匹配规则，导致性能下降，尤其是在高并发连接场景下。</li>
<li><strong>规则更新</strong>: 更新大量规则时可能引入短暂的延迟或性能抖动。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong><code>IPVS</code> (IP Virtual Server) 模式 (当前推荐和许多发行版的默认)</strong></p>
<ul>
<li><strong>原理</strong>: 利用 Linux 内核内置的高性能 L4 负载均衡模块 <code>IPVS</code>。<code>kube-proxy</code> 会为每个 Service 创建一个 IPVS 虚拟服务器 (Virtual Server, VS)，并将后端 Pod IP:Port 配置为真实服务器 (Real Server, RS)。IPVS 使用<strong>高效的哈希表</strong>来存储和查找服务规则，而不是线性规则链。当数据包到达时，IPVS 直接在内核中进行高效的查找和转发决策（DNAT）。</li>
<li><strong>优点</strong>:<ul>
<li><strong>高性能</strong>: 哈希查找使得即使在大量 Service 的情况下也能保持接近 O(1) 的查找效率，性能远超 <code>iptables</code> 模式。</li>
<li><strong>更多负载均衡算法</strong>: 支持多种负载均衡算法，如轮询 (rr)、最少连接 (lc)、加权轮询 (wrr)、源哈希 (sh) 等，可以通过 Service 注解进行配置。</li>
<li><strong>连接跟踪优化</strong>: IPVS 的连接同步机制通常更优。</li>
</ul>
</li>
<li><strong>缺点</strong>: 需要节点内核支持 IPVS 模块（现代 Linux 内核通常都支持）。</li>
</ul>
</li>
</ol>
<h3 id="Service-类型"><a href="#Service-类型" class="headerlink" title="Service 类型"></a>Service 类型</h3><p>Kubernetes Service 有多种类型，决定了服务如何被暴露：</p>
<ul>
<li><strong><code>ClusterIP</code></strong>: <strong>默认类型</strong>。为 Service 分配一个集群内部唯一的虚拟 IP 地址。这个 IP 地址<strong>只能在集群内部访问</strong>（Pod 到 Service，或 Node 到 Service）。这是实现集群内部服务间通信的基础。</li>
<li><strong><code>NodePort</code></strong>: 在 <code>ClusterIP</code> 的基础上，额外在<strong>每个集群节点</strong>上暴露一个<strong>静态端口</strong>（范围通常是 30000-32767）。外部客户端可以通过访问 <code>任意节点IP:NodePort</code> 来访问该 Service。流量到达节点后，会被转发给 Service 的 ClusterIP，最终由 <code>kube-proxy</code> (iptables&#x2F;IPVS) 转发给后端 Pod。主要用于开发测试或需要直接暴露 TCP&#x2F;UDP 服务给外部的简单场景。</li>
<li><strong><code>LoadBalancer</code></strong>: <strong>通常用于公有云环境</strong> (AWS, GCP, Azure 等)。在 <code>NodePort</code> 的基础上，请求云平台<strong>自动创建一个外部负载均衡器 (ELB)</strong>。这个 ELB 会获得一个<strong>公网 IP 地址</strong>，并将流量转发到所有集群节点的 <code>NodePort</code> 上。这是将服务标准地暴露给公网用户的方式。云平台的 <code>cloud-controller-manager</code> 组件负责与云 API 交互来创建和管理 ELB。对于裸金属或私有云环境，需要像 <strong>MetalLB</strong> 这样的附加组件来提供此功能。</li>
<li><strong><code>ExternalName</code></strong>: 一个特例，它不是提供负载均衡，而是将 Service 名称映射到<strong>集群外部的一个 DNS 名称</strong>。当查询这个 Service 时，集群 DNS (如 CoreDNS) 会返回配置的外部 DNS 名称的 CNAME 记录。</li>
</ul>
<h2 id="3-网络五元组-Network-5-Tuple"><a href="#3-网络五元组-Network-5-Tuple" class="headerlink" title="3. 网络五元组 (Network 5-Tuple)"></a>3. 网络五元组 (Network 5-Tuple)</h2><p>理解 L4 负载均衡离不开网络基础。网络五元组是唯一标识一个 TCP&#x2F;IP 网络连接的关键信息：</p>
<ol>
<li><strong>源 IP 地址 (Source IP)</strong></li>
<li><strong>源端口号 (Source Port)</strong></li>
<li><strong>目的 IP 地址 (Destination IP)</strong></li>
<li><strong>目的端口号 (Destination Port)</strong></li>
<li><strong>协议号 (Protocol)</strong> (例如，TCP 协议号为 6, UDP 协议号为 17)</li>
</ol>
<p><strong>重要性</strong>:</p>
<ul>
<li><strong>唯一标识连接</strong>: 网络设备（路由器、防火墙、负载均衡器）使用五元组来区分不同的网络会话。</li>
<li><strong>连接跟踪 (<code>conntrack</code>)</strong>: Linux 内核（以及 <code>iptables</code> 和 <code>IPVS</code>）使用五元组来跟踪 TCP&#x2F;UDP 连接的状态（如 NEW, ESTABLISHED, FIN_WAIT），确保同一连接的包被正确处理。</li>
<li><strong>NAT (网络地址转换)</strong>: NAT 设备（包括 <code>kube-proxy</code> 执行 DNAT 时）需要维护五元组的映射关系，以便正确地转换地址和端口，并将返回的流量送回正确的源。</li>
<li><strong>状态防火墙</strong>: 基于连接状态（通过跟踪五元组）进行访问控制。</li>
<li><strong>L4 负载均衡</strong>: 负载均衡器根据五元组（特别是目的 IP&#x2F;Port）将流量分发到后端服务器，并可能基于源 IP&#x2F;Port（如源哈希算法）来维持会话一致性。</li>
</ul>
<h1 id="L7-负载均衡-Ingress"><a href="#L7-负载均衡-Ingress" class="headerlink" title="L7 负载均衡: Ingress"></a>L7 负载均衡: Ingress</h1><p>虽然 Service 提供了基础的 L4 负载均衡，但对于现代 Web 应用和微服务架构中常见的 HTTP&#x2F;HTTPS 流量，它存在一些局限性：</p>
<ul>
<li><strong>端口管理复杂</strong>: 每个需要暴露的 HTTP&#x2F;S 服务如果都用 <code>NodePort</code> 或 <code>LoadBalancer</code> Service，会消耗大量端口或外部 IP，管理困难且成本高。</li>
<li><strong>缺乏应用层路由</strong>: Service 无法理解 HTTP 协议，不能基于域名 (Host header) 或 URL 路径 (Path) 来进行智能路由。</li>
<li><strong>TLS 管理分散</strong>: 如果需要 HTTPS，TLS 证书和加解密逻辑需要在每个后端应用 Pod 内部处理，管理和更新证书非常麻烦。</li>
</ul>
<p>为了解决这些问题，Kubernetes 引入了 <strong>Ingress</strong>。</p>
<p><strong>核心对象</strong>: <code>Ingress</code> (Kubernetes API Object)</p>
<p><strong>作用</strong>: Ingress 资源定义了一系列<strong>规则</strong>，描述了 <strong>HTTP&#x2F;HTTPS</strong> 流量应该如何从集群<strong>外部</strong>路由到集群<strong>内部</strong>的 <code>Service</code>。它充当了集群流量的“智能”入口，提供了更丰富的应用层路由能力。你可以把它想象成一个<strong>虚拟主机</strong>或<strong>反向代理</strong>的配置蓝图。</p>
<p><strong>关键点</strong>: <code>Ingress</code> 资源本身<strong>只是规则的集合，不具备处理流量的能力</strong>。</p>
<p><strong>实现者</strong>: <strong><code>Ingress Controller</code></strong></p>
<p>Ingress Controller 是一个<strong>实际运行在集群中的应用程序</strong>（通常以 Deployment 或 DaemonSet 的形式部署），它负责<strong>读取并实现</strong> <code>Ingress</code> 资源中定义的规则。</p>
<ul>
<li><strong>不是 K8s 内置组件</strong>: 你需要<strong>单独选择并部署</strong>一个 Ingress Controller，常见的有：<ul>
<li><code>ingress-nginx</code> (基于 Nginx，社区维护最广泛)</li>
<li><code>Traefik Ingress Controller</code> (基于 Traefik Proxy)</li>
</ul>
<ul>
<li><code>HAProxy Ingress</code> (基于 HAProxy)</li>
<li>云厂商提供的 Ingress Controller (如 AWS Load Balancer Controller, GKE Ingress Controller)，它们通常能更紧密地集成云平台的 L7 负载均衡器。</li>
<li>其他如 APISIX Ingress, Contour (基于 Envoy) 等。</li>
</ul>
</li>
<li><strong>工作原理 (控制循环 - Control Loop)</strong>:<ol>
<li><strong>Observe (监听)</strong>: Ingress Controller 持续监听（Watch）Kubernetes API Server 上的 <code>Ingress</code>, <code>Service</code>, <code>EndpointSlice</code>, <code>Secret</code> (用于 TLS 证书) 等资源的变化。</li>
<li><strong>Compare (比较)</strong>: 获取集群中定义的 Ingress 规则（<strong>期望状态</strong>），并与自身当前管理的底层反向代理（如 Nginx）的配置（<strong>实际状态</strong>）进行比较。</li>
<li><strong>Act (行动)</strong>: 如果期望状态与实际状态不符（例如，用户创建了一个新的 Ingress 规则），Ingress Controller 会<strong>动态地生成新的代理配置文件</strong>（如 <code>nginx.conf</code>），并<strong>应用这些配置</strong>（例如，通过 <code>nginx -s reload</code> 平滑地重新加载 Nginx 配置）。</li>
</ol>
</li>
<li><strong>流量处理</strong>:<ol>
<li>Ingress Controller 通常通过一个 <code>LoadBalancer</code> 或 <code>NodePort</code> 类型的 Service 将自身暴露给外部网络。</li>
<li>外部 HTTP&#x2F;S 流量到达 Ingress Controller 的 Pod。</li>
<li>Ingress Controller 根据请求的 Host header 和 URL Path，匹配 <code>Ingress</code> 规则。</li>
<li>将请求<strong>代理 (Proxy)</strong> 到规则指定的后端 <code>Service</code> (的 ClusterIP)。</li>
<li>后续从 <code>Service</code> 到最终 Pod 的 L4 转发<strong>仍然由 <code>kube-proxy</code> (iptables&#x2F;IPVS) 处理</strong>。</li>
</ol>
</li>
</ul>
<h3 id="Ingress-关键特性"><a href="#Ingress-关键特性" class="headerlink" title="Ingress 关键特性"></a>Ingress 关键特性</h3><ul>
<li><strong>Host-based Routing (基于主机的路由)</strong>: 根据请求头中的 <code>Host</code> 字段（域名）将流量转发到不同的后端服务。例如，<code>api.example.com</code> 指向 API 服务，<code>shop.example.com</code> 指向商店服务。</li>
<li><strong>Path-based Routing (基于路径的路由)</strong>: 根据请求 URL 中的路径将流量转发到不同的后端服务。例如，<code>example.com/api</code> 指向 API 服务，<code>example.com/ui</code> 指向 UI 服务。</li>
<li><strong>TLS Termination (TLS 终止)</strong>: Ingress Controller 可以配置 TLS 证书（存储在 K8s <code>Secret</code> 中），处理传入的 HTTPS 请求（解密），然后将<strong>未加密的 HTTP 流量</strong>转发给后端服务。这使得<strong>证书管理集中化</strong>，并减轻了后端应用的负担。</li>
<li><strong>资源共享与成本效益</strong>: 多个不同的 Service 可以共享同一个 Ingress Controller 和同一个外部入口点（通常是一个 <code>LoadBalancer</code> Service），<strong>极大地节省了公网 IP 和负载均衡器的成本</strong>。</li>
<li><strong>高级功能 (依赖 Controller 实现)</strong>: 不同的 Ingress Controller 提供不同的扩展功能，例如 URL 重写 (<code>rewrite-target</code>)、请求&#x2F;响应头修改、认证集成 (OAuth2, Basic Auth)、速率限制、灰度发布 (Canary Release) 等，通常通过 <code>Ingress</code> 资源的 <code>annotations</code> 来配置。</li>
</ul>
<h3 id="工作流程与配置示例"><a href="#工作流程与配置示例" class="headerlink" title="工作流程与配置示例"></a>工作流程与配置示例</h3><img src="/2025/04/22/Ingress/image-20250423165931807.png" srcset="/img/loading.gif" lazyload class="" title="image-20250423165931807">

<p>结合图片中的流程，一个典型的使用 Ingress 的工作流如下：</p>
<ol>
<li><p><strong>创建应用 Deployment</strong>: 定义你的应用程序容器和所需的副本数量。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 示例：创建一个名为 my-app 的 Deployment，使用 nginx 镜像，3个副本</span><br>kubectl create deployment my-app --image=nginx --replicas=3<br><span class="hljs-comment"># (确保 Pod 有合适的标签，例如 app=my-app)</span><br>kubectl label deployment my-app app=my-app<br></code></pre></td></tr></table></figure>
</li>
<li><p><strong>创建 Service</strong>: 为 Deployment 创建一个 <code>ClusterIP</code> 类型的 Service。因为流量将通过 Ingress Controller 代理进来，通常不需要直接从外部访问 Service IP。Service 的作用是提供一个稳定的内部 IP 和端口，并让 Ingress Controller 和 <code>kube-proxy</code> 能够发现后端 Pods。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 暴露 Deployment &#x27;my-app&#x27; 的容器端口 80，创建名为 &#x27;my-app-service&#x27; 的 ClusterIP Service</span><br>kubectl expose deployment my-app --port=80 --target-port=80 --name=my-app-service --<span class="hljs-built_in">type</span>=ClusterIP<br></code></pre></td></tr></table></figure>
<p>对应的 Service YAML 可能如下：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">my-app-service</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">selector:</span><br>    <span class="hljs-attr">app:</span> <span class="hljs-string">my-app</span> <span class="hljs-comment"># 确保这里的标签与 Deployment Pod 的标签匹配</span><br>  <span class="hljs-attr">ports:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span><br>      <span class="hljs-attr">port:</span> <span class="hljs-number">80</span>       <span class="hljs-comment"># Service 监听的端口 (供 Ingress Controller 指向)</span><br>      <span class="hljs-attr">targetPort:</span> <span class="hljs-number">80</span> <span class="hljs-comment"># Pod 容器实际监听的端口</span><br>  <span class="hljs-attr">type:</span> <span class="hljs-string">ClusterIP</span><br></code></pre></td></tr></table></figure>
</li>
<li><p><strong>部署 Ingress Controller</strong>: 如果集群中还没有部署，需要先部署一个 Ingress Controller (例如，使用 Helm 安装 <code>ingress-nginx</code>)。这一步通常只需要做一次。</p>
</li>
<li><p><strong>定义 Ingress 资源</strong>: 创建一个 Ingress 对象，定义路由规则。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs YAML"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">networking.k8s.io/v1</span> <span class="hljs-comment"># 使用当前推荐的 API 版本</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Ingress</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">example-ingress</span><br>  <span class="hljs-comment"># Annotations 用于向特定的 Ingress Controller 提供额外的配置指令</span><br>  <span class="hljs-attr">annotations:</span><br>    <span class="hljs-comment"># 示例：对于 nginx-ingress，指定 URL 重写规则</span><br>    <span class="hljs-attr">nginx.ingress.kubernetes.io/rewrite-target:</span> <span class="hljs-string">/$2</span><br>    <span class="hljs-comment"># 示例：如果集群中有多个 Ingress Controller，明确指定使用哪一个</span><br>    <span class="hljs-comment"># kubernetes.io/ingress.class: &quot;nginx&quot;</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-comment"># ingressClassName: nginx # (v1.18+) 另一种更标准的方式指定 Ingress Controller</span><br>  <span class="hljs-attr">rules:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">host:</span> <span class="hljs-string">&quot;myapp.example.com&quot;</span> <span class="hljs-comment"># 基于域名的路由</span><br>    <span class="hljs-attr">http:</span><br>      <span class="hljs-attr">paths:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">path:</span> <span class="hljs-string">/foo(/|$)(.*)</span> <span class="hljs-comment"># 基于路径的路由 (匹配 /foo, /foo/, /foo/...)</span><br>        <span class="hljs-comment"># pathType 定义路径匹配方式:</span><br>        <span class="hljs-comment"># - Prefix: 前缀匹配 (常用)</span><br>        <span class="hljs-comment"># - Exact: 精确匹配</span><br>        <span class="hljs-comment"># - ImplementationSpecific: 由 Ingress Controller 决定</span><br>        <span class="hljs-attr">pathType:</span> <span class="hljs-string">Prefix</span><br>        <span class="hljs-attr">backend:</span><br>          <span class="hljs-attr">service:</span><br>            <span class="hljs-comment"># 指向上面创建的 Service</span><br>            <span class="hljs-attr">name:</span> <span class="hljs-string">my-app-service</span><br>            <span class="hljs-attr">port:</span><br>              <span class="hljs-comment"># 指向 Service 暴露的端口 (spec.ports.port)</span><br>              <span class="hljs-attr">number:</span> <span class="hljs-number">80</span><br>  <span class="hljs-comment"># (可选) 添加 TLS 配置来启用 HTTPS</span><br>  <span class="hljs-attr">tls:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">hosts:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">myapp.example.com</span><br>    <span class="hljs-comment"># secretName 引用一个包含 TLS 证书和私钥的 Kubernetes Secret</span><br>    <span class="hljs-attr">secretName:</span> <span class="hljs-string">myapp-tls-secret</span><br></code></pre></td></tr></table></figure>

<p><strong>关键字段解释</strong>:</p>
<ul>
<li><code>apiVersion: networking.k8s.io/v1</code>: 当前推荐使用的 API 版本。</li>
<li><code>kind: Ingress</code>: 资源类型。</li>
<li><code>metadata.annotations</code>: 为特定的 Ingress Controller 提供额外配置。键名通常包含 Controller 的名称（如 <code>nginx.ingress.kubernetes.io/</code>）。</li>
<li><code>spec.ingressClassName</code>: (v1.18+) 显式指定处理此 Ingress 的 Controller 名称，是比 annotation 更标准的方式。</li>
<li><code>spec.rules</code>: 包含路由规则的列表。</li>
<li><code>spec.rules[].host</code>: 可选，用于基于 Host header 进行路由。如果省略，规则适用于所有到达 Ingress Controller IP 的请求。</li>
<li><code>spec.rules[].http.paths</code>: 定义路径及其对应的后端服务。</li>
<li><code>spec.rules[].http.paths[].path</code>: URL 路径。可以使用正则表达式（取决于 Controller 支持）。</li>
<li><code>spec.rules[].http.paths[].pathType</code>: 路径匹配类型 (<code>Prefix</code>, <code>Exact</code>, <code>ImplementationSpecific</code>)。<strong><code>Prefix</code> 是最常用的</strong>。</li>
<li><code>spec.rules[].http.paths[].backend.service.name</code>: 后端 Service 的名称。</li>
<li><code>spec.rules[].http.paths[].backend.service.port.number</code>: 后端 Service 暴露的端口号。</li>
<li><code>spec.tls</code>: 可选，用于配置 TLS 终止。需要指定域名列表 (<code>hosts</code>) 和包含证书&#x2F;私钥的 <code>secretName</code>。</li>
</ul>
</li>
</ol>
<p>当这个 Ingress 资源被创建后，运行中的 Ingress Controller (假设是 Nginx Ingress Controller 且它被配置为处理这个 Ingress 或默认处理所有 Ingress) 会：</p>
<ol>
<li>检测到新的 Ingress 资源。</li>
<li>解析规则，发现需要将 <code>myapp.example.com/foo/*</code> 的流量路由到 <code>my-app-service</code> 的 80 端口。</li>
<li>如果配置了 TLS，会从 <code>myapp-tls-secret</code> Secret 中获取证书和私钥。</li>
<li>生成新的 Nginx 配置（包含 server block, location block, SSL 配置等）。</li>
<li>重新加载 Nginx 配置 (<code>nginx -s reload</code>)。</li>
</ol>
<p>现在，当外部客户端访问 <code>https://myapp.example.com/foo/bar</code> 时：</p>
<ol>
<li>DNS 将 <code>myapp.example.com</code> 解析到 Ingress Controller 的外部 IP (来自 <code>LoadBalancer</code> Service)。</li>
<li>请求到达 Ingress Controller Pod。</li>
<li>Nginx 根据 Host (<code>myapp.example.com</code>) 和 Path (<code>/foo/bar</code>) 匹配到对应的 <code>location</code> 块。</li>
<li>Nginx 执行 TLS 解密。</li>
<li>Nginx 根据 <code>rewrite-target</code> 注解（如果配置了）可能修改 URL 路径。</li>
<li>Nginx 将请求通过 HTTP 代理到 <code>my-app-service</code> 的 ClusterIP 的 80 端口。</li>
<li>节点上的 <code>kube-proxy</code> (IPVS&#x2F;iptables) 拦截到发往 <code>my-app-service</code> ClusterIP 的流量。</li>
<li><code>kube-proxy</code> 将流量 DNAT 到 <code>my-app</code> Deployment 的某个健康 Pod 的 IP 的 80 端口。</li>
<li>Pod 内的 Nginx 应用处理请求。</li>
</ol>
<h3 id="对比-L4-Service-LoadBalancer-vs-L7-Ingress"><a href="#对比-L4-Service-LoadBalancer-vs-L7-Ingress" class="headerlink" title="对比: L4 Service (LoadBalancer) vs. L7 Ingress"></a>对比: L4 Service (LoadBalancer) vs. L7 Ingress</h3><img src="/2025/04/22/Ingress/image-20250423161359858.png" srcset="/img/loading.gif" lazyload class="" title="image-20250423161359858">

<table>
<thead>
<tr>
<th align="left">特性</th>
<th align="left">L4 Service (LoadBalancer 类型)</th>
<th align="left">L7 Ingress (通过 Ingress Controller)</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>核心层级</strong></td>
<td align="left">L4 (传输层 - TCP&#x2F;UDP)</td>
<td align="left">L7 (应用层 - HTTP&#x2F;HTTPS)</td>
</tr>
<tr>
<td align="left"><strong>架构</strong></td>
<td align="left">每个 Service <strong>独占</strong>一个外部 LB -&gt; NodePort -&gt; Pod</td>
<td align="left"><strong>共享</strong>外部 LB -&gt; NodePort -&gt; Ingress Controller -&gt; Service -&gt; Pod</td>
</tr>
<tr>
<td align="left"><strong>资源成本</strong></td>
<td align="left"><strong>高</strong> (每个 Service 一个外部 LB 实例，成本昂贵)</td>
<td align="left"><strong>低</strong> (多个 Service 共享一个外部 LB 和 Ingress Controller)</td>
</tr>
<tr>
<td align="left"><strong>IP&#x2F;DNS 管理</strong></td>
<td align="left"><strong>复杂</strong> (每个 LB 一个公网 IP&#x2F;DNS 记录)</td>
<td align="left"><strong>简单</strong> (少数公网 IP&#x2F;DNS 指向共享 LB&#x2F;Ingress Controller)</td>
</tr>
<tr>
<td align="left"><strong>TLS 处理</strong></td>
<td align="left"><strong>分散</strong> (通常在后端 Pod 应用内部处理)</td>
<td align="left"><strong>集中</strong> (在 Ingress Controller 层处理 TLS Termination)</td>
</tr>
<tr>
<td align="left"><strong>路由能力</strong></td>
<td align="left">基础 IP&#x2F;Port 路由</td>
<td align="left">丰富 L7 路由 (基于 Host, Path, Header 等)</td>
</tr>
<tr>
<td align="left"><strong>协议支持</strong></td>
<td align="left">TCP&#x2F;UDP</td>
<td align="left">主要 HTTP&#x2F;HTTPS (部分 Controller 通过 TCP&#x2F;UDP Service 暴露自身，或支持 L4 代理)</td>
</tr>
<tr>
<td align="left"><strong>网络跳数</strong></td>
<td align="left">Client -&gt; LB -&gt; Node -&gt; Pod</td>
<td align="left">Client -&gt; LB -&gt; Node -&gt; <strong>Ingress Controller Pod</strong> -&gt; Node -&gt; Pod (增加一跳)</td>
</tr>
<tr>
<td align="left"><strong>系统复杂性</strong></td>
<td align="left">相对简单 (依赖云厂商或 MetalLB)</td>
<td align="left"><strong>增加</strong> (需要额外部署、配置和维护 Ingress Controller)</td>
</tr>
<tr>
<td align="left"><strong>功能丰富度</strong></td>
<td align="left">有限</td>
<td align="left">高 (重写、认证、限流、WAF 集成等，取决于 Controller)</td>
</tr>
</tbody></table>
<p><strong>结论</strong>:</p>
<ul>
<li>对于 <strong>HTTP&#x2F;HTTPS</strong> 服务，<strong>L7 Ingress</strong> 通常是<strong>更优、更经济、更灵活</strong>的选择，尤其是在需要暴露多个 Web 服务或 API 时。</li>
<li>对于 <strong>非 HTTP&#x2F;HTTPS 的 TCP&#x2F;UDP</strong> 服务（如数据库、消息队列、游戏服务器），或者对<strong>网络延迟极其敏感</strong>、不需要 L7 功能的场景，<strong>L4 LoadBalancer Service</strong> (配合云 LB 或 MetalLB) 可能更直接、更合适。</li>
</ul>
<h1 id="高级服务暴露技术"><a href="#高级服务暴露技术" class="headerlink" title="高级服务暴露技术"></a>高级服务暴露技术</h1><p>除了标准的 Service 和 Ingress，Kubernetes 生态系统还发展出更高级的技术来应对特定场景的需求，例如在裸金属环境中提供 LoadBalancer 功能，或者追求极致的 L4 性能。</p>
<h2 id="1-传统应用网络拓扑与演进到-Kubernetes-Ingress"><a href="#1-传统应用网络拓扑与演进到-Kubernetes-Ingress" class="headerlink" title="1. 传统应用网络拓扑与演进到 Kubernetes Ingress"></a>1. 传统应用网络拓扑与演进到 Kubernetes Ingress</h2><img src="/2025/04/22/Ingress/image-20250423172643606.png" srcset="/img/loading.gif" lazyload class="" title="image-20250423172643606">

<h3 id="传统分层负载均衡架构"><a href="#传统分层负载均衡架构" class="headerlink" title="传统分层负载均衡架构"></a>传统分层负载均衡架构</h3><p>在 Kubernetes 出现之前，典型的 Web 应用部署通常采用多层负载均衡架构：</p>
<ul>
<li><strong>全局层 (GTM&#x2F;DNS):</strong> 使用智能 DNS 或全局流量管理器 (GTM) 进行<strong>跨地域&#x2F;跨数据中心</strong>的流量调度，基于用户位置、延迟、健康状况等将用户导向最近或最合适的区域入口。</li>
<li><strong>区域接入层 (Web Tier LB):</strong> 每个区域部署面向公网的 L4&#x2F;L7 负载均衡器，负责接收外部流量、<strong>SSL&#x2F;TLS 卸载</strong>、基础安全防护 (WAF)、并将流量分发给下一层。</li>
<li><strong>应用层 (App Tier LB):</strong> 更靠近应用实例的负载均衡器，负责更细粒度的服务发现、负载均衡策略（轮询、最少连接等）、会话保持。</li>
<li><strong>后端实例 (Instances):</strong> 实际运行业务逻辑的应用服务器。</li>
</ul>
<p><strong>痛点</strong>: 这种架构虽然成熟，但在动态、快速迭代的环境中显得<strong>笨重</strong>。配置复杂、变更流程长、弹性不足，且与应用的生命周期（部署、扩缩容）<strong>解耦</strong>，需要大量手动操作或复杂的自动化脚本。</p>
<h3 id="Kubernetes-Ingress-Controller：云原生网关"><a href="#Kubernetes-Ingress-Controller：云原生网关" class="headerlink" title="Kubernetes Ingress Controller：云原生网关"></a>Kubernetes Ingress Controller：云原生网关</h3><p>Kubernetes Ingress 和 Ingress Controller 的出现，旨在以<strong>云原生的方式</strong>解决传统 LB 的痛点：</p>
<ol>
<li><strong>动态配置与自动化</strong>: Ingress Controller 通过 K8s API 自动感知应用和路由规则的变化，动态更新代理配置，无需手动干预，紧密贴合应用的生命周期。</li>
<li><strong>成本效益</strong>: 通过共享 Ingress Controller 和外部入口点，避免为每个服务创建昂贵的外部 LB。</li>
<li><strong>统一入口与标准化</strong>: 提供标准的 API (<code>Ingress</code> 资源) 来定义 L7 路由规则，简化管理。</li>
<li><strong>丰富路由能力</strong>: 支持 Host&#x2F;Path 路由、TLS 终止等 L7 特性。</li>
</ol>
<p><strong>Ingress Controller 的本质</strong>: 它是一个运行在 K8s 集群内的<strong>反向代理&#x2F;负载均衡器</strong>，它将 K8s 的<strong>声明式配置 (Ingress 资源)</strong> 转化为<strong>底层代理引擎 (Nginx, Envoy, HAProxy 等) 的具体配置</strong>。</p>
<h3 id="Ingress-Controller-的构建与工作原理（回顾）"><a href="#Ingress-Controller-的构建与工作原理（回顾）" class="headerlink" title="Ingress Controller 的构建与工作原理（回顾）"></a>Ingress Controller 的构建与工作原理（回顾）</h3><p>图片下半部分展示了构建一个功能完备的 Ingress Controller（特别是与云平台或底层网络集成时）可能涉及的关键步骤：</p>
<h4 id="a-复用-管理外部负载均衡器接口-获取入口-VIP"><a href="#a-复用-管理外部负载均衡器接口-获取入口-VIP" class="headerlink" title="a. 复用&#x2F;管理外部负载均衡器接口 (获取入口 VIP)"></a>a. 复用&#x2F;管理外部负载均衡器接口 (获取入口 VIP)</h4><ul>
<li><strong>目标</strong>: 为 Ingress Controller Pods 创建一个稳定的、可从外部访问的入口点 (VIP)。</li>
<li><strong>机制</strong>:<ul>
<li><strong>云环境</strong>: Ingress Controller 通常会创建一个 <code>Service</code> 类型为 <code>LoadBalancer</code>。K8s 的 <code>cloud-controller-manager</code> 会调用云厂商 API 创建一个外部 LB，并将其公网 IP 写回 Service 状态。Ingress Controller Pod 接收来自这个云 LB 的流量。</li>
<li><strong>裸金属&#x2F;私有云</strong>: 需要像 <strong>MetalLB</strong> 这样的组件。MetalLB 会负责分配 VIP，并通过 <strong>Layer 2 (ARP&#x2F;NDP)</strong> 或 <strong>BGP</strong> 模式将 VIP 宣告出去，使得流量能够到达运行 Ingress Controller 的节点上。</li>
</ul>
</li>
<li><strong>接口抽象</strong>: 图片中提到的 <code>EnsureLoadBalancer</code>, <code>GetLoadBalancer</code>, <code>UpdateLoadBalancer</code> 等函数签名，代表了与底层 LB（云 LB 或 MetalLB管理的 VIP）交互的接口抽象。Ingress Controller（或相关组件）通过这些接口确保外部流量能正确导入。</li>
</ul>
<h4 id="b-定义-Informer，监控-K8s-资源变化-感知期望状态"><a href="#b-定义-Informer，监控-K8s-资源变化-感知期望状态" class="headerlink" title="b. 定义 Informer，监控 K8s 资源变化 (感知期望状态)"></a>b. 定义 Informer，监控 K8s 资源变化 (感知期望状态)</h4><ul>
<li><strong>目标</strong>: 实时感知 <code>Ingress</code>, <code>Service</code>, <code>EndpointSlice</code>, <code>Secret</code> 等资源的变化。</li>
<li><strong>机制</strong>: 使用 <code>client-go</code> 库的 <strong>Informer</strong> 机制。<ul>
<li>Informer 与 API Server 建立 Watch 连接，高效地接收资源变更事件。</li>
<li>维护本地缓存 (Store)，减少对 API Server 的直接请求。</li>
<li>注册事件处理函数 (<code>AddFunc</code>, <code>UpdateFunc</code>, <code>DeleteFunc</code>)。</li>
<li>事件触发时，通常将变更对象的 key <strong>放入工作队列 (Work Queue)</strong>，实现解耦和缓冲。</li>
</ul>
</li>
</ul>
<h4 id="c-启动-Worker，处理队列中的变更事件-调和实际状态"><a href="#c-启动-Worker，处理队列中的变更事件-调和实际状态" class="headerlink" title="c. 启动 Worker，处理队列中的变更事件 (调和实际状态)"></a>c. 启动 Worker，处理队列中的变更事件 (调和实际状态)</h4><ul>
<li><strong>目标</strong>: 从工作队列中取出变更项，执行实际的配置更新逻辑（<strong>Reconciliation Loop</strong>）。</li>
<li><strong>机制</strong>:<ul>
<li>Worker Goroutine 从队列获取 key。</li>
<li>从 Informer 缓存中获取最新的资源对象。</li>
<li>根据资源对象（特别是 Ingress 规则）计算出底层代理（如 Nginx）的<strong>期望配置</strong>。</li>
<li><strong>生成新配置</strong>文件（如 <code>nginx.conf</code>）。</li>
<li><strong>应用配置</strong>到代理实例（如 <code>nginx -s reload</code>）。</li>
<li>(可选) <strong>更新 DNS</strong>: 如果集成了 ExternalDNS，调用 DNS API 更新域名指向 VIP 的记录。</li>
<li>(可选) <strong>更新 Ingress 状态</strong>: 将 VIP 或主机名写回 Ingress 资源的 <code>status.loadBalancer.ingress</code> 字段。</li>
</ul>
</li>
</ul>
<p>这个持续的“监听-入队-处理-更新”循环，确保了 Ingress Controller 管理的代理配置始终与 Kubernetes 集群中定义的 Ingress 资源状态保持一致。</p>
<h2 id="2-BGP-在-Kubernetes-数据中心的应用"><a href="#2-BGP-在-Kubernetes-数据中心的应用" class="headerlink" title="2. BGP 在 Kubernetes 数据中心的应用"></a>2. BGP 在 Kubernetes 数据中心的应用</h2><p>在裸金属 (Bare Metal) 或私有云环境中，没有现成的云厂商 LoadBalancer 服务。同时，在追求高性能、低延迟、与现有网络深度集成的场景下，即使在云环境，也可能希望避免或优化 Kubernetes 默认的网络模式（如 Overlay 网络和 NAT）。<strong>BGP (Border Gateway Protocol)</strong> 在这些场景下扮演着关键角色。</p>
<h3 id="BGP-解决了什么问题？"><a href="#BGP-解决了什么问题？" class="headerlink" title="BGP 解决了什么问题？"></a>BGP 解决了什么问题？</h3><ol>
<li><strong>提供 <code>LoadBalancer</code> Service 实现</strong>: 像 <strong>MetalLB</strong> 可以使用 BGP 模式，将分配给 <code>LoadBalancer</code> Service 的 External IP 地址<strong>直接宣告</strong>到物理网络中。物理路由器学习到这些路由后，可以将外部流量直接路由到能够处理该 Service 的 Kubernetes 节点上。</li>
<li><strong>消除或减少 NAT</strong>: 当 Service IP 或 Pod IP 被直接宣告到物理网络后，外部流量可以<strong>保留原始客户端源 IP</strong> 地址到达目标节点或 Pod，避免了 <code>kube-proxy</code> 或 CNI 可能引入的 SNAT，简化了网络路径，便于审计和访问控制。</li>
<li><strong>高性能 Pod 网络 (配合 CNI)</strong>: CNI 插件如 <strong>Calico</strong> 可以配置为 BGP 模式。在这种模式下，每个节点将其负责的 Pod CIDR（地址段）通过 BGP 宣告给物理网络（通常是 ToR 交换机）。这使得：<ul>
<li><strong>节点间 Pod 通信</strong>可以直接使用 Pod IP 进行路由，<strong>无需 IPIP 或 VXLAN 封装</strong>，降低了网络开销和延迟。</li>
<li><strong>Pod 访问外部网络</strong>时，其源 IP (Pod IP) 在物理网络中是可路由的，可以在数据中心边界进行集中 NAT，而不是在每个 K8s 节点上进行 SNAT。</li>
</ul>
</li>
<li><strong>与物理网络策略集成</strong>: 物理网络设备（路由器、防火墙）可以直接“看到”Kubernetes 的 Service IP 和 Pod IP，可以基于这些真实的 IP 地址应用更精细化的网络策略（QoS, ACL 等）。</li>
</ol>
<h3 id="BGP-路由宣告的原理与过程"><a href="#BGP-路由宣告的原理与过程" class="headerlink" title="BGP 路由宣告的原理与过程"></a>BGP 路由宣告的原理与过程</h3><p>BGP 是互联网和大型数据中心的核心路由协议，用于在不同的<strong>自治系统 (AS - Autonomous System)</strong> 之间或内部交换<strong>网络可达性信息 (NLRI)</strong>，即 IP 地址前缀。</p>
<p>在 Kubernetes 场景下，宣告过程通常如下：</p>
<ol>
<li><p><strong>部署 BGP Agent&#x2F;Speaker</strong>: 在 Kubernetes 集群中部署能够运行 BGP 协议的组件。</p>
<ul>
<li><strong>MetalLB</strong>: 运行 <code>speaker</code> DaemonSet，每个（或部分）节点上的 Speaker Pod 负责宣告分配给本节点的 Service VIP。</li>
<li><strong>Calico</strong>: <code>calico-node</code> DaemonSet 中通常包含 BGP 功能（基于 Bird 或 GoBGP），负责宣告本节点的 Pod CIDR 和可能的 Service IP (如果配置了)。</li>
</ul>
</li>
<li><p><strong>配置 BGP Peering (对等关系)</strong>: 管理员需要配置 K8s 节点上的 BGP Speaker 与物理网络中的路由器（通常是 <strong>ToR - Top of Rack 交换机</strong>）建立 BGP 会话。关键配置包括：</p>
<ul>
<li><strong>自治系统号 (ASN)</strong>: 为 K8s 集群（或每个节点）和物理路由器分配合适的 ASN。可以在同一 ASN 内建立 <strong>iBGP</strong> (内部 BGP) 会话，或在不同 ASN 间建立 <strong>eBGP</strong> (外部 BGP) 会话。数据中心内部常用 iBGP 或 eBGP。</li>
<li><strong>Neighbor (邻居) 地址</strong>: 在 K8s BGP Agent 上配置 ToR 路由器的 IP 地址，反之亦然。</li>
<li><strong>认证 (可选)</strong>: 配置 MD5 密码增强安全性。</li>
</ul>
</li>
<li><p><strong>触发宣告</strong>:</p>
<ul>
<li><strong>Service IP (MetalLB)</strong>: 当 <code>LoadBalancer</code> Service 被分配 External IP 后，MetalLB Controller 会选择一个或多个节点承载该 VIP，并通知这些节点上的 Speaker 宣告此 <code>/32</code> 的主机路由。</li>
<li><strong>Pod IP (Calico)</strong>: 当 Calico 为节点分配 Pod CIDR 后，该节点上的 BGP Agent 会自动宣告这个前缀（如 <code>/24</code> 或 <code>/26</code>）。</li>
</ul>
</li>
<li><p><strong>发送 BGP UPDATE 消息</strong>: K8s 节点上的 BGP Speaker 向其 Peer (ToR 路由器) 发送 BGP UPDATE 消息，包含：</p>
<ul>
<li><strong>NLRI</strong>: 要宣告的 IP 前缀 (e.g., <code>198.51.100.5/32</code> 或 <code>10.244.1.0/24</code>)。</li>
<li><strong>Path Attributes (路径属性)</strong>:<ul>
<li><strong>AS_PATH</strong>: 路由经过的 AS 列表，用于防环和选路。</li>
<li><strong>NEXT_HOP</strong>: <strong>极其重要</strong>。指示到达目标前缀的下一跳路由器 IP 地址。<strong>当 K8s 节点宣告 Service IP 或 Pod CIDR 时，它会将 NEXT_HOP 设置为自身（节点）的 IP 地址</strong>。</li>
<li><strong>ORIGIN</strong>: 路由来源。</li>
<li><strong>MED, Local Preference (可选)</strong>: 用于影响路径选择。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>物理路由器处理宣告</strong>: ToR 路由器收到 UPDATE 消息后：</p>
<ul>
<li>验证并根据<strong>路由策略 (Route Policy &#x2F; Route Map)</strong> 决定是否接受。</li>
<li>如果接受，将路由（例如 <code>198.51.100.5/32 via &lt;K8s_Node_IP&gt;</code>）添加到 BGP 路由表。</li>
<li>通过 BGP 决策过程选出最优路径，并可能将其安装到<strong>全局路由表 (FIB - Forwarding Information Base)</strong> 中。</li>
<li>路由器可能将此路由<strong>再次宣告</strong>给其他 BGP Peer（如上行 Spine 交换机），使整个网络知道如何到达 K8s 的资源。</li>
</ul>
</li>
</ol>
<p><strong>核心机制</strong>: 物理路由器通过 BGP 学习到，要想到达某个 K8s Service IP 或 Pod IP，需要将数据包直接发送到宣告该路由的那个 K8s 节点的 IP 地址 (NEXT_HOP)。</p>
<h3 id="边缘路由器配置示例-概念性"><a href="#边缘路由器配置示例-概念性" class="headerlink" title="边缘路由器配置示例 (概念性)"></a>边缘路由器配置示例 (概念性)</h3><p>以下是在 ToR 交换机上配置与 K8s 节点建立 BGP Peering 的通用示例 (类 Cisco NX-OS&#x2F;Arista EOS 语法)：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs bash">! 进入配置模式<br>configure terminal<br><br>! 启用 BGP 特性<br>feature bgp<br><br>! 配置 BGP 路由进程，指定本地 AS 号 (假设 ToR 和 K8s 节点都在 AS 65000 - iBGP)<br>router bgp 65000<br>  router-id 10.1.1.1 ! 路由器唯一标识<br><br>  ! 配置与 K8s 节点 Node1 (IP: 10.0.0.101) 的邻居关系<br>  neighbor 10.0.0.101<br>    remote-as 65000         ! 邻居的 AS 号 (与本地相同，表示 iBGP)<br>    description K8S_Node1_BGP<br>    address-family ipv4 unicast ! 启用 IPv4 单播路由交换<br>      ! (可选) 应用路由策略，过滤或修改收到的路由<br>      ! route-map K8S_IMPORT_POLICY <span class="hljs-keyword">in</span><br>      ! (可选 iBGP) 防止下一跳问题<br>      ! next-hop-self<br>    update-source loopback0  ! (推荐) 使用稳定的 Loopback 接口作为 BGP 源 IP<br><br>  ! 配置与 K8s 节点 Node2 (IP: 10.0.0.102) 的邻居关系<br>  neighbor 10.0.0.102<br>    remote-as 65000<br>    description K8S_Node2_BGP<br>    address-family ipv4 unicast<br>      ! route-map K8S_IMPORT_POLICY <span class="hljs-keyword">in</span><br>      ! next-hop-self<br>    update-source loopback0<br><br>  ! ... 为所有需要 Peering 的 K8s 节点配置 neighbor ...<br><br>! (可选) 定义路由策略 (Route Map)<br>! route-map K8S_IMPORT_POLICY permit 10<br>!   match ip address prefix-list K8S_ACCEPTED_PREFIXES ! 匹配允许的前缀列表<br>!   ! (可以设置 local-preference, community 等属性)<br><br>! (可选) 定义前缀列表 (Prefix List)<br>! ip prefix-list K8S_ACCEPTED_PREFIXES <span class="hljs-built_in">seq</span> 10 permit 198.51.100.0/24 ge 32 le 32  ! 只接受 /32 Service IP<br>! ip prefix-list K8S_ACCEPTED_PREFIXES <span class="hljs-built_in">seq</span> 20 permit 10.244.0.0/16 ge 24 le 24    ! 只接受 /24 Pod CIDR<br><br>! 保存配置<br>copy running-config startup-config<br></code></pre></td></tr></table></figure>

<p><strong>关键配置</strong>: <code>neighbor &lt;IP&gt; remote-as &lt;ASN&gt;</code> 是核心。确保 K8s 端（MetalLB&#x2F;Calico 配置）也做了相应的 Peer 配置。</p>
<h3 id="DNS-与-BGP-宣告的-IP"><a href="#DNS-与-BGP-宣告的-IP" class="headerlink" title="DNS 与 BGP 宣告的 IP"></a>DNS 与 BGP 宣告的 IP</h3><p>BGP 负责让 Service 的 External IP 在网络层面<strong>可达</strong>。但用户通常通过<strong>域名</strong>访问服务。因此，你需要<strong>在 DNS 服务中创建记录</strong>（如 A 记录），将域名指向这个通过 BGP 宣告的 IP 地址。</p>
<p><strong>自动化</strong>: <strong>ExternalDNS</strong> 这个 Kubernetes 控制器可以自动完成这个过程。它监控 Service (或 Ingress) 资源，当发现带有 External IP 和特定注解的 Service 时，会自动调用 DNS 提供商（如 Route 53, Cloudflare, CoreDNS）的 API 来创建或更新 DNS 记录。</p>
<p><strong>总结</strong>: BGP 使得 IP 可路由，DNS 使得域名可解析到该 IP。两者协同工作。</p>
<h2 id="3-L4-DSR-Direct-Server-Return-负载均衡架构"><a href="#3-L4-DSR-Direct-Server-Return-负载均衡架构" class="headerlink" title="3. L4 DSR (Direct Server Return) 负载均衡架构"></a>3. L4 DSR (Direct Server Return) 负载均衡架构</h2><img src="/2025/04/22/Ingress/image-20250423190233621.png" srcset="/img/loading.gif" lazyload class="" title="image-20250423190233621">

<p>在需要处理<strong>极大 L4 流量</strong>（特别是响应流量远大于请求流量的场景，如视频流、大文件下载）时，传统的负载均衡器（包括 <code>kube-proxy</code> 或标准云 LB）可能会因为需要同时处理进出流量而成为<strong>性能瓶颈</strong>。<strong>DSR (Direct Server Return)</strong> 是一种优化技术，旨在解决这个问题。</p>
<h3 id="DSR-核心思想"><a href="#DSR-核心思想" class="headerlink" title="DSR 核心思想"></a>DSR 核心思想</h3><p><strong>非对称路由</strong>:</p>
<ul>
<li><strong>请求路径</strong>: Client -&gt; Load Balancer (Director) -&gt; Real Server (Backend Pod)</li>
<li><strong>响应路径</strong>: Real Server (Backend Pod) -&gt; Client (直接返回，<strong>绕过 Load Balancer</strong>)</li>
</ul>
<p>负载均衡器 (Director) 只负责接收客户端请求，进行调度决策，并将请求转发给后端服务器。而后端服务器在处理完请求后，<strong>直接将响应数据包发送回客户端</strong>，不再经过负载均衡器。</p>
<h3 id="Kubernetes-中的-L4-DSR-架构"><a href="#Kubernetes-中的-L4-DSR-架构" class="headerlink" title="Kubernetes 中的 L4 DSR 架构"></a>Kubernetes 中的 L4 DSR 架构</h3><p>该图展示了一种在 Kubernetes 中实现 L4 DSR 的架构，通常依赖 <strong>IPVS</strong> 的 DSR 模式（也称为 DR - Direct Routing 模式）。</p>
<p><strong>架构组件</strong>:</p>
<ul>
<li><strong>Client</strong>: 发起请求。</li>
<li><strong>VIP (Virtual IP Address)</strong>: 暴露给客户端的服务入口 IP。</li>
<li><strong>Router</strong>: 物理网络路由器，通常配置 <strong>ECMP (Equal-Cost Multi-Path)</strong>，将发往 VIP 的流量负载均衡到多个 L4 LB 节点。通过 <strong>BGP</strong> 从 L4 LB 节点学习 VIP 的可达性。</li>
<li><strong>k8s minion node (L4 LB 层 - Director)</strong>: 运行 DSR Director 角色的 K8s 节点。<ul>
<li><strong>Elb-Director (k8s pod)</strong>: 控制平面组件，监听 K8s Service&#x2F;Endpoints，获取 VIP 和后端 Pod IP 列表。通过 <strong>Netlink</strong> 接口配置<strong>本节点</strong>的 Linux 内核 <strong>IPVS</strong> 模块。</li>
<li><strong>Linux Kernel &#x2F; IPVS</strong>: 实际处理请求转发。收到目标为 VIP 的包后，根据调度算法选择一个后端 Pod (Real Server)。<strong>关键操作</strong>: 在 DSR&#x2F;DR 模式下，IPVS <strong>不修改 IP 头部 (源 IP&#x3D;ClientIP, 目标 IP&#x3D;VIP)</strong>，而是<strong>仅修改 L2 帧头</strong>，将目标 MAC 地址改为选定后端 Pod&#x2F;节点 的 MAC 地址，然后将帧转发出去。</li>
</ul>
</li>
<li><strong>k8s minion node (Backend Service 层 - Real Server)</strong>: 运行实际业务 Pod 的 K8s 节点。<ul>
<li><strong>Backend Pod (k8s pod)</strong>: 接收来自 L4 LB 节点转发的、目标 IP 仍为 VIP 的数据包。</li>
</ul>
</li>
</ul>
<p><strong>控制路径 (配置同步)</strong>:</p>
<ol>
<li>用户创建&#x2F;更新 Service (带有特定配置启用 DSR)。</li>
<li>Elb-Director Pod 监听到变化。</li>
<li>Elb-Director 通过 Netlink 配置 L4 LB 节点上的 IPVS：<ul>
<li>添加 VIP 作为 Virtual Service。</li>
<li>添加健康的 Backend Pod IP 作为 Real Server，并指定<strong>转发模式为 DSR&#x2F;DR (<code>-g</code> 选项 in <code>ipvsadm</code>)</strong>。</li>
</ul>
</li>
</ol>
<p><strong>数据路径 (核心流程)</strong>:</p>
<ol>
<li><p><strong>请求</strong>: Client -&gt; Router (ECMP) -&gt; L4 LB Node (IPVS) -&gt; Backend Node</p>
<ul>
<li>Client 发送包：<code>[ IP(Src:ClientIP, Dst:VIP) | TCP | Payload ]</code></li>
<li>Router 通过 ECMP 选择一个 L4 LB 节点，转发 L2 帧。</li>
<li>L4 LB 节点 IPVS 模块收到包，选择后端 Pod (RS_IP, RS_MAC)。</li>
<li>IPVS <strong>重写 L2 帧头</strong>: <code>[ Eth(Src:LB_MAC, Dst:RS_MAC) | IP(Src:ClientIP, Dst:VIP) | TCP | Payload ]</code> (IP 头不变!)</li>
<li>帧发送给后端节点。</li>
</ul>
</li>
<li><p><strong>后端节点处理请求</strong>:</p>
<ul>
<li>后端节点网卡收到帧，解开 L2，得到 IP 包 <code>[ IP(Src:ClientIP, Dst:VIP) | ... ]</code>。</li>
<li><strong>关键配置</strong>: 为了让后端节点内核接受这个目标 IP 是 VIP 的包，<strong>必须在所有后端节点（或 Pod 网络命名空间内）配置 VIP 地址</strong>，通常配置在 <code>lo</code> (loopback) 接口上，并<strong>抑制该接口对 VIP 的 ARP 响应</strong>（见下文 ARP 问题）。</li>
<li>内核将包路由给监听相应端口的 Backend Pod。Pod 内的应用看到请求的源 IP 是真实的 <strong>ClientIP</strong>。</li>
</ul>
</li>
<li><p><strong>响应</strong>: Backend Node -&gt; Router -&gt; Client (<strong>绕过 L4 LB</strong>)</p>
<ul>
<li>Backend Pod 处理完毕，生成响应包。</li>
<li>构建响应 IP 包：<strong>Source IP &#x3D; VIP</strong>, <strong>Destination IP &#x3D; ClientIP</strong>。 (源 IP 必须是 VIP!)</li>
<li><code>[ IP(Src:VIP, Dst:ClientIP) | TCP | Response Payload ]</code></li>
<li>后端节点内核根据 <strong>Destination IP (ClientIP)</strong> 查询路由表，找到通往客户端的路径（通常是默认网关 Router）。</li>
<li>构建 L2 帧头：<code>[ Eth(Src:Backend_MAC, Dst:Router_MAC) | IP(Src:VIP, Dst:ClientIP) | TCP | Response Payload ]</code></li>
<li>帧直接发送给 Router。</li>
<li>Router 将响应包路由回 Client。</li>
</ul>
</li>
</ol>
<h3 id="DSR-的核心挑战：The-ARP-Problem"><a href="#DSR-的核心挑战：The-ARP-Problem" class="headerlink" title="DSR 的核心挑战：The ARP Problem"></a>DSR 的核心挑战：The ARP Problem</h3><p>如果后端服务器节点响应了网络上对 VIP 地址的 ARP 请求，那么路由器或其他设备可能会错误地学习到 VIP 对应的 MAC 地址是某个后端服务器的 MAC 地址。这会导致后续发往 VIP 的流量被直接发送到该后端服务器，绕过了 L4 LB Director，破坏了负载均衡。</p>
<p><strong>解决方案</strong>: 必须在<strong>所有后端服务器节点</strong>上进行内核参数调整，以阻止它们响应对配置在 <code>lo</code> 接口上的 VIP 的 ARP 请求。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 1. 在所有后端节点（或 Pod 网络命名空间内）的 lo 接口上配置 VIP</span><br><span class="hljs-built_in">sudo</span> ip addr add <span class="hljs-variable">$&#123;VIP&#125;</span>/32 dev lo scope host<br><br><span class="hljs-comment"># 2. 配置 sysctl 参数抑制 ARP 响应</span><br><span class="hljs-comment"># (写入 /etc/sysctl.conf 或 /etc/sysctl.d/dsr.conf)</span><br><span class="hljs-comment"># arp_ignore=1: 只在请求的目标 IP 配置在接收 ARP 请求的接口上时才回应 (lo 接口收到对 VIP 的 ARP 不会回应)</span><br>net.ipv4.conf.all.arp_ignore = 1<br>net.ipv4.conf.lo.arp_ignore = 1<br><span class="hljs-comment"># arp_announce=2: 总是使用接口上最合适的本地地址作为源 IP 进行通告，避免将 lo 上的 VIP 向外通告</span><br>net.ipv4.conf.all.arp_announce = 2<br>net.ipv4.conf.lo.arp_announce = 2<br><br><span class="hljs-comment"># 应用配置</span><br><span class="hljs-built_in">sudo</span> sysctl -p /etc/sysctl.d/dsr.conf<br></code></pre></td></tr></table></figure>

<p><strong>必须确保所有 Real Server 都应用了这些配置。</strong> 在 Kubernetes 中，这通常通过运行一个特权的 DaemonSet 来自动化完成。</p>
<h3 id="DSR-优缺点"><a href="#DSR-优缺点" class="headerlink" title="DSR 优缺点"></a>DSR 优缺点</h3><ul>
<li><strong>优点:</strong><ul>
<li><strong>极高吞吐量</strong>: LB 节点只处理请求流量，性能瓶颈大大缓解。</li>
<li><strong>良好伸缩性</strong>: 后端服务实例增加对 LB 节点压力影响小。</li>
<li><strong>保持客户端源 IP</strong>: 后端服务可见真实客户端 IP。</li>
</ul>
</li>
<li><strong>缺点:</strong><ul>
<li><strong>配置复杂</strong>: 后端节点需要特殊配置（VIP on lo, ARP 抑制）。</li>
<li><strong>网络环境要求</strong>: 需要后端节点可以直接路由回客户端。</li>
<li><strong>故障排查难度增加</strong>: 非对称路由可能使问题定位更复杂。</li>
</ul>
</li>
</ul>
<h2 id="4-L7-集群架构与-IPIP-隧道"><a href="#4-L7-集群架构与-IPIP-隧道" class="headerlink" title="4. L7 集群架构与 IPIP 隧道"></a>4. L7 集群架构与 IPIP 隧道</h2><img src="/2025/04/22/Ingress/image-20250423190256226.png" srcset="/img/loading.gif" lazyload class="" title="image-20250423190256226">

<p>这张图展示了一个典型的 Kubernetes L7 架构，其中 L7 Proxy Pods (如 Nginx Ingress Controller) 作为流量入口，并将请求转发给后端的 Application Pods。图中还涉及到了 VIP 的管理和跨节点通信可能使用的 IPIP 隧道技术。</p>
<h3 id="L7-架构回顾"><a href="#L7-架构回顾" class="headerlink" title="L7 架构回顾"></a>L7 架构回顾</h3><ul>
<li><strong>适用场景</strong>: 微服务网关、Web 应用接入层、Kubernetes Ingress&#x2F;Gateway API 实现。</li>
<li><strong>核心组件</strong>: L7 Proxy Pods (运行 Nginx, Envoy 等), Application Pods, Virtual IPs (VIPs)。</li>
<li><strong>VIP 管理</strong>: VIP 是稳定的逻辑访问点。在 K8s 中，可以通过 <code>Service Type=LoadBalancer</code> (云环境或配合 MetalLB)、MetalLB (裸金属)、或 Ingress Controller 自身的 Service 来实现 VIP 的分配和路由。最终目标是让发往 VIP 的流量能够负载均衡地到达健康的 L7 Proxy Pod 实例。<code>kube-proxy</code> (特别是 IPVS 模式) 在节点层面负责将到达节点的 VIP 流量转发给本地或远程的 L7 Proxy Pod。</li>
</ul>
<h3 id="IPIP-隧道与跨节点-Pod-通信"><a href="#IPIP-隧道与跨节点-Pod-通信" class="headerlink" title="IPIP 隧道与跨节点 Pod 通信"></a>IPIP 隧道与跨节点 Pod 通信</h3><p>当 L7 Proxy Pod (运行在 Node A) 需要将请求转发给一个运行在<strong>不同节点 (Node B)</strong> 上的 App Pod 时，就需要跨节点 Pod 通信。如果底层网络不支持直接路由 Pod IP，或者需要构建 Overlay 网络，<strong>IPIP (IP in IP)</strong> 隧道是一种常用的技术。</p>
<p><strong>为何需要隧道?</strong></p>
<ul>
<li>解决 Pod IP 在物理网络中不可路由的问题。</li>
<li>简化物理网络配置，只需路由节点 IP。</li>
<li>跨越 L2&#x2F;L3 边界。</li>
</ul>
<p><strong>IPIP 封包 (Encapsulation) - Node A 发送给 Node B 上的 Pod</strong>:</p>
<ol>
<li><strong>原始 IP 包 (Inner)</strong>: <code>[ IP(Src:Proxy_Pod_IP, Dst:App_Pod_IP) | TCP | Payload ]</code></li>
<li>Node A 内核决定通过 IPIP 隧道发送。</li>
<li><strong>添加外部 IP 头 (Outer)</strong>:<ul>
<li>Outer SrcIP: <code>NodeA_IP</code></li>
<li>Outer DstIP: <code>NodeB_IP</code></li>
<li>Outer Protocol: <code>4</code> (IP-in-IP)</li>
</ul>
</li>
<li><strong>封装</strong>: 原始包成为外部包的 Payload。<br><code>[ OuterIP(Src:NodeA, Dst:NodeB, Proto:4) | InnerIP(Src:ProxyPod, Dst:AppPod) | TCP | Payload ]</code></li>
<li>封装后的包通过物理网络发送给 Node B。</li>
</ol>
<p><strong>IPIP 解包 (Decapsulation) - Node B 接收</strong>:</p>
<ol>
<li>Node B 的物理网卡收到一个 IP 数据包，其目的 IP 是 <code>NodeB_IP</code>。</li>
<li><strong>协议检查</strong>: Node B 的 Linux 内核检查收到的 IP 包的 <strong>IP Header 中的 Protocol 字段</strong>。发现其值为 <strong><code>4</code></strong>。</li>
<li><strong>IPIP 解封装</strong>: 内核识别出这是一个 IPIP 封装的数据包，于是：<ul>
<li><strong>剥离 (Strip)</strong> 掉外部 IP Header。</li>
<li>将内部载荷（即<strong>原始的 IP 数据包</strong>）提取出来。</li>
</ul>
</li>
<li><strong>内部包路由</strong>: 内核现在处理这个解封装后的原始 IP 数据包。根据其 <strong>内部目的 IP (<code>App_Pod_IP</code>)</strong>，查询本地的路由表，将数据包转发给本地的目标 App Pod。</li>
</ol>
<p><strong>Kubernetes CNI 与 IPIP</strong>:<br>CNI 插件如 <strong>Calico</strong> (在 IPIP 模式下) 或 <strong>Flannel</strong> (也可配置为 IPIP) 会自动管理 IPIP 隧道的创建和路由规则配置，使得跨节点 Pod 通信对应用透明。它们通常利用 Linux 内核的 <code>ipip</code> 模块或 XFRM&#x2F;FOU 机制来实现封装和解封装。</p>
<h1 id="数据流综合示例"><a href="#数据流综合示例" class="headerlink" title="数据流综合示例"></a>数据流综合示例</h1><p>让我们通过一个更具体的例子，结合多种技术，来追踪数据包的旅程。</p>
<img src="/2025/04/22/Ingress/image-20250423201824385.png" srcset="/img/loading.gif" lazyload class="" title="image-20250423201824385">

<p><strong>场景</strong>: 外部客户端 (<code>66.0.0.1</code>) 访问 K8s 集群中的服务，入口 VIP 为 <code>10.0.0.1</code>。流量最终由运行在 Gateway Pod (<code>10.0.3.48</code>) 内的 Envoy 代理处理。集群使用 Calico CNI，配置了 BGP 和 IP-in-IP 隧道，并使用 IPVS 模式的 <code>kube-proxy</code>。</p>
<p><strong>数据包流向</strong>:</p>
<ol>
<li><p><strong>外部请求与路由</strong>:</p>
<ul>
<li>Client (<code>s:66.0.0.1</code>) 发送请求到 VIP (<code>d:10.0.0.1</code>)。</li>
<li>数据包经互联网&#x2F;企业网到达数据中心边界，最终抵达 ToR 交换机 (<code>10.0.2.1</code>)。</li>
<li>ToR 根据其路由表（可能通过 BGP 从 K8s 节点学习到 <code>10.0.0.1</code> 的路由，或者 <code>10.0.0.1</code> 是配置在外部 LB 上的 VIP，LB 再将流量导向节点）将数据包转发给 K8s 节点 (<code>10.0.2.46</code>)。</li>
</ul>
</li>
<li><p><strong>节点入口与 Kube-Proxy&#x2F;IPVS</strong>:</p>
<ul>
<li>数据包到达节点 <code>10.0.2.46</code> 的 <code>eth0</code> 接口。</li>
<li>内核网络栈处理。<code>kube-proxy</code> (IPVS 模式) 拦截到目标为 VIP <code>10.0.0.1</code> 的流量。</li>
<li>IPVS 查找规则，根据负载均衡算法选择后端 Pod，即 Gateway Pod (<code>10.0.3.48</code>)。</li>
<li>IPVS 执行 <strong>DNAT</strong>，准备将包转发给 <code>10.0.3.48</code>。由于目标 Pod 在不同子网的另一个节点上，IPVS (或后续的路由决策) 知道需要通过隧道。</li>
</ul>
</li>
<li><p><strong>Calico BGP&#x2F;IP-in-IP 封装与转发</strong>:</p>
<ul>
<li>节点 <code>10.0.2.46</code> 需要将包发送给 <code>10.0.3.48</code>。</li>
<li><strong>Calico BGP</strong>: 节点上的 Bird&#x2F;BGP 客户端已经通过 BGP 学习到 <code>10.0.3.x</code> Pod 子网的可达性信息，知道目标 Pod 位于哪个节点（我们称之为 Node B）。</li>
<li><strong>Calico IP-in-IP</strong>: 由于跨子网，Calico 配置为使用 IP-in-IP 隧道。内核执行封装：<ul>
<li><strong>Inner Packet</strong>: <code>[ IP(Src:66.0.0.1, Dst:10.0.0.1) | ... ]</code> (注意：这里 Dst 仍是 VIP，DNAT 可能发生在封装后或解封装前，具体取决于 IPVS 和 CNI 的交互细节) 或 <code>[ IP(Src:66.0.0.1, Dst:10.0.3.48) | ... ]</code> (如果 DNAT 在封装前完成)。图中似乎暗示 DNAT 发生在 IPVS 阶段。</li>
<li><strong>Outer Header</strong>: <code>[ IP(Src:NodeA_TunnelIP=10.0.2.24, Dst:NodeB_IP or PodIP=10.0.3.48, Proto:4) ]</code> (Calico 的 IPIP 模式可以直接将 Outer Dst 设为 Pod IP)。</li>
</ul>
</li>
<li>封装后的包 <code>[ OuterIP | InnerIP | ... ]</code> 根据 Outer Dst IP (<code>10.0.3.48</code>) 进行路由，通过 ToR <code>10.0.2.1</code> 发往目标节点所在的网络。</li>
</ul>
</li>
<li><p><strong>目标节点解封装与 Pod 交付</strong>:</p>
<ul>
<li>封装包到达目标节点 (Node B)。</li>
<li>Node B 内核检查 Outer IP Header，发现 Proto&#x3D;4，执行 <strong>IP-in-IP 解封装</strong>，剥离 Outer Header。</li>
<li>得到 <strong>Inner Packet</strong> (<code>[ IP(Src:66.0.0.1, Dst:10.0.0.1 or 10.0.3.48) | ... ]</code>)。</li>
<li>内核将 Inner Packet 通过 <strong>veth pair</strong> (<code>caliXXX</code> &lt;-&gt; <code>eth0</code> in Pod) 路由到 Gateway Pod (<code>10.0.3.48</code>) 的网络命名空间。</li>
</ul>
</li>
<li><p><strong>Envoy 处理与响应</strong>:</p>
<ul>
<li>数据包到达 Gateway Pod 内的 <code>eth0</code> 接口。</li>
<li>Pod 内运行的 <strong>Envoy</strong> 代理监听端口 80，接收请求。</li>
<li>Envoy 根据其配置（Listener, Route, Cluster）处理请求，可能包括路由、限流、认证等，并将请求转发给真正的后端应用服务（图中未画出后端服务）。</li>
<li>Envoy 收到后端响应后，构建响应包：<code>[ IP(Src:PodIP=10.0.3.48 or VIP=10.0.0.1, Dst:ClientIP=66.0.0.1) | ... ]</code>。</li>
<li>响应包通过 Pod 的 <code>eth0</code> -&gt; veth pair -&gt; 宿主机网络 -&gt; ToR -&gt; 外部网络，<strong>直接返回给客户端</strong>（通常无需再次封装）。</li>
</ul>
</li>
</ol>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>Kubernetes 提供了灵活多样的网络服务暴露机制：</p>
<ul>
<li><strong>Service (<code>ClusterIP</code>, <code>NodePort</code>)</strong>: 集群内部服务发现和基础 L4 暴露。</li>
<li><strong>Service (<code>LoadBalancer</code>)</strong>: 云环境或配合 MetalLB 实现标准外部 L4 暴露。</li>
<li><strong>Ingress</strong>: 成本效益高、功能丰富的 L7 HTTP&#x2F;S 流量入口管理。</li>
<li><strong>BGP (配合 MetalLB&#x2F;Calico)</strong>: 适用于裸金属环境，提供高性能、原生 IP 路由的 L4&#x2F;Pod 网络集成。</li>
<li><strong>DSR (配合 IPVS)</strong>: 针对超高 L4 吞吐量场景的性能优化技术。</li>
</ul>
<p>选择哪种技术取决于具体的应用场景、性能需求、成本考虑以及运维复杂度。理解这些技术的工作原理和它们之间的关系，对于构建健壮、可扩展的 Kubernetes 应用至关重要。</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/kubernetes/" class="print-no-link">#kubernetes</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Kubernetes 网络服务暴露：从 Service 到 Ingress 与 BGP/DSR</div>
      <div>https://mfzzf.github.io/2025/04/22/Ingress/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Mzzf</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年4月22日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/04/24/kubernetes%E7%94%9F%E4%BA%A7%E5%8C%96%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/" title="kubernetes生产化集群管理">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">kubernetes生产化集群管理</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/04/17/docker-backup/" title="docker-backup">
                        <span class="hidden-mobile">docker-backup</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
