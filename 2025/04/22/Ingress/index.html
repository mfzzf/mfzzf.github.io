

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Mzzf">
  <meta name="keywords" content="">
  
    <meta name="description" content="Kubernetes 网络负载均衡1. K8s 负载均衡的必要性 问题: Pod IP 是动态分配且 Pod 是短暂的 (ephemeral)。直接访问 Pod IP 不可靠。 需求: 提供稳定的服务访问入口 (IP&#x2F;Port)。 将请求分发到健康的后端 Pod 实例。 自动感知后端 Pod 变化并更新路由。    2. L4 负载均衡: Service 核心对象: Service (K">
<meta property="og:type" content="article">
<meta property="og:title" content="Ingress">
<meta property="og:url" content="https://mfzzf.github.io/2025/04/22/Ingress/index.html">
<meta property="og:site_name" content="Mzzf&#39;s Blog">
<meta property="og:description" content="Kubernetes 网络负载均衡1. K8s 负载均衡的必要性 问题: Pod IP 是动态分配且 Pod 是短暂的 (ephemeral)。直接访问 Pod IP 不可靠。 需求: 提供稳定的服务访问入口 (IP&#x2F;Port)。 将请求分发到健康的后端 Pod 实例。 自动感知后端 Pod 变化并更新路由。    2. L4 负载均衡: Service 核心对象: Service (K">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/22/Ingress/image-20250423161359858.png">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/22/Ingress/image-20250423165931807.png">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/22/Ingress/image-20250423172643606.png">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/22/Ingress/image-20250423190212569.png">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/22/Ingress/image-20250423190233621.png">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/22/Ingress/image-20250423190256226.png">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/22/Ingress/image-20250423201824385.png">
<meta property="article:published_time" content="2025-04-22T14:19:47.000Z">
<meta property="article:modified_time" content="2025-04-23T12:31:36.255Z">
<meta property="article:author" content="Mzzf">
<meta property="article:tag" content="kubernetes">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://mfzzf.github.io/2025/04/22/Ingress/image-20250423161359858.png">
  
  
  
  <title>Ingress - Mzzf&#39;s Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"mfzzf.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Mzzf&#39;s blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="Ingress"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-04-22 22:19" pubdate>
          2025年4月22日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          18k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          152 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">Ingress</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="Kubernetes-网络负载均衡"><a href="#Kubernetes-网络负载均衡" class="headerlink" title="Kubernetes 网络负载均衡"></a>Kubernetes 网络负载均衡</h1><h2 id="1-K8s-负载均衡的必要性"><a href="#1-K8s-负载均衡的必要性" class="headerlink" title="1. K8s 负载均衡的必要性"></a>1. K8s 负载均衡的必要性</h2><ul>
<li><strong>问题</strong>: Pod IP 是动态分配且 Pod 是短暂的 (ephemeral)。直接访问 Pod IP 不可靠。</li>
<li><strong>需求</strong>:<ul>
<li>提供稳定的服务访问入口 (IP&#x2F;Port)。</li>
<li>将请求分发到健康的后端 Pod 实例。</li>
<li>自动感知后端 Pod 变化并更新路由。</li>
</ul>
</li>
</ul>
<h2 id="2-L4-负载均衡-Service"><a href="#2-L4-负载均衡-Service" class="headerlink" title="2. L4 负载均衡: Service"></a>2. L4 负载均衡: Service</h2><ul>
<li><strong>核心对象</strong>: <code>Service</code> (K8s API Object)</li>
<li><strong>作用</strong>: 为一组功能相同的 Pod 提供统一、稳定的 L4 (TCP&#x2F;UDP) 访问入口 (ClusterIP + Port)。解决集群内部服务发现和基础负载均衡。</li>
<li><strong>实现者</strong>: <code>kube-proxy</code> (运行在每个 Node 上的 Agent)<ul>
<li>监视 API Server (<code>Service</code>, <code>EndpointSlice</code> 变更)。</li>
<li>在 Node 上修改网络规则 (iptables 或 IPVS) 来实现流量转发。</li>
</ul>
</li>
</ul>
<h3 id="kube-proxy-模式"><a href="#kube-proxy-模式" class="headerlink" title="kube-proxy 模式"></a><code>kube-proxy</code> 模式</h3><ol>
<li><p><strong><code>iptables</code> 模式 (默认)</strong></p>
<ul>
<li><strong>原理</strong>: 使用 Linux 内核 <code>netfilter</code> 的 <code>iptables</code> 规则链 (<code>nat</code> 表: <code>KUBE-SERVICES</code> -&gt; <code>KUBE-SVC-*</code> -&gt; <code>KUBE-SEP-*</code>) 进行 <strong>DNAT</strong>，将 Service IP 流量转发到 Pod IP。</li>
<li><strong>优点</strong>: 成熟稳定。</li>
</ul>
<ul>
<li><strong>缺点</strong>: Service&#x2F;Pod 数量大时，规则链长，线性查找<strong>性能下降</strong>。</li>
</ul>
</li>
<li><p><strong><code>IPVS</code> (IP Virtual Server) 模式</strong></p>
<ul>
<li><strong>原理</strong>: 使用 Linux 内核的 <code>IPVS</code> 模块，创建虚拟服务器 (VS) 和真实服务器 (RS)。使用<strong>哈希表</strong>管理规则。</li>
<li><strong>优点</strong>: <strong>高性能</strong>，支持多种负载均衡算法 (rr, lc, wrr 等)。</li>
<li><strong>缺点</strong>: 需内核支持 IPVS 模块。</li>
</ul>
</li>
</ol>
<h3 id="Service-类型"><a href="#Service-类型" class="headerlink" title="Service 类型"></a>Service 类型</h3><ul>
<li><strong><code>ClusterIP</code></strong>: 默认类型，提供一个集群内部 IP，只能在集群内访问。</li>
<li><strong><code>NodePort</code></strong>: 在每个 Node 上暴露一个静态端口 (<code>NodeIP:NodePort</code>)，外部流量可通过此端口访问 Service。</li>
<li><strong><code>LoadBalancer</code></strong>: (通常用于云环境) 创建一个外部云负载均衡器 (ELB)，指向服务的 <code>NodePort</code> 或直接到 Pod (取决于云厂商集成)。提供一个公网 IP&#x2F;DNS。</li>
</ul>
<h2 id="3-L7-负载均衡-Ingress"><a href="#3-L7-负载均衡-Ingress" class="headerlink" title="3. L7 负载均衡: Ingress"></a>3. L7 负载均衡: Ingress</h2><ul>
<li><strong>核心对象</strong>: <code>Ingress</code> (K8s API Object)</li>
<li><strong>作用</strong>: 定义 <strong>HTTP&#x2F;HTTPS</strong> 流量如何从集群外部路由到集群内部的 <code>Service</code>。提供更丰富的应用层路由能力。</li>
<li><strong>实现者</strong>: <strong><code>Ingress Controller</code></strong> (集群内运行的独立 Pod&#x2F;Deployment)<ul>
<li><strong>不是 K8s 自带组件</strong>，需要单独部署 (e.g., Nginx Ingress, Traefik, HAProxy Ingress)。</li>
<li>监视 API Server (<code>Ingress</code>, <code>Service</code>, <code>EndpointSlice</code>)。</li>
<li>根据 <code>Ingress</code> 规则动态配置其内部的反向代理 (Nginx&#x2F;HAProxy等)。</li>
<li>接收外部流量 (通常通过 <code>LoadBalancer</code> 或 <code>NodePort</code> Service 暴露 Controller 自身)，并将流量代理到后端 <code>Service</code>。</li>
</ul>
</li>
</ul>
<h3 id="Ingress-关键特性"><a href="#Ingress-关键特性" class="headerlink" title="Ingress 关键特性"></a>Ingress 关键特性</h3><ul>
<li><strong>Host&#x2F;Path Based Routing</strong>: 基于域名或 URL 路径转发流量。</li>
<li><strong>TLS Termination</strong>: 在 Ingress Controller 层处理 HTTPS 加解密，<strong>集中管理证书</strong>。后端应用只需处理 HTTP。</li>
<li><strong>资源共享</strong>: 多个 Service 可共享同一个 Ingress Controller 和外部 LB，<strong>节省成本</strong>。</li>
<li><strong>依赖 Controller</strong>: 高级功能 (rewrite, auth) 取决于所选 Ingress Controller 的能力。</li>
</ul>
<p><strong>注意</strong>: Ingress Controller 将 L7 流量导向 <code>Service</code> 后，后续从 <code>Service</code> 到 <code>Pod</code> 的 L4 转发<strong>仍然由 <code>kube-proxy</code> (iptables&#x2F;IPVS) 处理</strong>。</p>
<h2 id="4-核心概念-网络五元组-Network-5-Tuple"><a href="#4-核心概念-网络五元组-Network-5-Tuple" class="headerlink" title="4. 核心概念: 网络五元组 (Network 5-Tuple)"></a>4. 核心概念: 网络五元组 (Network 5-Tuple)</h2><ul>
<li><strong>定义</strong>: 唯一标识一个 TCP&#x2F;IP 网络连接的五个元素：<ol>
<li><strong>源 IP 地址 (Source IP)</strong></li>
<li><strong>源端口号 (Source Port)</strong></li>
<li><strong>目的 IP 地址 (Destination IP)</strong></li>
<li><strong>目的端口号 (Destination Port)</strong></li>
<li><strong>协议号 (Protocol)</strong> (e.g., TCP&#x3D;6, UDP&#x3D;17)</li>
</ol>
</li>
<li><strong>重要性</strong>:<ul>
<li><strong>唯一标识连接</strong>: 网络设备区分不同会话的基础。</li>
<li><strong>连接跟踪 (<code>conntrack</code>)</strong>: Linux 内核使用五元组跟踪 TCP&#x2F;UDP 连接状态 (ESTABLISHED, FIN_WAIT 等)。</li>
<li><strong>NAT</strong>: 维护内外地址&#x2F;端口映射关系。</li>
<li><strong>状态防火墙</strong>: 基于连接状态进行访问控制。</li>
<li><strong>L4 负载均衡</strong>: 分发流量和维持会话一致性。</li>
</ul>
</li>
</ul>
<h2 id="5-对比-L4-Service-LoadBalancer-vs-L7-Ingress"><a href="#5-对比-L4-Service-LoadBalancer-vs-L7-Ingress" class="headerlink" title="5. 对比: L4 Service (LoadBalancer) vs. L7 Ingress"></a>5. 对比: L4 Service (LoadBalancer) vs. L7 Ingress</h2><img src="/2025/04/22/Ingress/image-20250423161359858.png" srcset="/img/loading.gif" lazyload class="" title="image-20250423161359858">

<table>
<thead>
<tr>
<th align="left">特性</th>
<th align="left">L4 Service (LoadBalancer 类型)</th>
<th align="left">L7 Ingress (通过 Ingress Controller)</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>核心层级</strong></td>
<td align="left">L4 (Transport Layer - TCP&#x2F;UDP)</td>
<td align="left">L7 (Application Layer - HTTP&#x2F;HTTPS)</td>
</tr>
<tr>
<td align="left"><strong>架构</strong></td>
<td align="left">每个 Service <strong>独占</strong>一个外部 LB -&gt; NodePort -&gt; Pod</td>
<td align="left"><strong>共享</strong>外部 LB -&gt; NodePort -&gt; Ingress Controller -&gt; Service -&gt; Pod</td>
</tr>
<tr>
<td align="left"><strong>资源成本</strong></td>
<td align="left"><strong>高</strong> (每个 Service 一个 LB)</td>
<td align="left"><strong>低</strong> (多个 Service 共享一个 LB)</td>
</tr>
<tr>
<td align="left"><strong>DNS 管理</strong></td>
<td align="left"><strong>复杂</strong> (每个 LB 一个 DNS&#x2F;IP)</td>
<td align="left"><strong>简单</strong> (少数 DNS 指向共享 LB)</td>
</tr>
<tr>
<td align="left"><strong>TLS 处理</strong></td>
<td align="left"><strong>分散</strong> (在 Pod 应用内部处理)</td>
<td align="left"><strong>集中</strong> (在 Ingress Controller 处理 TLS Termination)</td>
</tr>
<tr>
<td align="left"><strong>路由能力</strong></td>
<td align="left">基础 IP&#x2F;Port 路由</td>
<td align="left">丰富 L7 路由 (Host, Path)</td>
</tr>
<tr>
<td align="left"><strong>协议支持</strong></td>
<td align="left">TCP&#x2F;UDP</td>
<td align="left">主要 HTTP&#x2F;HTTPS (部分 Controller 支持 TCP&#x2F;UDP)</td>
</tr>
<tr>
<td align="left"><strong>网络跳数</strong></td>
<td align="left">较少</td>
<td align="left"><strong>增加</strong> (引入 Ingress Controller Hop)</td>
</tr>
<tr>
<td align="left"><strong>系统复杂性</strong></td>
<td align="left">相对简单</td>
<td align="left"><strong>增加</strong> (需部署和维护 Ingress Controller)</td>
</tr>
</tbody></table>
<p><strong>结论</strong>:</p>
<ul>
<li>对于 <strong>HTTP&#x2F;HTTPS</strong> 服务，<strong>L7 Ingress</strong> 通常是<strong>更优</strong>的选择 (成本、管理、功能)。</li>
<li>对于 <strong>非 HTTP&#x2F;HTTPS 的 TCP&#x2F;UDP</strong> 服务，或对<strong>延迟极其敏感</strong>、不需 L7 功能的场景，<strong>L4 LoadBalancer Service</strong> 可能更合适。</li>
</ul>
<h1 id="什么是-Ingress？"><a href="#什么是-Ingress？" class="headerlink" title="什么是 Ingress？"></a>什么是 Ingress？</h1><p>从概念上讲，<strong>Ingress 不是一个具体的服务或进程，而是一个 Kubernetes API 对象</strong>。它定义了一系列<strong>路由规则</strong>，描述了外部 HTTP&#x2F;S 流量应该如何被转发到集群内部的 Services。你可以把它想象成一个<strong>虚拟主机</strong>或者<strong>反向代理</strong>的配置蓝图。</p>
<p>这个 <code>Ingress</code> 资源本身<strong>并不具备处理流量的能力</strong>。它仅仅是规则的集合。真正让这些规则生效的是 <strong>Ingress Controller</strong>。</p>
<h2 id="1-Ingress-Controller-的角色与原理"><a href="#1-Ingress-Controller-的角色与原理" class="headerlink" title="1. Ingress Controller 的角色与原理"></a>1. Ingress Controller 的角色与原理</h2><p><strong>Ingress Controller</strong> 是实际处理流量、实现 Ingress 规则的关键组件。它是一个运行在集群中的 <strong>Controller</strong>（通常以 Deployment 或 DaemonSet 的形式运行），持续<strong>监听（Watch）</strong> Kubernetes API Server 中 Ingress、Service、Endpoint、Secret（用于 TLS）等资源的变化。</p>
<p>其核心工作机制是一个典型的 <strong>Control Loop（控制循环）</strong>：</p>
<ol>
<li><strong>Observe</strong>: 监测集群中 Ingress 资源的<strong>期望状态（Desired State）</strong>，例如用户定义了 <code>www.foo.com/app</code> 应该路由到 <code>service-a</code>。</li>
<li><strong>Compare</strong>: 获取当前<strong>实际状态（Actual State）</strong>。这通常意味着检查其管理的底层负载均衡器（可能是 Nginx、HAProxy、Envoy 或云厂商的 LB）的配置。</li>
<li><strong>Act</strong>: 如果实际状态与期望状态不符，Ingress Controller 就会采取行动，<strong>修改底层负载均衡器的配置</strong>，使其与 Ingress 资源中定义的规则保持一致。例如，如果用户创建了一个新的 Ingress 规则，Nginx Ingress Controller 就会更新 Nginx 的配置文件（<code>nginx.conf</code>）并重新加载 Nginx 配置，使得新的路由规则生效。</li>
</ol>
<p>所以，Ingress Controller 负责：</p>
<ul>
<li><strong>实现路由规则</strong>：根据 Ingress 资源中的 <code>host</code> 和 <code>path</code> 配置，将流量正确代理到后端 Service。</li>
<li><strong>负载均衡</strong>：将流量分发到目标 Service 关联的多个 Pod 上。这通常是基于 Service 发现的 Pod IP 列表进行负载均衡。</li>
<li><strong>SSL&#x2F;TLS 终止</strong>：可以配置 Ingress 来处理 HTTPS 请求，解密 TLS 流量，然后将未加密的 HTTP 流量转发给后端服务，减轻后端服务的负担。这需要引用包含 TLS 证书和私钥的 Secret。</li>
<li><strong>配置边缘路由器&#x2F;负载均衡器</strong>：在某些场景下（尤其是裸金属或私有云），Ingress Controller 可能直接配置物理或软件边缘负载均衡设备。在公有云上，它通常管理一个 <code>LoadBalancer</code> 类型的 Service 作为流量入口点，或者直接与云厂商的 L7 负载均衡器 API 交互。</li>
<li><strong>(可选) DNS 配置</strong>：某些高级的 Ingress Controller 或相关工具（如 <code>external-dns</code>）可以根据 Ingress 的 <code>host</code> 自动更新 DNS 记录，将域名指向 Ingress Controller 的入口 IP。</li>
</ul>
<p>常见的 Ingress Controller 实现包括：<code>ingress-nginx</code> (社区维护), <code>Traefik</code>, <code>HAProxy Ingress</code>, 以及各大云厂商提供的原生 Ingress Controller (如 AWS Load Balancer Controller, GKE Ingress Controller)。</p>
<h2 id="2-工作流程与配置示例"><a href="#2-工作流程与配置示例" class="headerlink" title="2. 工作流程与配置示例"></a>2. 工作流程与配置示例</h2><img src="/2025/04/22/Ingress/image-20250423165931807.png" srcset="/img/loading.gif" lazyload class="" title="image-20250423165931807">

<p>结合图片中的流程，一个典型的使用 Ingress 的工作流如下：</p>
<ol>
<li><p><strong>创建应用 Deployment</strong>: 定义你的应用程序容器和所需的副本数量。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">kubectl create deployment my-app --image=nginx-basic --replicas=3<br></code></pre></td></tr></table></figure>
</li>
<li><p><strong>创建 Service</strong>: 为 Deployment 创建一个 Service，通常类型为 <code>ClusterIP</code>，因为流量将通过 Ingress Controller 代理进来，不需要直接从外部或节点访问 Service IP。Service 的作用是提供一个稳定的内部 IP 和端口，并发现后端 Pods。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 暴露 Deployment &#x27;my-app&#x27; 在端口 8080，创建名为 &#x27;my-app-service&#x27; 的 ClusterIP Service</span><br>kubectl expose deployment my-app --port=80 --target-port=8080 --name=nginx-basic --<span class="hljs-built_in">type</span>=ClusterIP<br></code></pre></td></tr></table></figure>

<p>对应的 Service YAML 可能如下：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-basic</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">selector:</span><br>    <span class="hljs-attr">app:</span> <span class="hljs-string">nginx-basic</span> <span class="hljs-comment"># 确保这里的标签与 Deployment Pod 的标签匹配</span><br>  <span class="hljs-attr">ports:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span><br>      <span class="hljs-attr">port:</span> <span class="hljs-number">80</span>       <span class="hljs-comment"># Service 监听的端口</span><br>      <span class="hljs-attr">targetPort:</span> <span class="hljs-number">8080</span> <span class="hljs-comment"># Pod 容器实际监听的端口</span><br>  <span class="hljs-attr">type:</span> <span class="hljs-string">ClusterIP</span><br></code></pre></td></tr></table></figure>
</li>
<li><p><strong>定义 Ingress 资源</strong>: 创建一个 Ingress 对象，定义路由规则。</p>
<p><em>注意：图片中的 <code>apiVersion: extensions/v1beta1</code> 已经*<em>废弃</em></em>。现在应该使用 <code>networking.k8s.io/v1</code>。*</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs YAML"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">networking.k8s.io/v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Ingress</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">example-ingress</span><br>  <span class="hljs-comment"># Annotations 用于向 Ingress Controller 提供额外的配置指令</span><br>  <span class="hljs-comment"># 例如，对于 nginx-ingress，可以指定重写规则、SSL 配置等</span><br>  <span class="hljs-comment"># annotations:</span><br>  <span class="hljs-comment">#   nginx.ingress.kubernetes.io/rewrite-target: /</span><br>  <span class="hljs-comment">#   kubernetes.io/ingress.class: &quot;nginx&quot; # 如果有多个 Ingress Controller，指定使用哪一个</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-comment"># ingressClassName: nginx # 另一种指定 Ingress Controller 的方式 (v1.18+)</span><br>  <span class="hljs-attr">rules:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">host:</span> <span class="hljs-string">&quot;cncamp.com&quot;</span> <span class="hljs-comment"># 基于域名的路由</span><br>    <span class="hljs-attr">http:</span><br>      <span class="hljs-attr">paths:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">path:</span> <span class="hljs-string">/app</span> <span class="hljs-comment"># 基于路径的路由</span><br>        <span class="hljs-comment"># pathType 定义路径匹配方式: Prefix (前缀匹配) 或 Exact (精确匹配) 或 ImplementationSpecific</span><br>        <span class="hljs-attr">pathType:</span> <span class="hljs-string">Prefix</span><br>        <span class="hljs-attr">backend:</span><br>          <span class="hljs-attr">service:</span><br>            <span class="hljs-comment"># 指向上面创建的 Service</span><br>            <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-basic</span><br>            <span class="hljs-attr">port:</span><br>              <span class="hljs-comment"># 指向 Service 暴露的端口 (spec.ports.port)</span><br>              <span class="hljs-attr">number:</span> <span class="hljs-number">80</span><br>  <span class="hljs-comment"># 可以添加 TLS 配置来启用 HTTPS</span><br>  <span class="hljs-comment"># tls:</span><br>  <span class="hljs-comment"># - hosts:</span><br>  <span class="hljs-comment">#   - www.example.com</span><br>  <span class="hljs-comment">#   secretName: example-tls-secret # 包含证书和私钥的 Secret 名称</span><br></code></pre></td></tr></table></figure>

<p><strong>关键字段解释</strong>:</p>
<ul>
<li><code>apiVersion: networking.k8s.io/v1</code>: 当前推荐使用的 API 版本。</li>
<li><code>kind: Ingress</code>: 资源类型。</li>
<li><code>metadata.name</code>: Ingress 资源的名称。</li>
<li><code>metadata.annotations</code>: 为特定的 Ingress Controller 提供额外配置。</li>
<li><code>spec.ingressClassName</code>: (v1.18+) 显式指定处理此 Ingress 的 Controller 名称。</li>
<li><code>spec.rules</code>: 包含路由规则的列表。</li>
<li><code>spec.rules[].host</code>: 可选，用于基于 Host header 进行路由。如果省略，规则适用于所有到达 Ingress Controller IP 的请求。</li>
<li><code>spec.rules[].http.paths</code>: 定义路径及其对应的后端服务。</li>
<li><code>spec.rules[].http.paths[].path</code>: URL 路径。</li>
<li><code>spec.rules[].http.paths[].pathType</code>: 路径匹配类型 (<code>Prefix</code>, <code>Exact</code>, <code>ImplementationSpecific</code>)。<strong><code>Prefix</code> 是最常用的</strong>。</li>
<li><code>spec.rules[].http.paths[].backend.service.name</code>: 后端 Service 的名称。</li>
<li><code>spec.rules[].http.paths[].backend.service.port.number</code>: 后端 Service 暴露的端口号。</li>
<li><code>spec.tls</code>: 可选，用于配置 TLS 终止。需要指定域名列表 (<code>hosts</code>) 和包含证书&#x2F;私钥的 <code>secretName</code>。</li>
</ul>
</li>
</ol>
<p>当这个 Ingress 资源被创建后，运行中的 Ingress Controller (假设是 Nginx Ingress Controller 且它被配置为处理这个 Ingress) 会检测到它，解析规则，并更新其 Nginx 配置，使得访问 <code>http://&lt;Ingress-Controller-IP&gt;/app</code> (如果 Host 匹配 <code>www.example.com</code>) 的流量被转发到 <code>my-app-service</code> 的 80 端口，最终由 <code>kube-proxy</code> 通过 iptables 或 IPVS 规则将流量负载均衡到 <code>my-app</code> Deployment 的某个 Pod 的 8080 端口上。</p>
<p>总结来说，Ingress 提供了一种强大且标准化的方式来管理 Kubernetes 集群的入口流量，实现了基于主机名和路径的智能路由、SSL&#x2F;TLS 终止以及通过单一入口点服务多个应用程序，是构建生产级 Web 服务的重要组件。</p>
<h1 id="传统应用网络拓扑与演进"><a href="#传统应用网络拓扑与演进" class="headerlink" title="传统应用网络拓扑与演进"></a>传统应用网络拓扑与演进</h1><img src="/2025/04/22/Ingress/image-20250423172643606.png" srcset="/img/loading.gif" lazyload class="" title="image-20250423172643606">

<h2 id="传统分层负载均衡架构"><a href="#传统分层负载均衡架构" class="headerlink" title="传统分层负载均衡架构"></a>传统分层负载均衡架构</h2><ul>
<li><strong>全局层 (GTM&#x2F;DNS):</strong> 用户请求首先通过 <strong>DNS</strong> 解析。在多区域部署中，通常会有一个<strong>全局流量管理器 (GTM)</strong> 或智能 DNS 服务。它的核心职责是根据用户的地理位置、服务器健康状况、网络延迟等因素，将用户请求导向最合适的区域入口（Region 1, 2, or 3）。这解决了<strong>跨地域容灾和就近访问</strong>的问题。</li>
<li><strong>区域接入层 (Web Tier LB)</strong>:每个区域内部，通常有一个面向公网的负载均衡器 (Load Balancer)，称为接入层或 Web 层 LB。它接收来自 GTM&#x2F;DNS 的流量，主要负责：<ul>
<li><strong>公网 IP 暴露:</strong> 提供单一的公网入口点。</li>
<li><strong>SSL&#x2F;TLS 卸载:</strong> 处理 HTTPS 加解密，减轻后端服务器的负担。</li>
<li><strong>基础安全防护:</strong> 可能集成 WAF (Web Application Firewall) 功能。</li>
<li><strong>流量分发:</strong> 将请求分发给下一层的应用负载均衡器。</li>
</ul>
</li>
<li><strong>应用层 (App Tier LB)</strong>: 在接入层之后，通常还会有应用层负载均衡器。它更接近实际的应用实例，负责：<ul>
<li><strong>服务发现:</strong> 感知后端应用实例的健康状态和地址。</li>
<li><strong>负载均衡策略:</strong> 根据轮询、最少连接、响应时间等策略将请求分发到健康的后端实例 (Inst1, Inst2, etc.)。</li>
<li><strong>会话保持:</strong> 对于需要状态保持的应用，确保来自同一用户的请求被转发到同一个后端实例。</li>
</ul>
</li>
<li><strong>后端实例 (Instances):</strong> 最终处理用户业务逻辑的应用服务器实例。</li>
</ul>
<p>这种架构清晰地划分了不同层级的网络职责。然而，它的主要<strong>痛点</strong>在于：<strong>配置复杂、弹性不足、与应用生命周期解耦</strong>。每当需要上线新服务或扩缩容时，往往需要手动配置各层 LB，流程繁琐且易出错，尤其是在需要频繁变更的现代应用开发模式下。云厂商虽然提供了 LBaaS (Load Balancer as a Service)，简化了硬件维护，但配置和集成逻辑依然存在。</p>
<h2 id="Kubernetes-Ingress-Controller：云原生网关"><a href="#Kubernetes-Ingress-Controller：云原生网关" class="headerlink" title="Kubernetes Ingress Controller：云原生网关"></a>Kubernetes Ingress Controller：云原生网关</h2><p>Kubernetes 的出现极大地改变了应用的部署和管理方式。为了解决传统 LB 在动态环境下的痛点，并以 K8s 原生的方式管理集群入口流量，<strong>Ingress</strong> 和 <strong>Ingress Controller</strong> 应运而生。</p>
<h3 id="Ingress-Controller-解决了什么问题？"><a href="#Ingress-Controller-解决了什么问题？" class="headerlink" title="Ingress Controller 解决了什么问题？"></a>Ingress Controller 解决了什么问题？</h3><ol>
<li><strong>动态配置:</strong> Ingress Controller 监听 Kubernetes API Server，根据 <strong>Ingress 资源</strong>的变化自动更新负载均衡和路由规则，无需手动干预，紧密贴合应用的生命周期。</li>
<li><strong>成本效益:</strong> 避免为每个需要对外暴露的 Service 都创建一个昂贵的 <code>LoadBalancer</code> 类型的 Service。通过一个（或一组）Ingress Controller Pod 和一个外部 LB (通常是 <code>LoadBalancer</code> Service)，可以管理大量服务的入口流量。</li>
<li><strong>统一入口:</strong> 为集群内的多个 Service 提供一个统一的 HTTP&#x2F;S 流量入口点。</li>
<li><strong>丰富路由规则:</strong> 支持基于主机名 (Host-based routing) 和路径 (Path-based routing) 的 L7 路由，比 Service 的 L4 负载均衡更灵活。</li>
<li><strong>标准化:</strong> Ingress API 是 K8s 的标准资源，虽然具体实现由不同的 Ingress Controller (Nginx Ingress, Traefik, HAProxy Ingress, APISIX Ingress 等) 提供，但定义规则的方式是统一的。</li>
</ol>
<h3 id="Ingress-Controller-是什么？"><a href="#Ingress-Controller-是什么？" class="headerlink" title="Ingress Controller 是什么？"></a>Ingress Controller 是什么？</h3><p><strong>Ingress Controller</strong> 并非 Kubernetes 内置的功能，而是一个需要<strong>单独部署在集群中的应用程序 (通常是一组 Pod)</strong>。它本质上是一个<strong>反向代理和负载均衡器</strong>，持续监听（Watch）Kubernetes API Server 中 <strong>Ingress</strong>、<strong>Service</strong>、<strong>Endpoint</strong>、<strong>Secret</strong> (用于 TLS) 等资源的变化。当这些资源发生变化时，Ingress Controller 会<strong>动态地将这些变化转换成其内部代理引擎（如 Nginx、HAProxy、Envoy）的配置</strong>，并应用这些配置，从而实现将集群外部的 HTTP&#x2F;S 请求路由到集群内部正确的 Service 上。</p>
<p><strong>Ingress 资源</strong>本身只是一个<strong>规则集合</strong>，它定义了如何将外部请求（例如 <code>http://example.com/app</code>）路由到哪个内部 Service（例如 <code>my-app-service:8080</code>）。真正让这些规则生效的是运行中的 <strong>Ingress Controller</strong>。</p>
<h3 id="Ingress-Controller-的工作原理（三步构建解析）"><a href="#Ingress-Controller-的工作原理（三步构建解析）" class="headerlink" title="Ingress Controller 的工作原理（三步构建解析）"></a>Ingress Controller 的工作原理（三步构建解析）</h3><p>图片下半部分勾勒了构建一个 Ingress Controller 的核心步骤和关键机制，这通常涉及到与 Kubernetes API 的深度交互以及可能的云平台集成。</p>
<h4 id="1-复用-管理外部负载均衡器接口-通常通过-Service-类型-LoadBalancer"><a href="#1-复用-管理外部负载均衡器接口-通常通过-Service-类型-LoadBalancer" class="headerlink" title="1. 复用&#x2F;管理外部负载均衡器接口 (通常通过 Service 类型 LoadBalancer)"></a>1. 复用&#x2F;管理外部负载均衡器接口 (通常通过 <code>Service</code> 类型 <code>LoadBalancer</code>)</h4><ul>
<li><strong>目标:</strong> 为 Ingress Controller Pods 创建一个稳定的外部入口点（一个 <strong>VIP - Virtual IP Address</strong>）。</li>
<li>机制:<ul>
<li>Ingress Controller 本身通常会创建一个 Kubernetes <strong><code>Service</code></strong> 对象，其类型被设置为 <strong><code>LoadBalancer</code></strong>。</li>
<li>Kubernetes 集群中运行的 <strong><code>cloud-controller-manager</code></strong> (如果是云环境) 或类似组件会监听到这个 <code>LoadBalancer</code> Service 的创建请求。</li>
<li><code>cloud-controller-manager</code> 会调用<strong>云服务商的 API</strong> (例如 AWS ELB, GCP Cloud Load Balancer, Azure Load Balancer) 来<strong>实际创建一个外部负载均衡器</strong>。</li>
<li>云平台会为这个 LB 分配一个公网 IP (VIP)。</li>
<li><code>cloud-controller-manager</code> 将获取到的公网 IP 写回到 <code>LoadBalancer</code> Service 的 <code>status.loadBalancer.ingress</code> 字段中。</li>
<li>图片中列出的 Go 函数签名（如 <code>EnsureLoadBalancer</code>, <code>GetLoadBalancer</code>, <code>UpdateLoadBalancer</code>）正是 <code>cloud-controller-manager</code> 或类似组件与云平台 LB API 交互时可能使用到的<strong>接口抽象</strong>。这些函数封装了创建、查询、更新底层云 LB 资源的逻辑。<strong>Ingress Controller 可能直接或间接（通过创建 <code>LoadBalancer</code> Service）触发这些操作</strong>，以确保外部 LB 能将流量正确转发到 Ingress Controller 的 Pods 所在的节点端口（NodePort）或直接转发到 Pod IP（如果 CNI 和云平台支持）。</li>
</ul>
</li>
<li><strong>关键点:</strong> 这是将外部流量引入集群的第一跳，指向的是 Ingress Controller 服务本身，而不是最终的应用服务。</li>
</ul>
<h4 id="2-定义-Informer，监控-K8s-资源变化"><a href="#2-定义-Informer，监控-K8s-资源变化" class="headerlink" title="2. 定义 Informer，监控 K8s 资源变化"></a>2. 定义 Informer，监控 K8s 资源变化</h4><ul>
<li><strong>目标:</strong> 实时感知 Ingress 规则、后端服务(Service)、服务实例(Endpoint)、证书(Secret) 等的变化。</li>
<li>机制:<ul>
<li>Ingress Controller 使用 Kubernetes 官方 Go 客户端库 <strong><code>client-go</code></strong> 中的 <strong>Informer</strong> 机制。</li>
<li><strong>Informer</strong> 提供了高效的<strong>事件驱动模型</strong>来监控资源变化。它内部维护了一个<strong>本地缓存 (Store)</strong>（如代码中的 <code>lbc.ingLister.Store</code>），并通过与 API Server 建立 <strong>Watch</strong> 连接来接收实时的变更事件（Add, Update, Delete）。</li>
<li>开发者为关心的资源类型（<code>Ingress</code>, <code>Service</code>, <code>Endpoint</code>, <code>Secret</code> 等）分别创建 Informer（如 <code>lbc.ingController</code>, <code>lbc.svcInformer</code>）。</li>
<li>为每个 Informer 注册<strong>事件处理函数 (ResourceEventHandlerFuncs)</strong>，如 <code>AddFunc</code>, <code>UpdateFunc</code>, <code>DeleteFunc</code>。</li>
<li>当事件发生时（例如，用户 <code>kubectl apply -f my-ingress.yaml</code>），对应的事件处理函数被触发。</li>
<li><strong>关键模式:</strong> 事件处理函数通常<strong>不会立即执行复杂的业务逻辑</strong>，而是将发生变化的对象（或其 key）放入一个<strong>工作队列 (Work Queue)</strong> 中（如代码中的 <code>lbc.enqueueIngress(obj)</code>）。这实现了<strong>变更检测与处理逻辑的解耦</strong>，并能处理事件的突发和合并。</li>
</ul>
</li>
<li><strong>Golang 源码关联:</strong> <code>client-go/tools/cache</code> 包是 Informer 机制的核心实现，包括 <code>SharedInformerFactory</code>, <code>Store</code>, <code>Controller</code> (这里的 Controller 指的是 Informer 的内部控制器，非 Ingress Controller 本身) 等。</li>
</ul>
<h4 id="3-启动-Worker，处理队列中的变更事件"><a href="#3-启动-Worker，处理队列中的变更事件" class="headerlink" title="3. 启动 Worker，处理队列中的变更事件"></a>3. 启动 Worker，处理队列中的变更事件</h4><ul>
<li><p><strong>目标:</strong> 从工作队列中取出变更项，执行实际的配置更新逻辑（<strong>Reconciliation Loop</strong>）。</p>
</li>
<li><p>机制:</p>
<ul>
<li><p>Ingress Controller 会启动一个或多个 <strong>Worker Goroutine(s)</strong>。</p>
</li>
<li><p>每个 Worker 不断地从工作队列中获取需要处理的对象 key。</p>
</li>
<li><p>对于每个 key，Worker 执行</p>
<p>核心的调和（Reconcile）逻辑</p>
<p>：</p>
<ol>
<li><strong>获取最新状态:</strong> 从 Informer 的本地缓存中获取与 key 相关的最新资源对象（Ingress, Service, Endpoints 等）。</li>
<li><strong>计算期望状态:</strong> 根据获取到的资源对象（特别是 Ingress 规则），计算出底层代理（如 Nginx）应该具有的配置。</li>
<li><strong>应用配置:</strong> 将计算出的配置应用到 Ingress Controller Pod 内运行的代理实例上。这可能涉及生成新的配置文件（如 <code>nginx.conf</code>）并触发代理重新加载（<code>nginx -s reload</code>）。</li>
<li><strong>(可选) 更新 DNS:</strong> 如果 Ingress 配置了特定的域名，并且集成了 ExternalDNS 等组件，Worker 可能需要调用 DNS 服务商的 API 来创建或更新 A 记录，将其指向在步骤 1 中获取的外部 LB 的 VIP。 (对应图片中的 “为 Ingress Domain 创建 DNS A record”)</li>
<li><strong>更新 Ingress 状态:</strong> 将外部 LB 的 IP 或主机名更新回 Ingress 资源的 <code>status.loadBalancer.ingress</code> 字段，这样用户可以通过 <code>kubectl get ingress</code> 查看到入口地址。(对应图片中的 “更新 Ingress 状态”)</li>
</ol>
</li>
</ul>
</li>
<li><p><strong>关键点:</strong> 这个持续的“监听-入队-处理-更新”循环，确保了 Ingress Controller 管理的代理配置始终与 Kubernetes 集群中定义的 Ingress 资源状态保持一致。</p>
</li>
</ul>
<h2 id="BGP-的作用：连接-Kubernetes-与物理网络"><a href="#BGP-的作用：连接-Kubernetes-与物理网络" class="headerlink" title="BGP 的作用：连接 Kubernetes 与物理网络"></a>BGP 的作用：连接 Kubernetes 与物理网络</h2><p>在传统的网络模型中，Kubernetes 集群通常运行在一个覆盖网络（Overlay Network）之上，例如 Flannel (VXLAN) 或 Calico (IPIP 或 VXLAN)。这种模型下，Pod 的 IP 地址只在集群内部可见，当需要将服务暴露给外部，或者 Pod 需要直接访问外部网络时，通常需要经过 <strong>网络地址转换 (NAT)</strong>，这发生在 Kubernetes 节点的层面。例如，一个 <code>type=LoadBalancer</code> 的 Service 或者 Ingress Controller 会监听在节点的某个端口上，外部流量到达节点 IP 和端口后，再由 <code>kube-proxy</code>（或其替代者如 Cilium 的 eBPF 程序）转发给后端的 Pod。</p>
<p>这种模式虽然能工作，但在大规模、高性能或需要与现有网络基础设施深度集成的场景下，会遇到一些<strong>挑战</strong>：</p>
<ol>
<li><strong>NAT 性能瓶颈和复杂性</strong>：大量的 NAT 转换会消耗节点的 CPU 资源，并且 <code>kube-proxy</code> 的 iptables&#x2F;ipvs 规则可能变得非常庞大和复杂，难以调试和维护。同时，源 IP 地址经过 NAT 后会丢失，给追踪和安全策略带来困难（虽然有 <code>externalTrafficPolicy: Local</code> 这样的缓解措施，但并非完美）。</li>
<li><strong>Overlay 网络的封装开销</strong>：VXLAN 或 IPIP 封装会增加数据包的头部大小，降低有效载荷的传输效率，并可能对网络设备的处理能力提出更高要求。</li>
<li><strong>网络集成困难</strong>：Overlay 网络使得 Kubernetes 集群像一个“黑盒子”，物理网络对内部的 Pod IP 和 Service IP 一无所知，难以实现精细化的路由控制和网络策略。</li>
</ol>
<p><strong>BGP (Border Gateway Protocol)</strong> 在 Kubernetes 数据中心中的核心作用就是<strong>解决上述问题，实现 Kubernetes 集群网络与底层物理网络的直接集成</strong>。它允许 Kubernetes 将 <strong>Service IP</strong>（特别是 <code>type=LoadBalancer</code> 分配的 External IP）甚至 <strong>Pod IP 地址段 (CIDR)</strong> 直接宣告到物理网络中。物理网络路由器（通常是 <strong>ToR - Top of Rack 交换机</strong> 或更上层的路由设备）学习到这些路由后，就可以直接将流量路由到对应的 Kubernetes 节点，<strong>无需经过中间的 NAT 或复杂的 Overlay 封装</strong>（在某些 BGP 方案下，例如 Calico BGP）。</p>
<p><strong>总结来说，BGP 在 K8s 数据中心的主要作用包括：</strong></p>
<ul>
<li><strong>原生 Service LoadBalancer 实现</strong>：允许像 MetalLB 这样的项目直接将 <code>LoadBalancer</code> Service 的 External IP 宣告到物理网络，使得外部流量可以直接路由到持有该 Service IP 的一个或多个节点上（通常配合 ECMP - Equal-Cost Multi-Path 实现负载均衡和高可用）。</li>
<li><strong>消除或减少 NAT</strong>：当 Pod IP 或 Service IP 被直接宣告后，外部流量可以不经过 SNAT&#x2F;DNAT 直接到达目标 Pod 或 Service Endpoint 所在的节点，保留了原始的源 IP 地址，简化了网络路径。</li>
<li><strong>高性能 Pod 网络</strong>：配合像 Calico 这样的 CNI 插件，可以直接将每个节点的 Pod CIDR 宣告给物理网络。这样，节点间或节点与外部通信时，数据包可以直接使用 Pod IP 进行路由，无需 IPIP 或 VXLAN 封装，降低了延迟和开销。</li>
<li><strong>与物理网络策略集成</strong>：物理网络设备可以直接看到 K8s 的 Service IP 或 Pod IP，可以基于这些 IP 应用更精细化的访问控制、QoS 策略等。</li>
</ul>
<h2 id="BGP-路由宣告的原理与过程"><a href="#BGP-路由宣告的原理与过程" class="headerlink" title="BGP 路由宣告的原理与过程"></a>BGP 路由宣告的原理与过程</h2><p>BGP 是一个基于 TCP 的路径矢量路由协议，工作在 TCP 端口 179。其核心是通过 <strong>BGP Speaker</strong>（运行 BGP 协议的实体，可以是路由器或服务器上的软件进程）之间建立 <strong>Peering 会话</strong> 来交换<strong>网络可达性信息 (NLRI - Network Layer Reachability Information)</strong>，也就是 IP 地址前缀（例如 <code>192.168.10.1/32</code> 或 <code>10.244.1.0/24</code>）。</p>
<p>在 Kubernetes 场景下，路由宣告过程通常如下：</p>
<ol>
<li><p><strong>部署 BGP Agent&#x2F;Controller</strong>：需要在 Kubernetes 集群中部署能够运行 BGP 协议的组件。</p>
<ul>
<li>对于 <strong>Service IP 宣告</strong> (如 MetalLB)：通常有一个 Controller 负责分配 External IP，并有运行在每个（或部分）节点上的 <strong>Speaker</strong> 进程（通常是基于成熟的开源 BGP 实现如 <strong>GoBGP</strong> 或 <strong>FRRouting&#x2F;Bird</strong>）。</li>
<li>对于 <strong>Pod IP 宣告</strong> (如 Calico)：Calico 的节点 Agent (Felix&#x2F;Typha) 内部集成了 BGP 功能（也通常基于 Bird 或 GoBGP），负责宣告本节点的 Pod CIDR。</li>
</ul>
</li>
<li><p><strong>配置 BGP Peering</strong>：管理员需要配置 Kubernetes 集群中的 BGP Agent&#x2F;Speaker 与物理网络中的路由器（通常是 ToR 交换机）建立 BGP Peering 关系。这涉及到：</p>
<ul>
<li><strong>自治系统号 (ASN - Autonomous System Number)</strong>：需要为 K8s 集群（或每个节点）和物理路由器分配合适的 ASN。可以在同一个 ASN 内建立 <strong>iBGP (Internal BGP)</strong> 会话，或者在不同 ASN 间建立 <strong>eBGP (External BGP)</strong> 会话。数据中心内部为了简化配置和避免复杂的路由策略，有时会选择 iBGP，将 K8s 节点和 ToR 放在同一个 ASN 下；或者将 K8s 视为一个独立的 AS，使用 eBGP 与 ToR 对接。</li>
<li><strong>Neighbor (对等体) 地址</strong>：在 K8s BGP Agent 上配置 ToR 路由器的 IP 地址作为 Neighbor，反之亦然，在 ToR 路由器上配置 K8s 节点（或其 BGP Agent 监听的 IP）作为 Neighbor。</li>
<li><strong>认证 (可选)</strong>：可以配置 MD5 密码来增强 BGP 会话的安全性。</li>
</ul>
</li>
<li><p><strong>触发宣告</strong>：</p>
<ul>
<li><strong>Service IP (MetalLB 示例)</strong>：当一个 <code>type=LoadBalancer</code> Service 被创建或分配了 External IP 时，MetalLB Controller 会检测到这个变化。它会选择一个或多个节点来承载这个 IP（基于配置策略，如标签选择器）。然后，Controller 会通知这些选定节点上的 Speaker 进程：“你需要宣告这个 IP 地址 <code>X.X.X.X/32</code>”。</li>
<li><strong>Pod IP (Calico 示例)</strong>：当 Calico CNI 为节点分配了一个 Pod CIDR（例如 <code>10.244.1.0/24</code>）后，该节点上的 Calico BGP Agent 会自动准备宣告这个前缀。</li>
</ul>
</li>
<li><p><strong>发送 BGP UPDATE 消息</strong>：负责宣告的 K8s 节点上的 BGP Agent&#x2F;Speaker 会向其配置的 BGP Peer（物理路由器）发送 <strong>BGP UPDATE</strong> 消息。这个消息包含了：</p>
<ul>
<li><strong>NLRI</strong>：要宣告的 IP 地址前缀（如 <code>192.168.10.1/32</code> 或 <code>10.244.1.0/24</code>）。</li>
<li><strong>Path Attributes (路径属性)</strong>：这是一组描述路由特性的属性，其中最重要的几个是：<ul>
<li><strong>AS_PATH</strong>：路由经过的 AS 列表，用于防止路由环路和选择最佳路径。对于从 K8s 节点发起的宣告，它通常会包含 K8s 集群（或节点）的 ASN。</li>
<li><strong>NEXT_HOP</strong>：指示到达目标前缀下一跳路由器的 IP 地址。<strong>对于从 K8s 节点发起的宣告，这通常设置为该节点自身的 IP 地址</strong>。物理路由器收到这个宣告后，就知道要将去往 <code>X.X.X.X</code> 的流量发送给这个 <code>NEXT_HOP</code> IP 地址（即 K8s 节点）。</li>
<li><strong>ORIGIN</strong>：指示路由来源（IGP、EGP 或 Incomplete）。</li>
<li><strong>MED (Multi-Exit Discriminator), Local Preference</strong> (可选)：用于影响路径选择，实现流量工程。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>物理路由器处理宣告</strong>：物理路由器（如 ToR 交换机）收到来自 K8s 节点的 BGP UPDATE 消息后：</p>
<ul>
<li>验证消息的合法性（例如，基于 Neighbor 配置和认证）。</li>
<li>根据配置的**路由策略（Route Policy &#x2F; Route Map）**决定是否接受这条路由。策略可以基于前缀、AS_PATH 等属性进行过滤。</li>
<li>如果接受，路由器会将这条路由（<code>X.X.X.X/32</code> via <code>Node_IP</code>）添加到自己的 <strong>BGP 路由表</strong>，并通过 BGP 决策过程选出最优路径，最终可能将其安装到<strong>全局路由表 (FIB - Forwarding Information Base)</strong> 中。</li>
<li>路由器可能还会将这条学到的路由<strong>再次宣告</strong>给它的其他 BGP Peer（例如，上行的 Spine 交换机或核心路由器），使得整个数据中心网络甚至外部网络都能知道如何到达这个 K8s Service 或 Pod。</li>
</ul>
</li>
</ol>
<p><strong>关键点</strong>：<strong>NEXT_HOP 属性是实现直接路由的核心</strong>。物理路由器知道将目标 IP 的流量直接发送到宣告该路由的 Kubernetes 节点的 IP 地址。</p>
<p><strong>Golang 源代码视角</strong>：像 MetalLB 的 Speaker 和 Calico 的 BGP Agent 通常使用 Go 语言编写。它们会使用像 <code>gobgp</code> (<a target="_blank" rel="noopener" href="https://github.com/osrg/gobgp">https://github.com/osrg/gobgp</a>) 这样的 Go 库来处理 BGP 协议的细节，如建立 TCP 连接、构造和解析 BGP 消息（OPEN, UPDATE, KEEPALIVE, NOTIFICATION）、管理 BGP 邻居状态机等。这些库内部会调用底层的操作系统网络 API（例如 Linux 的 <code>socket</code> 系统调用）来发送和接收 TCP 数据。同时，它们还需要与 Kubernetes API Server 交互（使用 <code>client-go</code> 库）来获取 Service、Node 等资源信息，并可能与本地 Linux 内核路由表交互（例如通过 <code>netlink</code> 接口）来管理本地路由或 IP 地址。</p>
<hr>
<p><strong>场景</strong>：当你使用 BGP（例如通过 MetalLB）为一个 <code>type=LoadBalancer</code> Service 分配并宣告了一个 External IP（比如 <code>198.51.100.10</code>）之后，这个 IP 地址本身是可路由的，外部客户端可以直接访问。但是，用户通常希望通过一个友好的域名（如 <code>myapp.example.com</code>）来访问服务，而不是记住 IP 地址。</p>
<p><strong>解决方案</strong>：你需要<strong>在你的 DNS 服务中创建一个 DNS 记录</strong>（通常是 A 记录对于 IPv4，AAAA 记录对于 IPv6），将域名 <code>myapp.example.com</code> 指向这个由 BGP 宣告的 IP 地址 <code>198.51.100.10</code>。</p>
<p><strong>自动化</strong>：这个过程可以手动完成，但更常见的是使用 <strong>ExternalDNS</strong> (<a target="_blank" rel="noopener" href="https://github.com/kubernetes-sigs/external-dns">https://github.com/kubernetes-sigs/external-dns</a>) 这样的 Kubernetes 控制器来实现自动化。ExternalDNS 会监控 Kubernetes Service（或 Ingress）资源，当它发现一个 Service 具有 External IP 时，它会自动根据 Service 的注解（Annotation）或配置，在配置好的 DNS 提供商（如 AWS Route 53, Google Cloud DNS, Cloudflare, 或本地 BIND&#x2F;CoreDNS 服务器）中创建或更新对应的 DNS 记录。</p>
<p><strong>配置 DNS 记录，使其指向通过 BGP 宣告和路由的 Kubernetes 服务 IP 地址。</strong> BGP 负责让 IP 可达，DNS 负责将域名解析到这个可达的 IP。两者协同工作，但配置是分开的。</p>
<h2 id="边缘路由器配置示例"><a href="#边缘路由器配置示例" class="headerlink" title="边缘路由器配置示例"></a>边缘路由器配置示例</h2><p>边缘路由器（Edge Router）在这里通常指直接与 Kubernetes 节点建立 BGP Peering 的物理网络设备，最常见的是 <strong>ToR (Top of Rack) 交换机</strong>。配置这些路由器的目的是让它们能够与 K8s 节点上的 BGP Agent 建立邻居关系，并交换路由信息。</p>
<p>以下是一个<strong>通用概念性</strong>的配置示例，语法类似于 Cisco NX-OS 或 Arista EOS，你需要根据你实际使用的网络设备品牌和型号调整具体命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs bash">! 进入配置模式<br>configure terminal<br><br>! 启用 BGP 特性 (如果尚未启用)<br>feature bgp<br><br>! 配置 BGP 路由进程，指定本地 AS 号 (假设 ToR 和 K8s 节点都在 AS 65000)<br>router bgp 65000<br><br>  ! 配置 Router ID (通常是路由器的一个 Loopback 接口 IP)<br>  router-id 10.1.1.1<br><br>  ! 配置与 Kubernetes 节点的 BGP Neighbor (假设 K8s 节点 Node1 的 IP 是 10.0.0.101)<br>  neighbor 10.0.0.101<br>    remote-as 65000         ! 指定邻居的 AS 号 (iBGP 场景，与本地 AS 相同)<br>    description K8S_Node1_BGP<br>    address-family ipv4 unicast ! 启用 IPv4 单播地址族<br>      ! (可选) 配置路由策略，例如只接受特定的前缀长度或应用过滤<br>      ! route-map K8S_IMPORT_POLICY <span class="hljs-keyword">in</span><br>      ! (可选) 如果是 iBGP，可能需要配置 next-hop-self 防止下一跳问题<br>      ! next-hop-self<br>    update-source loopback0  ! (推荐) 使用稳定的 Loopback 接口作为 BGP 源 IP<br><br>  ! 配置与 Kubernetes 节点 Node2 的 BGP Neighbor (假设 IP 是 10.0.0.102)<br>  neighbor 10.0.0.102<br>    remote-as 65000<br>    description K8S_Node2_BGP<br>    address-family ipv4 unicast<br>      ! route-map K8S_IMPORT_POLICY <span class="hljs-keyword">in</span><br>      ! next-hop-self<br>    update-source loopback0<br><br>  ! ... 为所有需要建立 BGP Peering 的 K8s 节点配置 neighbor ...<br><br>! (可选) 定义路由策略 (Route Map) 来过滤或修改从 K8s 接收的路由<br>! route-map K8S_IMPORT_POLICY permit 10<br>!   match ip address prefix-list K8S_ACCEPTED_PREFIXES<br>!   ! (可以设置 local-preference, metric 等)<br><br>! (可选) 定义前缀列表 (Prefix List) 来指定允许接收的 K8s 前缀<br>! ip prefix-list K8S_ACCEPTED_PREFIXES <span class="hljs-built_in">seq</span> 10 permit 198.51.100.0/24 ge 32 le 32  ! 只接受 /32 的主机路由 (如 Service IP)<br>! ip prefix-list K8S_ACCEPTED_PREFIXES <span class="hljs-built_in">seq</span> 20 permit 10.244.0.0/16 ge 24 le 24    ! 只接受 /24 的 Pod CIDR (假设 Calico 使用 /24)<br><br>! 保存配置<br>copy running-config startup-config<br></code></pre></td></tr></table></figure>

<p><strong>关键配置项解释</strong>：</p>
<ul>
<li><code>router bgp &lt;ASN&gt;</code>: 启动 BGP 进程并指定本地 AS 号。</li>
<li><code>neighbor &lt;IP&gt; remote-as &lt;ASN&gt;</code>: 定义一个 BGP 邻居，指定其 IP 地址和 AS 号。<strong>这是建立 Peering 的核心配置</strong>。</li>
<li><code>address-family ipv4 unicast</code>: 指定要在此邻居关系上交换的地址类型（这里是 IPv4 单播路由）。IPv6 则使用 <code>ipv6 unicast</code>。</li>
<li><code>update-source &lt;interface&gt;</code>: （推荐）指定用于建立 BGP TCP 连接的源 IP 地址，通常使用稳定的 Loopback 接口地址，而不是物理接口地址，以提高稳定性。</li>
<li><code>route-map &lt;name&gt; in/out</code>: (可选但推荐) 应用路由策略，<code>in</code> 表示过滤从该邻居接收的路由，<code>out</code> 表示过滤向该邻居宣告的路由。路由策略可以用来<strong>控制路由的传播，增加安全性，并实现流量工程</strong>。</li>
<li><code>ip prefix-list &lt;name&gt;</code>: (可选) 定义 IP 地址前缀列表，常用于路由策略中进行匹配。例如，可以只允许接收来自 K8s 的 <code>/32</code> Service IP 或特定范围内的 Pod CIDR。</li>
</ul>
<p><strong>注意</strong>：</p>
<ul>
<li>上述配置是 <strong>iBGP</strong> (Internal BGP) 的示例，因为 <code>remote-as</code> 与本地 <code>router bgp</code> 的 AS 号相同。如果 K8s 集群使用不同的 AS 号（例如 AS 65001），那么 <code>remote-as</code> 就应该配置为 65001，这将建立 <strong>eBGP</strong> (External BGP) 会话。eBGP 通常用于连接不同的管理域。</li>
<li>确保 K8s 节点与边缘路由器之间 IP 连通性是 BGP 建立的前提。</li>
<li>在 K8s 端（例如 MetalLB 或 Calico 的配置中）也需要做相应的 BGP 配置，指定 ToR 路由器的 IP 和 AS 号作为 Peer。</li>
</ul>
<p>通过 BGP 将 Kubernetes 网络与物理数据中心网络深度集成，是一种强大且可扩展的方式，特别适合需要高性能、低延迟、精细化网络控制的大型或复杂的 Kubernetes 部署.</p>
<h2 id="数据包流向：从外部客户端到-Kubernetes-Pod"><a href="#数据包流向：从外部客户端到-Kubernetes-Pod" class="headerlink" title="数据包流向：从外部客户端到 Kubernetes Pod"></a>数据包流向：从外部客户端到 Kubernetes Pod</h2><p>假设：</p>
<ul>
<li>外部客户端 IP: <code>203.0.113.10</code></li>
<li>Kubernetes Service (<code>type=LoadBalancer</code>) 的 External IP (由 MetalLB 分配并宣告): <code>198.51.100.5</code></li>
<li>宣告 <code>198.51.100.5</code> 的 K8s 节点 (Node A): <code>10.0.0.101</code> (这是与 ToR 交换机互联的 IP)</li>
<li>处理 Service <code>198.51.100.5</code> 的后端 Pod (Pod P1): <code>10.244.1.5</code> (运行在 Node A 上)</li>
<li>ToR 交换机 IP (K8s 节点的网关): <code>10.0.0.1</code></li>
</ul>
<p><strong>过程：</strong></p>
<ol>
<li><p><strong>BGP 宣告与学习</strong>:</p>
<ul>
<li>Node A 上的 BGP Agent (如 MetalLB Speaker) 向 ToR 交换机发送 BGP UPDATE 消息，宣告前缀 <code>198.51.100.5/32</code>，并将 <code>NEXT_HOP</code> 属性设置为 <code>10.0.0.101</code>。</li>
<li>ToR 交换机接收并验证这个宣告，如果策略允许，它会在自己的 BGP 表中添加这条路由，并通过 BGP 决策过程选择它为最佳路径。</li>
<li><strong>关键一步</strong>: ToR 将这条最优 BGP 路由安装到它的 <strong>FIB</strong> 中。FIB 中会有一条类似这样的条目：<strong><code>Destination: 198.51.100.5/32, Next Hop: 10.0.0.101, Outgoing Interface: &lt;连接 Node A 的端口&gt;</code></strong>。</li>
<li>ToR 可能还会将这条路由进一步宣告给上游的 Spine 交换机或核心路由器，使得整个数据中心网络都知道如何到达 <code>198.51.100.5</code>。</li>
</ul>
</li>
<li><p><strong>外部客户端发起请求</strong>:</p>
<ul>
<li>客户端 <code>203.0.113.10</code> 发送一个 IP 数据包，目标 IP 地址为 <code>198.51.100.5</code>。</li>
<li>数据包经过互联网和数据中心边界路由器，最终到达能够路由到 <code>198.51.100.5</code> 的内部路由器，比如直连 K8s 节点的 ToR 交换机。</li>
</ul>
</li>
<li><p><strong>ToR 交换机路由决策</strong>:</p>
<ul>
<li>ToR 交换机收到目标 IP 为 <code>198.51.100.5</code> 的数据包。</li>
<li>它查询自己的 <strong>FIB</strong>。</li>
<li>FIB 查询匹配到条目：<code>Destination: 198.51.100.5/32, Next Hop: 10.0.0.101, Outgoing Interface: &lt;端口 X&gt;</code>。</li>
<li><strong>路由执行</strong>: ToR 将数据包从端口 X 发送出去，目标 MAC 地址是 Node A (<code>10.0.0.101</code>) 的 MAC 地址（通过 ARP 或 NDP 解析得到）。数据包直接被发送到 Node A。</li>
</ul>
</li>
<li><p><strong>Kubernetes 节点 (Node A) 处理</strong>:</p>
<ul>
<li>Node A 的网络接口收到目标 IP 为 <code>198.51.100.5</code> 的数据包。</li>
<li>数据包进入 Linux 内核的网络协议栈。</li>
<li><strong>Service 路由 (kube-proxy &#x2F; CNI)</strong>: 此时，Kubernetes 的 Service 转发机制介入（这通常发生在 Netfilter 的 <code>PREROUTING</code> 或 <code>INPUT</code> 链，或者由 eBPF 程序处理）。<ul>
<li>像 <code>kube-proxy</code> (工作在 IPVS 或 iptables 模式) 或 Cilium&#x2F;Calico (使用 eBPF 或 iptables) 检测到目标 IP <code>198.51.100.5</code> 是一个本地处理的 Service IP。</li>
<li>它根据 Service 的规则进行<strong>负载均衡</strong>，选择一个健康的后端 Pod，比如 Pod P1 (<code>10.244.1.5</code>)。</li>
<li>它执行 <strong>DNAT (Destination Network Address Translation)</strong>，将数据包的目标 IP 地址从 <code>198.51.100.5</code> 修改为 Pod P1 的 IP <code>10.244.1.5</code>。源 IP 地址 (<code>203.0.113.10</code>) 通常保持不变。</li>
</ul>
</li>
<li><strong>本地路由到 Pod</strong>: 内核现在需要将这个修改后的数据包（目标 IP <code>10.244.1.5</code>）路由到 Pod P1。<ul>
<li>内核查询 <strong>Node A 的本地路由表 (FIB)</strong>。在 Linux 上，可以用 <code>ip route show</code> 查看。</li>
<li>因为 Pod P1 就在 Node A 上，通常会有一条指向 Pod P1 网络接口（如 <code>veth</code> 对的一端，或者由 CNI 管理的其他接口）的路由。例如：<code>10.244.1.5 dev &lt;pod_interface_name&gt;</code>。</li>
<li>内核将数据包通过这个接口发送给 Pod P1 内的应用程序。</li>
</ul>
</li>
</ul>
</li>
</ol>
<p><strong>查看 Linux 路由表示例:</strong></p>
<p>在 Kubernetes 节点上，你可以使用以下命令查看路由表：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 显示主路由表</span><br>ip route show<br><br><span class="hljs-comment"># 显示所有路由表 (包括策略路由使用的表)</span><br>ip route show table all<br><br><span class="hljs-comment"># 查看特定 IP 的路由路径</span><br>ip route get 198.51.100.5 <span class="hljs-comment"># 应该显示本地处理或特定接口</span><br>ip route get 10.244.1.5   <span class="hljs-comment"># 应该显示到 Pod 网络接口的路由</span><br>ip route get 8.8.8.8      <span class="hljs-comment"># 应该显示默认网关 (ToR)</span><br></code></pre></td></tr></table></figure>

<h2 id="如果-Pod-IP-也通过-BGP-宣告-例如-Calico-BGP"><a href="#如果-Pod-IP-也通过-BGP-宣告-例如-Calico-BGP" class="headerlink" title="如果 Pod IP 也通过 BGP 宣告 (例如 Calico BGP)"></a>如果 Pod IP 也通过 BGP 宣告 (例如 Calico BGP)</h2><p>在这种情况下，不仅 Service IP，每个节点的 Pod CIDR 也会被宣告到物理网络。</p>
<p>假设 Node A 的 Pod CIDR 是 <code>10.244.1.0/24</code>，Node B 的是 <code>10.244.2.0/24</code>。</p>
<ul>
<li>Node A 会向 ToR 宣告 <code>10.244.1.0/24 via 10.0.0.101</code>。</li>
<li>Node B 会向 ToR 宣告 <code>10.244.2.0/24 via 10.0.0.102</code>。</li>
<li>ToR 的 FIB 会包含这些路由。</li>
</ul>
<p><strong>好处</strong>:</p>
<ol>
<li><strong>节点间 Pod 通信</strong>: 当 Node A 上的 Pod (<code>10.244.1.5</code>) 要访问 Node B 上的 Pod (<code>10.244.2.8</code>) 时：<ul>
<li>Node A 查询其 FIB 找到路由 <code>10.244.2.0/24 via 10.0.0.102</code>（这条路由可能是直接从 Node B 学到的 iBGP 路由，或者从 ToR 反射回来的）。</li>
<li>Node A 直接将数据包（源 IP <code>10.244.1.5</code>, 目标 IP <code>10.244.2.8</code>）发送给 Node B (<code>10.0.0.102</code>)，<strong>无需 Overlay 封装 (如 VXLAN&#x2F;IPIP)</strong>。流量可能经过 ToR，也可能在同一 L2 网络直接发送。</li>
</ul>
</li>
<li><strong>Pod 访问外部</strong>: 当 Pod (<code>10.244.1.5</code>) 访问外部 IP 时：<ul>
<li>数据包（源 IP <code>10.244.1.5</code>）发送到 Node A 的默认网关（ToR <code>10.0.0.1</code>）。</li>
<li>ToR 收到源 IP 为 <code>10.244.1.5</code> 的包。由于 ToR 已经通过 BGP 学习到 <code>10.244.1.0/24</code> 是合法的、可路由的内部地址段，它<strong>不会</strong>因为源地址检查而丢弃这个包。</li>
<li>ToR 将包正常路由出去。<strong>此时 Pod IP 可以直接暴露给外部（如果网络策略允许），或者在数据中心边界进行集中的 NAT，而不是在每个 K8s 节点上做 SNAT</strong>。这保留了源 Pod IP，便于追踪和策略执行。</li>
</ul>
</li>
</ol>
<img src="/2025/04/22/Ingress/image-20250423190212569.png" srcset="/img/loading.gif" lazyload class="" title="image-20250423190212569">

<h1 id="L4-集群架构"><a href="#L4-集群架构" class="headerlink" title="L4 集群架构"></a>L4 集群架构</h1><img src="/2025/04/22/Ingress/image-20250423190233621.png" srcset="/img/loading.gif" lazyload class="" title="image-20250423190233621">

<h1 id="Kubernetes-L4-DSR-负载均衡架构深度解析"><a href="#Kubernetes-L4-DSR-负载均衡架构深度解析" class="headerlink" title="Kubernetes L4 DSR 负载均衡架构深度解析"></a>Kubernetes L4 DSR 负载均衡架构深度解析</h1><p>该架构的核心采用了 <strong>Direct Server Return (DSR)</strong> 模式。这种模式的设计精髓在于，仅有客户端发起的请求流量会经过负载均衡器节点进行调度决策，而服务器响应客户端的流量则绕过负载均衡器，由后端服务节点直接返回给客户端，从而极大地提升了负载均衡集群的吞吐能力和性能。</p>
<h2 id="架构概述与解决的问题"><a href="#架构概述与解决的问题" class="headerlink" title="架构概述与解决的问题"></a>架构概述与解决的问题</h2><p>在现代微服务架构中，服务的可伸缩性和高可用性至关重要。Kubernetes 提供了 Service 资源抽象来暴露应用，但其内置的 <code>kube-proxy</code> 实现（如 iptables 或 IPVS 模式）虽然能实现基本的负载均衡，但在大规模、高吞吐量的场景下可能会遇到性能瓶颈，特别是 <code>kube-proxy</code> 需要处理入向和出向的所有流量。</p>
<p><strong>此 L4 DSR 架构旨在解决以下核心问题：</strong></p>
<ol>
<li><strong>性能瓶颈：</strong> 传统 L4 负载均衡器（如 <code>kube-proxy</code> 或某些云厂商 LB）需要同时处理请求和响应流量，响应流量通常远大于请求流量，这使得负载均衡器本身成为性能瓶颈点。</li>
<li><strong>可伸缩性：</strong> 当后端服务规模扩大时，负载均衡器节点的处理能力需要同步线性增长，成本较高。</li>
<li><strong>源 IP 地址保持：</strong> 某些应用场景需要后端服务能够直接看到客户端的真实源 IP 地址，而某些代理模式（如 SNAT）会隐藏源 IP。</li>
</ol>
<p>通过采用 <strong>DSR 模式</strong>，负载均衡器仅承担轻量的请求分发任务，将繁重的响应流量处理卸载到后端服务节点，<strong>有效解决了负载均衡器自身的性能和伸缩性瓶颈</strong>，并且由于请求路径上 IP 头部通常不被修改（或仅在 L2 层面处理），<strong>后端服务能够直接获取到客户端的真实 IP 地址</strong>。</p>
<h2 id="核心架构组件"><a href="#核心架构组件" class="headerlink" title="核心架构组件"></a>核心架构组件</h2><p>理解该架构需要熟悉其关键组成部分及其在系统中的角色：</p>
<ul>
<li><strong>Client:</strong> 发起服务请求的用户或应用程序。</li>
<li><strong>VIP (Virtual IP Address):</strong> <strong>虚拟 IP 地址</strong>，这是暴露给外部客户端的服务统一入口。客户端访问服务时，目标地址即为 VIP。可以有多个 VIP 对应不同的服务。</li>
<li><strong>Router:</strong> 企业级路由器，负责网络数据包的路由转发。在此架构中，通常配置 <strong>ECMP (Equal-Cost Multi-Path)</strong> 路由。这意味着当路由器收到目标为 VIP 的数据包时，如果有多条等价的路由路径（指向不同的 L4 LB 节点），路由器会根据哈希算法将流量分发到这些路径上，实现了对 L4 负载均衡器本身的负载分担和高可用。路由器通过 <strong>BGP (Border Gateway Protocol)</strong> 等动态路由协议从 L4 LB 节点学习到 VIP 的可达性信息。</li>
<li><strong>k8s minion node (L4 LB 层):</strong> 运行 L4 负载均衡核心逻辑的 Kubernetes 工作节点。这些节点是 DSR 架构中的 <strong>Director</strong> 角色。<ul>
<li><strong>Elb-Director (k8s pod &#x2F; network ns):</strong> 通常以 Pod 的形式部署在 L4 LB 节点上，作为控制平面的代理。它的核心职责是：<ol>
<li>与 <strong>Kubernetes API Server</strong> 通信，通过 <strong>声明式 API</strong> (如监听 Service 和 Endpoints&#x2F;EndpointSlice 资源) 获取服务的 VIP、端口以及后端健康的 Pod IP 列表（Real Server IPs）。</li>
<li>基于获取到的服务配置信息，通过 <strong>Netlink</strong> 接口与运行在同一节点上的 Linux 内核 <strong>IPVS (IP Virtual Server)</strong> 模块交互，动态地创建和更新 IPVS 规则（如添加&#x2F;删除 VIP、添加&#x2F;删除 Real Server）。</li>
</ol>
</li>
<li><strong>Linux Kernel &#x2F; IPVS:</strong> <strong>Linux 内核内置的高性能 L4 负载均衡模块</strong>。IPVS 工作在内核态，利用高效的哈希表存储和查找连接信息，直接进行数据包的转发决策。在 DSR 模式下，IPVS 根据配置的调度算法（如 Round Robin, Least Connection, Weighted Round Robin 等）选择一个后端 Real Server，并<strong>仅修改数据包的 L2 目标 MAC 地址</strong>（在典型的 IPVS DSR MAC 转发模式下），然后将数据包转发给选定的后端服务器，而<strong>不修改 IP 头部的源 IP 和目标 IP</strong>。</li>
</ul>
</li>
<li><strong>k8s minion node (Backend Service 层):</strong> 运行实际业务应用的 Kubernetes 工作节点。<ul>
<li><strong>L7 Proxy &#x2F; Backend Pod (k8s pod):</strong> 部署在这些节点上的后端服务实例（Real Server）。它们接收来自 L4 LB 节点转发的、目标 IP 仍为 VIP 的数据包。图例中虽为 L7 Proxy，但它可以是任何提供 TCP&#x2F;UDP 服务的 Pod。</li>
</ul>
</li>
</ul>
<h2 id="控制路径-Control-Path"><a href="#控制路径-Control-Path" class="headerlink" title="控制路径 (Control Path)"></a>控制路径 (Control Path)</h2><p>控制路径负责配置和管理负载均衡规则，确保其与 Kubernetes 集群中服务的状态保持一致。</p>
<ol>
<li><p>用户通过 <code>kubectl</code> 或其他 K8s 客户端创建或更新 <code>Service</code> 资源（通常是 <code>type=LoadBalancer</code> 或特定 Annotation 的 <code>type=ClusterIP/NodePort</code>，由 Elb-Director 实现支持）。</p>
</li>
<li><p><strong>Elb-Director Pod</strong> 运行在 L4 LB 节点上，它 <strong>Watch</strong> Kubernetes API Server 上的 <code>Service</code> 和 <code>Endpoints</code> (或 <code>EndpointSlice</code>) 资源。</p>
</li>
<li><p>当检测到相关资源变化（如 Service 创建、删除、更新，或者后端 Pod 实例增减、健康状态变化）时，Elb-Director 会解析这些信息，提取出 VIP、端口、协议以及健康的后端 Pod IP 地址列表。</p>
</li>
<li><p>Elb-Director 通过 <strong>Go 语言的 Netlink 库</strong>（或其他语言实现）构造 Netlink 消息，与本节点的 Linux 内核通信。</p>
</li>
<li><p>内核中的 <strong>IPVS 模块</strong> 接收 Netlink 消息，据此<strong>动态地创建、更新或删除 IPVS virtual service (对应 VIP) 和 real server (对应后端 Pod IP) 的配置</strong>。例如，使用 <code>ipvsadm</code> 命令（底层也是通过 Netlink）可以手动查看或配置：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 添加一个 TCP 类型的 VIP 服务，使用轮询调度</span><br><span class="hljs-built_in">sudo</span> ipvsadm -A -t 192.168.100.100:80 -s rr<br><br><span class="hljs-comment"># 为该 VIP 添加一个 Real Server，使用 DSR (Direct Routing) 模式</span><br><span class="hljs-comment"># 注意：ipvsadm 的 &#x27;-g&#x27; 选项代表 Gatewaying，即 DSR 模式</span><br><span class="hljs-built_in">sudo</span> ipvsadm -a -t 192.168.100.100:80 -r 10.0.1.5:8080 -g -w 1<br></code></pre></td></tr></table></figure></li>
</ol>
<p>这条控制路径保证了 L4 LB 的配置与 K8s 服务发现机制的<strong>实时联动和自动化</strong>。</p>
<h2 id="数据路径-Data-Path-DSR-核心流程"><a href="#数据路径-Data-Path-DSR-核心流程" class="headerlink" title="数据路径 (Data Path) - DSR 核心流程"></a>数据路径 (Data Path) - DSR 核心流程</h2><p>数据路径描述了客户端请求和服务器响应的实际流量走向，是理解 DSR 模式的关键。</p>
<h3 id="1-请求-Request-流程-Client-Router-L4-LB-Node-Backend-Node"><a href="#1-请求-Request-流程-Client-Router-L4-LB-Node-Backend-Node" class="headerlink" title="1. 请求 (Request) 流程: Client -&gt; Router -&gt; L4 LB Node -&gt; Backend Node"></a>1. 请求 (Request) 流程: Client -&gt; Router -&gt; L4 LB Node -&gt; Backend Node</h3><ul>
<li><p><strong>Client -&gt; Router:</strong></p>
<ul>
<li>客户端构建 IP 包：<strong>Source IP &#x3D; ClientIP</strong>, <strong>Destination IP &#x3D; VIP</strong>。</li>
<li><code>[ IP(Src:ClientIP, Dst:VIP) | TCP/UDP | Payload ]</code></li>
<li>客户端将包发送至其默认网关（Router）。</li>
</ul>
</li>
<li><p><strong>Router -&gt; L4 LB Node:</strong></p>
<ul>
<li>路由器收到目标为 VIP 的包。</li>
<li>通过 <strong>ECMP</strong> 查询路由表，选择一个可达 VIP 的下一跳 L4 LB 节点（例如 LB-Node1）。</li>
<li>路由器<strong>重写 L2 帧头</strong>：<strong>Destination MAC &#x3D; LB-Node1_MAC</strong>, Source MAC &#x3D; Router_Interface_MAC。IP 头部保持不变。</li>
<li><code>[ Eth(Src:RouterMAC, Dst:LB-Node1_MAC) | IP(Src:ClientIP, Dst:VIP) | TCP/UDP | Payload ]</code></li>
<li>帧被发送到 LB-Node1。</li>
</ul>
</li>
<li><p><strong>L4 LB Node (IPVS 处理):</strong></p>
<ul>
<li>LB-Node1 网卡接收帧，内核网络栈解开 L2 帧头，得到 IP 包。</li>
<li><strong>IPVS 模块</strong> 介入，检查 Destination IP (VIP)。发现匹配已配置的 Virtual Service。</li>
<li>IPVS 根据选定的<strong>调度算法</strong>（如 Least Connection），选择一个健康的 <strong>Real Server (RS)</strong>，假设为 Backend-NodeA 上的 Pod (IP: <code>RS_IP</code>)。</li>
<li><strong>关键 DSR 操作 (MAC 转发模式):</strong> IPVS <strong>不修改 IP 头部 (ClientIP, VIP)</strong>。它直接<strong>重写 L2 帧头</strong>：<strong>Destination MAC &#x3D; RS_MAC</strong> (后端 Pod&#x2F;节点 的 MAC 地址), <strong>Source MAC &#x3D; LB-Node1_MAC</strong>。</li>
<li><code>[ Eth(Src:LB-Node1_MAC, Dst:RS_MAC) | IP(Src:ClientIP, Dst:VIP) | TCP/UDP | Payload ]</code><ul>
<li><em>旁注：另一种 DSR 实现是 IP Tunneling (ipip)，IPVS 会将原始 IP 包封装在一个新的 IP 包中 <code>[ IP(Src:LB_IP, Dst:RS_IP) | IP(Src:ClientIP, Dst:VIP) | ... ]</code>。后端需要配置解隧道。但在 IPVS 中，MAC 转发（也称 DR - Direct Routing）模式更常见且性能更好。</em></li>
</ul>
</li>
<li>LB-Node1 将修改后的 L2 帧发送给 Backend-NodeA。</li>
</ul>
</li>
<li><p><strong>Backend Service Node:</strong></p>
<ul>
<li>Backend-NodeA 网卡收到帧，确认目标 MAC 是自己（或其监听的 Pod 的 MAC），接收。</li>
<li>内核网络栈解开 L2 帧头，得到原始 IP 包 <code>[ IP(Src:ClientIP, Dst:VIP) | ... ]</code>。</li>
<li><strong>至关重要的配置:</strong> 为了让 Backend-NodeA 内核能够接受并处理这个目标 IP 是 VIP 的数据包，<strong>必须在所有后端服务器节点（或 Pod 的网络命名空间内）的网络接口上配置 VIP 地址</strong>。通常这个 VIP 配置在 <strong><code>lo</code> (loopback) 接口</strong> 或 <code>dummy</code> 接口上，并且需要配置内核参数，<strong>阻止该接口对 VIP 地址进行 ARP 响应</strong>。</li>
<li>数据包被路由到目标端口对应的应用程序（L7 Proxy Pod）。应用程序看到请求的源 IP 是真实的 <strong>ClientIP</strong>。</li>
</ul>
</li>
</ul>
<h3 id="2-响应-Response-流程-Backend-Node-Router-Client-绕过-L4-LB"><a href="#2-响应-Response-流程-Backend-Node-Router-Client-绕过-L4-LB" class="headerlink" title="2. 响应 (Response) 流程: Backend Node -&gt; Router -&gt; Client (绕过 L4 LB)"></a>2. 响应 (Response) 流程: Backend Node -&gt; Router -&gt; Client (绕过 L4 LB)</h3><ul>
<li><p><strong>Backend Service Node (L7 Proxy 处理完毕):</strong></p>
<ul>
<li>应用程序处理请求，生成响应数据。</li>
<li>构建响应 IP 包：<strong>Source IP &#x3D; VIP</strong>, <strong>Destination IP &#x3D; ClientIP</strong>。注意源 IP 必须是 VIP，这样客户端才能正确匹配到之前的请求。</li>
<li><code>[ IP(Src:VIP, Dst:ClientIP) | TCP/UDP | Response Payload ]</code></li>
</ul>
</li>
<li><p><strong>Backend Node -&gt; Router:</strong></p>
<ul>
<li>后端节点的内核网络栈根据 <strong>Destination IP (ClientIP)</strong> 查询其路由表。</li>
<li>找到通往 ClientIP 的下一跳（通常是同一 Router）。</li>
<li>构建 L2 帧头：<strong>Source MAC &#x3D; Backend-NodeA_MAC</strong>, <strong>Destination MAC &#x3D; Router_MAC</strong>。</li>
<li><code>[ Eth(Src:Backend-NodeA_MAC, Dst:RouterMAC) | IP(Src:VIP, Dst:ClientIP) | ... ]</code></li>
<li>帧被发送给 Router。</li>
</ul>
</li>
<li><p><strong>Router -&gt; Client:</strong></p>
<ul>
<li>路由器收到来自 Backend-NodeA 的帧，解开 L2 帧头，看到 IP 包 (Src: VIP, Dst: ClientIP)。</li>
<li>根据 <strong>Destination IP (ClientIP)</strong> 查找路由表，将包路由回客户端。</li>
</ul>
</li>
<li><p><strong>Client:</strong></p>
<ul>
<li>客户端收到响应包，源 IP 是 VIP，与它请求的目标 IP 一致，TCP 连接状态机可以正常工作，通信完成。</li>
</ul>
</li>
</ul>
<h2 id="DSR-的关键配置要求"><a href="#DSR-的关键配置要求" class="headerlink" title="DSR 的关键配置要求"></a>DSR 的关键配置要求</h2><p>成功实施 DSR 模式，<strong>后端服务器（Real Server 节点或 Pod 网络命名空间）的正确配置是核心</strong>：</p>
<ol>
<li><p><strong>在所有后端服务器上配置 VIP 地址:</strong> 通常添加到 <code>lo</code> 回环接口，使其能够接受目标为 VIP 的数据包。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 在后端节点或 Pod 网络命名空间内执行</span><br><span class="hljs-built_in">sudo</span> ip addr add <span class="hljs-variable">$&#123;VIP&#125;</span>/32 dev lo scope host<br></code></pre></td></tr></table></figure>
<p>使用 <code>/32</code> 掩码确保它是一个精确的主机地址。<code>scope host</code> 限制此地址仅在本机有效。</p>
</li>
<li><p><strong>抑制 ARP 响应 (The ARP Problem):</strong> 如果后端服务器响应了针对 VIP 的 ARP 请求，网络上的设备（如路由器）可能会学习到 VIP 对应的 MAC 地址是后端服务器的 MAC，导致后续发往 VIP 的流量直接被路由到某个后端服务器，绕过了 L4 LB Director，破坏了负载均衡。因此，必须配置内核参数，阻止 <code>lo</code> 接口（或其他配置了 VIP 的接口）响应对 VIP 的 ARP 请求。常用的 <code>sysctl</code> 参数组合：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># /etc/sysctl.conf 或 /etc/sysctl.d/dsr.conf</span><br><span class="hljs-comment"># arp_ignore: 定义对目标 IP 是本机 IP 的 ARP 请求的响应模式</span><br><span class="hljs-comment">#   1: 只在请求的目标 IP 地址配置在接收 ARP 请求的接口上时，才回应</span><br>net.ipv4.conf.all.arp_ignore = 1<br>net.ipv4.conf.lo.arp_ignore = 1 <span class="hljs-comment"># 确保 lo 接口也应用此规则</span><br><br><span class="hljs-comment"># arp_announce: 定义本机 IP 地址通告给网络的行为模式</span><br><span class="hljs-comment">#   2: 总是使用接口上最合适的本地地址作为源 IP 地址进行通告，</span><br><span class="hljs-comment">#      对于 lo 上的地址（如 VIP），避免将其作为 ARP 回应的源 IP 向外通告</span><br>net.ipv4.conf.all.arp_announce = 2<br>net.ipv4.conf.lo.arp_announce = 2 <span class="hljs-comment"># 确保 lo 接口也应用此规则</span><br><br><span class="hljs-comment"># 应用配置</span><br><span class="hljs-built_in">sudo</span> sysctl -p /etc/sysctl.conf <span class="hljs-comment"># 或者指定你的配置文件</span><br></code></pre></td></tr></table></figure>
<p><strong>必须确保所有 Real Server 都应用了这些配置。</strong></p>
</li>
</ol>
<h2 id="优点与考量"><a href="#优点与考量" class="headerlink" title="优点与考量"></a>优点与考量</h2><ul>
<li><strong>优点:</strong><ul>
<li><strong>极高性能和吞吐量:</strong> L4 LB 节点仅处理入向请求流量，不处理（通常更大的）出向响应流量，极大地降低了 LB 节点的网络和 CPU 负担。集群整体吞吐能力主要受限于后端服务器的处理能力和网络带宽。</li>
<li><strong>良好的可伸缩性:</strong> 增加后端服务实例通常不需要等比例增加 L4 LB 节点的容量。</li>
<li><strong>保持客户端源 IP:</strong> 后端服务可以直接看到客户端的真实 IP 地址，便于日志记录、访问控制和地理定位等。</li>
</ul>
</li>
<li><strong>考量:</strong><ul>
<li><strong>后端配置复杂性:</strong> 需要在所有后端服务器上配置 VIP 和调整内核 ARP 参数，增加了运维的复杂性。在 Kubernetes 环境中，这通常需要通过 DaemonSet 运行特权 Pod 来自动化配置，或者依赖特定的 CNI 插件支持。</li>
<li><strong>网络环境要求:</strong> 需要确保从后端服务器到客户端的网络路径是通畅的，且路由器等网络设备不会阻止源 IP 为 VIP 的数据包回传。</li>
<li><strong>故障排查:</strong> 非对称的路由路径有时会增加网络故障排查的难度。</li>
</ul>
</li>
</ul>
<h2 id="L7-集群架构"><a href="#L7-集群架构" class="headerlink" title="L7 集群架构"></a>L7 集群架构</h2><img src="/2025/04/22/Ingress/image-20250423190256226.png" srcset="/img/loading.gif" lazyload class="" title="image-20250423190256226">

<h3 id="适用场景"><a href="#适用场景" class="headerlink" title="适用场景"></a>适用场景</h3><p>这种架构广泛应用于：</p>
<ul>
<li><strong>微服务网关 (API Gateway)</strong>：作为所有微服务的统一入口，处理认证、路由、聚合、限流等横切关注点。</li>
<li><strong>Web 应用接入层</strong>：为网站或 Web 应用提供负载均衡、SSL 卸载、HTTP 协议优化（如 HTTP&#x2F;2）、静态内容缓存等。</li>
<li><strong>Ingress Controller &#x2F; Gateway API 实现</strong>：在 Kubernetes 中，Ingress Controller 和较新的 Gateway API 就是 L7 代理的具体实现，用于将集群外部流量路由到内部服务。</li>
</ul>
<h3 id="架构组成-是什么"><a href="#架构组成-是什么" class="headerlink" title="架构组成 (是什么)"></a>架构组成 (是什么)</h3><p>如图所示，典型的 K8s L7 架构包含：</p>
<ul>
<li><strong>Kubernetes Worker Nodes (k8s minion node)</strong>：运行业务负载的物理机或虚拟机。</li>
<li><strong>L7 Proxy Pods</strong>：运行 L7 代理软件（如 Nginx）的 Pod。这些 Pod 通常由 K8s 的 Deployment 或 DaemonSet 管理，以保证所需的实例数量和高可用性。</li>
<li><strong>Application Pods (App1 Pod, App2 Pod)</strong>：运行实际业务逻辑的应用实例。</li>
<li><strong>Virtual IP Addresses (vip1, vip2)</strong>：这是关键的<strong>逻辑访问点</strong>。客户端的目标地址是 VIP，而不是任何具体的 L7 Proxy Pod IP 或 Node IP。这些 VIP 必须通过某种机制（下面详述）被路由到集群内，并最终将流量导向健康的 L7 Proxy Pod 实例。</li>
<li><strong>网络基础设施</strong>：包括 Kubernetes CNI 插件提供的 Pod 网络、节点间的物理或虚拟网络，以及可能用于暴露 VIP 的外部负载均衡器或路由器。</li>
</ul>
<h2 id="VIP-虚拟-IP-地址-的配置与管理"><a href="#VIP-虚拟-IP-地址-的配置与管理" class="headerlink" title="VIP (虚拟 IP 地址) 的配置与管理"></a>VIP (虚拟 IP 地址) 的配置与管理</h2><p>VIP 是实现服务入口稳定性和高可用的核心技术。它是一个<strong>不与任何特定物理网卡绑定的 IP 地址</strong>，但集群网络系统需要确保发往该 VIP 的流量能够被正确接收和处理。</p>
<h3 id="VIP-的核心作用"><a href="#VIP-的核心作用" class="headerlink" title="VIP 的核心作用"></a>VIP 的核心作用</h3><p>VIP 提供了一个<strong>稳定的抽象层</strong>。客户端始终访问这个不变的 VIP，而底层处理请求的 L7 Proxy 实例可以动态增减、漂移或升级，对客户端完全透明。这<strong>解耦</strong>了服务的消费者和提供者。</p>
<h3 id="Kubernetes-中的-VIP-实现方式"><a href="#Kubernetes-中的-VIP-实现方式" class="headerlink" title="Kubernetes 中的 VIP 实现方式"></a>Kubernetes 中的 VIP 实现方式</h3><p>在 Kubernetes 生态中，有多种技术可以配置和管理 VIP，使其将流量导向 L7 Proxy 服务：</p>
<ol>
<li><p><strong>Service <code>Type=LoadBalancer</code></strong>:</p>
<ul>
<li><strong>原理</strong>：这是最常见的方式，尤其是在公有云环境中（AWS, GCP, Azure 等）。当创建一个 <code>Type=LoadBalancer</code> 的 Service 时，K8s 会与云提供商的 API 交互，<strong>自动请求并配置一个云负载均衡器 (Cloud Load Balancer)</strong>。云 LB 会获得一个<strong>公网或内网的可路由 IP 地址作为 VIP</strong>。云 LB 负责接收外部流量，并将其转发到集群中运行 L7 Proxy Pod 的节点的特定端口（<strong>NodePort</strong>）上，或者如果云厂商支持，直接转发到 Pod IP。</li>
<li><strong>关键</strong>：VIP 的生命周期和路由由云平台管理。流量路径通常是：Client -&gt; Cloud LB (VIP) -&gt; Node (NodePort) -&gt; kube-proxy (iptables&#x2F;IPVS) -&gt; L7 Proxy Pod。</li>
<li><strong>配置示例</strong> (Service YAML 片段):<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">l7-proxy-service</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">type:</span> <span class="hljs-string">LoadBalancer</span> <span class="hljs-comment"># 请求云厂商 LB</span><br>  <span class="hljs-attr">selector:</span><br>    <span class="hljs-attr">app:</span> <span class="hljs-string">l7-proxy</span> <span class="hljs-comment"># 选择带有此标签的 L7 Proxy Pods</span><br>  <span class="hljs-attr">ports:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span><br>    <span class="hljs-attr">port:</span> <span class="hljs-number">80</span>       <span class="hljs-comment"># Service 暴露的端口</span><br>    <span class="hljs-attr">targetPort:</span> <span class="hljs-number">8080</span> <span class="hljs-comment"># L7 Proxy Pod 监听的端口</span><br></code></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p><strong>MetalLB (裸金属或私有云)</strong>:</p>
<ul>
<li><strong>解决的问题</strong>：在没有云厂商 LB 的环境中提供 <code>Type=LoadBalancer</code> 的功能。</li>
<li><strong>原理</strong>：MetalLB 在集群内部运行，接管 <code>Type=LoadBalancer</code> Service 的 VIP 分配和宣告。它主要有两种工作模式：<ul>
<li><strong>Layer 2 模式</strong>：MetalLB 在一个选定的节点上（通过 Leader 选举）<strong>响应对 VIP 的 ARP (IPv4) 或 NDP (IPv6) 请求</strong>，将该节点的 MAC 地址与 VIP 绑定。所有发往 VIP 的 L2 流量会直接到达该 Leader 节点。<strong>该节点上的 <code>kube-proxy</code> (IPVS 或 iptables 模式) 进一步将流量负载均衡到所有 L7 Proxy Pods</strong>。如果 Leader 节点故障，会自动进行重新选举。</li>
<li><strong>BGP 模式</strong>：MetalLB 在配置的节点上运行 BGP Speaker，与<strong>外部物理路由器建立 BGP 对等会话</strong>。通过 BGP 协议，MetalLB 向路由器<strong>宣告 Service VIP 的路由信息</strong>，通常下一跳指向运行 Speaker 的节点 IP。路由器根据 BGP 路由将流量转发到这些节点。这种模式可以实现更优的负载均衡和故障切换（ECMP 多路径）。</li>
</ul>
</li>
<li><strong>配置示例</strong> (MetalLB ConfigMap - L2 模式):<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">ConfigMap</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">metallb-system</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">config</span><br><span class="hljs-attr">data:</span><br>  <span class="hljs-attr">config:</span> <span class="hljs-string">|</span><br><span class="hljs-string">    address-pools:</span><br><span class="hljs-string">    - name: default</span><br><span class="hljs-string">      protocol: layer2 # 指定 L2 模式</span><br><span class="hljs-string">      addresses:</span><br><span class="hljs-string">      - 192.168.10.100-192.168.10.120 # 定义可供分配的 VIP 地址池</span><br></code></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p><strong>kube-proxy (IPVS 模式)</strong>:</p>
<ul>
<li><strong>原理</strong>：<code>kube-proxy</code> 是 K8s Service 实现的核心组件，运行在每个节点上。当它工作在 <strong>IPVS (IP Virtual Server)</strong> 模式时（相比 iptables 模式，通常性能更好，尤其是在 Service 数量多时），它会利用 <strong>Linux 内核的 IPVS 模块</strong>来配置负载均衡规则。对于 L7 Proxy 的 Service（即使是 <code>ClusterIP</code> 类型，也由 Ingress Controller 间接使用），<code>kube-proxy</code> 会：<ul>
<li>在节点上创建一个 <strong>IPVS 虚拟服务 (Virtual Service)</strong>，其 IP 地址就是 Service 的 ClusterIP (或 NodePort 对应的监听地址)。</li>
<li>将所有健康的 <strong>L7 Proxy Pod IP 和端口</strong> 添加为该虚拟服务的 <strong>真实服务器 (Real Servers)</strong>。</li>
<li>当流量到达节点（无论是外部通过 NodePort 还是内部访问 ClusterIP），内核的 IPVS 模块会根据选定的负载均衡算法（如 Round Robin, Least Connection）<strong>直接将数据包的目标地址进行 DNAT (Destination Network Address Translation)</strong>，修改为某个选中的 L7 Proxy Pod 的 IP 和端口，然后转发出去。</li>
</ul>
</li>
<li><strong>查看规则</strong>: 在节点上执行 <code>sudo ipvsadm -Ln</code> 可以列出当前配置的 IPVS 规则。</li>
<li><strong>关键</strong>：IPVS 在 <strong>Linux 内核层面</strong>高效地实现了 Service 的负载均衡逻辑。</li>
</ul>
</li>
<li><p><strong>Ingress Controller &#x2F; Gateway API 实现</strong>:</p>
<ul>
<li><strong>原理</strong>：像 Nginx Ingress Controller, Traefik, Istio Gateway 等，它们本身就是运行在 Pod 中的 L7 Proxy。它们通过监听 K8s 的 <strong>Ingress</strong> 或 <strong>Gateway API</strong> 资源，动态地生成 L7 代理（如 Nginx）的配置文件。这些配置定义了如何根据 Host, Path 等将流量路由到后端的 K8s Service (通常是 App Service，但请求首先到达 L7 Proxy Service)。为了让外部流量能够到达 Ingress Controller Pod，通常会为 Ingress Controller 本身创建一个 <code>Type=LoadBalancer</code> 的 Service（依赖云 LB 或 MetalLB 获取外部 VIP）。</li>
<li><strong>关键</strong>：这类控制器将 K8s API 对象（声明式配置）<strong>翻译</strong>为 L7 代理的具体路由规则。VIP 的获取通常依赖于前面提到的 Service <code>Type=LoadBalancer</code> 或 MetalLB。</li>
</ul>
</li>
</ol>
<p>总结 VIP 管理：核心是确保一个<strong>稳定且可路由的 IP (VIP)</strong>，能将流量<strong>负载均衡</strong>到集群内运行 L7 Proxy 的<strong>节点</strong>上，再由节点上的网络组件（如 kube-proxy&#x2F;IPVS）或 CNI 将流量精确转发到<strong>目标 L7 Proxy Pod</strong>。</p>
<h2 id="IPIP-隧道与跨节点-Pod-通信"><a href="#IPIP-隧道与跨节点-Pod-通信" class="headerlink" title="IPIP 隧道与跨节点 Pod 通信"></a>IPIP 隧道与跨节点 Pod 通信</h2><p>当 L7 Proxy Pod (运行在 Node A) 需要将请求转发给一个运行在<strong>不同节点 (Node B)</strong> 上的 App Pod 时，就涉及到跨节点 Pod 通信。Kubernetes CNI 插件负责构建 Pod 网络，确保 Pod IP 在集群范围内是可达的。<strong>IPIP (IP in IP)</strong> 是一种常用的<strong>隧道封装技术</strong>，用于实现这种跨节点通信，尤其是在底层物理网络不支持直接路由 Pod IP 或需要构建 Overlay 网络时。</p>
<h3 id="为何需要隧道技术-如-IPIP"><a href="#为何需要隧道技术-如-IPIP" class="headerlink" title="为何需要隧道技术 (如 IPIP)?"></a>为何需要隧道技术 (如 IPIP)?</h3><ul>
<li><strong>网络隔离与地址空间重叠</strong>：不同节点的 Pod 可能位于相同的逻辑 CIDR (由 CNI 管理)，但这些 CIDR 在物理网络中并不可路由。隧道技术创建了一个<strong>虚拟的 Overlay 网络</strong>，在此网络中 Pod IP 可以自由通信。</li>
<li><strong>简化物理网络路由</strong>：物理网络只需要知道如何路由节点 IP 即可。Pod 间的通信被封装在节点 IP 之间的数据包中。</li>
<li><strong>跨越 L2&#x2F;L3 边界</strong>：即使节点分布在不同的物理子网，只要节点 IP 之间可以路由，隧道就能工作。</li>
</ul>
<h3 id="IPIP-封包-Encapsulation-详解"><a href="#IPIP-封包-Encapsulation-详解" class="headerlink" title="IPIP 封包 (Encapsulation) 详解"></a>IPIP 封包 (Encapsulation) 详解</h3><p>发生在<strong>发送方节点 (Node A)</strong>，当内核决定将一个源自 L7 Proxy Pod (<code>Proxy_Pod_IP</code>)、发往 App Pod (<code>App_Pod_IP</code> on Node B) 的数据包通过 IPIP 隧道发送时：</p>
<ol>
<li><p><strong>原始 IP 数据包 (Inner Packet)</strong>：</p>
<ul>
<li>IP Header: <code>SrcIP = Proxy_Pod_IP</code>, <code>DstIP = App_Pod_IP</code></li>
<li>Transport Header (TCP&#x2F;UDP): <code>SrcPort = Proxy_Port</code>, <code>DstPort = App_Port</code></li>
<li>Payload: HTTP Request&#x2F;Response Data</li>
</ul>
</li>
<li><p><strong>路由决策</strong>：Node A 的内核路由表指示，要到达 <code>App_Pod_IP</code>（它位于由 Node B 管理的 Pod CIDR 内），需要通过一个指向 Node B 的 IPIP 隧道接口（例如，由 Calico 或 Flannel 自动管理的接口或路由规则）。</p>
</li>
<li><p><strong>IPIP 封装 (由 Linux 内核 <code>ipip</code> 模块处理)</strong>：</p>
<ul>
<li>创建一个<strong>新的外部 IP 头 (Outer Header)</strong>：<ul>
<li><strong>Outer Source IP</strong>: <code>NodeA_IP</code> (Node A 的物理网络接口 IP)</li>
<li><strong>Outer Destination IP</strong>: <code>NodeB_IP</code> (Node B 的物理网络接口 IP)</li>
<li><strong>Outer Protocol Number</strong>: <strong><code>4</code></strong> (IANA 分配给 IPv4-in-IPv4 的协议号)。这个字段至关重要，它告诉接收方内核，IP 数据包的载荷是另一个 IP 数据包。</li>
</ul>
</li>
<li><strong>封装</strong>: 原始的整个 IP 数据包（从 Inner IP Header 到 Payload）成为新 IP 数据包的<strong>载荷 (Payload)</strong>。</li>
</ul>
<figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-code">+-----------------------+</span><br><span class="hljs-section">|    Outer IP Header    |  (SrcIP=NodeA_IP, DstIP=NodeB_IP, Proto=4)</span><br><span class="hljs-section">+-----------------------+</span><br><span class="hljs-section">|  Inner IP Header      |  (SrcIP=Proxy_Pod_IP, DstIP=App_Pod_IP)</span><br><span class="hljs-section">+-----------------------+</span><br><span class="hljs-section">|  Inner TCP/UDP Header |  (SrcPort=Proxy_Port, DstPort=App_Port)</span><br><span class="hljs-section">+-----------------------+</span><br><span class="hljs-section">|      Payload          |  (HTTP Data, etc.)</span><br><span class="hljs-section">+-----------------------+</span><br></code></pre></td></tr></table></figure>
</li>
<li><p><strong>发送</strong>: 封装后的数据包（现在是一个标准的 IP 包，源&#x2F;目的 IP 是节点 IP）通过 Node A 的物理网卡发送到物理网络，目标是 Node B。</p>
</li>
</ol>
<h3 id="IPIP-解包-Decapsulation-详解"><a href="#IPIP-解包-Decapsulation-详解" class="headerlink" title="IPIP 解包 (Decapsulation) 详解"></a>IPIP 解包 (Decapsulation) 详解</h3><p>发生在<strong>接收方节点 (Node B)</strong>：</p>
<ol>
<li><strong>接收</strong>: Node B 的物理网卡收到一个 IP 数据包，其目的 IP 是 <code>NodeB_IP</code>。</li>
<li><strong>协议检查</strong>: Node B 的 Linux 内核检查收到的 IP 包的 <strong>IP Header 中的 Protocol 字段</strong>。发现其值为 <strong><code>4</code></strong>。</li>
<li><strong>IPIP 解封装</strong>: 内核识别出这是一个 IPIP 封装的数据包，于是：<ul>
<li><strong>剥离 (Strip)</strong> 掉外部 IP Header。</li>
<li>将内部载荷（即<strong>原始的 IP 数据包</strong>）提取出来。</li>
</ul>
</li>
<li><strong>内部包路由</strong>: 内核现在处理这个解封装后的原始 IP 数据包。根据其 <strong>内部目的 IP (<code>App_Pod_IP</code>)</strong>，通过 Node B 上的路由规则（通常指向连接该 App Pod 的 veth 设备或网桥），将数据包<strong>转发给本地的目标 App Pod</strong>。</li>
</ol>
<h3 id="Linux-内核实现与配置示例"><a href="#Linux-内核实现与配置示例" class="headerlink" title="Linux 内核实现与配置示例"></a>Linux 内核实现与配置示例</h3><p>虽然在 K8s 中 CNI 插件会自动管理这些，但手动配置 IPIP 隧道有助于理解其工作原理：</p>
<ul>
<li><strong>加载内核模块</strong>: <code>sudo modprobe ipip</code> (确保 <code>ipip.ko</code> 模块可用)</li>
<li><strong>创建隧道接口</strong> (示例：Node A 10.0.0.1, Node B 10.0.0.2):<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 在 Node A 上:</span><br><span class="hljs-built_in">sudo</span> ip tunnel add tunl0 mode ipip remote 10.0.0.2 <span class="hljs-built_in">local</span> 10.0.0.1 ttl 255 <span class="hljs-comment"># 创建隧道接口，指向 Node B</span><br><span class="hljs-built_in">sudo</span> ip <span class="hljs-built_in">link</span> <span class="hljs-built_in">set</span> tunl0 up                                               <span class="hljs-comment"># 激活接口</span><br><span class="hljs-built_in">sudo</span> ip route add &lt;NodeB_Pod_CIDR&gt; dev tunl0                           <span class="hljs-comment"># 添加路由，将发往 Node B Pod CIDR 的流量导向隧道</span><br><br><span class="hljs-comment"># 在 Node B 上:</span><br><span class="hljs-built_in">sudo</span> ip tunnel add tunl0 mode ipip remote 10.0.0.1 <span class="hljs-built_in">local</span> 10.0.0.2 ttl 255 <span class="hljs-comment"># 创建反向隧道接口</span><br><span class="hljs-built_in">sudo</span> ip <span class="hljs-built_in">link</span> <span class="hljs-built_in">set</span> tunl0 up<br><span class="hljs-built_in">sudo</span> ip route add &lt;NodeA_Pod_CIDR&gt; dev tunl0                           <span class="hljs-comment"># 添加反向路由</span><br></code></pre></td></tr></table></figure></li>
<li><strong>关键配置</strong>: <code>mode ipip</code> 指定隧道类型，<code>remote</code> 和 <code>local</code> 定义隧道端点的<strong>物理 IP</strong>。<code>ip route add</code> 命令是核心，它告诉内核何时应该使用隧道接口，从而触发<strong>自动的 IPIP 封装</strong>。</li>
</ul>
<h3 id="Kubernetes-CNI-与-IPIP"><a href="#Kubernetes-CNI-与-IPIP" class="headerlink" title="Kubernetes CNI 与 IPIP"></a>Kubernetes CNI 与 IPIP</h3><p>像 <strong>Calico</strong> 这样的 CNI 插件可以选择使用 IPIP 模式（通常是默认或可选模式之一，特别是当 BGP 不可用或不希望使用时）。Calico 在 IPIP 模式下，通常<strong>不会为每个节点对都创建显式的 <code>tunl</code> 设备</strong>。相反，它依赖于 Linux 内核的 XFRM 或 FOU (Foo Over UDP, 如果使用 UDP 封装) 策略，或者直接在路由层面指定封装。它会配置路由规则，使得发往其他节点 Pod CIDR 的流量在传出节点时被<strong>动态封装</strong>，并在接收节点被<strong>自动解封装</strong>，这个过程对用户和 Pod 来说是透明的。Flannel CNI 插件虽然更常用 VXLAN，但也支持 IPIP（通常需要手动配置）。</p>
<h1 id="数据流"><a href="#数据流" class="headerlink" title="数据流"></a>数据流</h1><img src="/2025/04/22/Ingress/image-20250423201824385.png" srcset="/img/loading.gif" lazyload class="" title="image-20250423201824385">

<h2 id="整体场景概述"><a href="#整体场景概述" class="headerlink" title="整体场景概述"></a>整体场景概述</h2><p>此图描绘了一个外部<strong>客户端</strong>（IP: <code>66.0.0.1</code>）尝试访问部署在 Kubernetes 集群中的一个服务的过程，该服务的入口 IP 地址为 <code>10.0.0.1</code>。最终处理这个请求的是运行在一个<strong>网关 Pod</strong>（Gateway Pod，IP: <code>10.0.3.48</code>）内的 <strong>Envoy</strong> 代理。集群的网络方案看起来是由 <strong>Calico</strong> 管理，它利用 <strong>BGP</strong> 协议（通过 <strong>Bird</strong> 守护进程）在节点间分发路由，并使用 <strong>IP-in-IP 隧道</strong>来处理跨子网的 Pod 通信。同时，运行在 <strong>IPVS</strong> 模式下的 <strong>kube-proxy</strong> 在入口节点上负责初始的服务到 Pod 的流量重定向。</p>
<h2 id="步骤-1-初始请求与外部路由"><a href="#步骤-1-初始请求与外部路由" class="headerlink" title="步骤 1: 初始请求与外部路由"></a>步骤 1: 初始请求与外部路由</h2><p>整个流程始于<strong>客户端</strong>（源 IP <code>s:66.0.0.1</code>）发起一个请求（很可能是 HTTP 请求），目标 IP 是 <code>10.0.0.1</code>，目标端口隐含为 80（根据 Envoy 的监听器配置）。这个 <code>10.0.0.1</code> IP 很可能是一个 Kubernetes Service（类型为 LoadBalancer）的外部 IP，或者是某个 Ingress 控制器暴露给外部的 IP 地址，它已经被通告到外部网络中。</p>
<p>数据包首先经过客户端的本地网络，可能通过一个<strong>路由器</strong>，然后穿越互联网或企业内部网络，最终到达托管 Kubernetes 集群的数据中心或云环境的边界。数据包抵达一个<strong>机架顶端交换机（TOR, Top-of-Rack Switch）</strong>（<code>10.0.2.1</code>），这个交换机负责连接该机架内的物理或虚拟服务器（节点）。根据标准的 <strong>IP 路由</strong>表，TOR 交换机将数据包转发给负责处理此入向流量，或者是通往 <code>10.0.0.1</code> 的下一跳的 <strong>K8S 节点</strong>（其 <code>eth0</code> 接口 IP 为 <code>10.0.2.46</code>）。</p>
<p><strong>核心概念</strong>：这个阶段依赖于标准的 <strong>IP 路由</strong>。路由器和交换机（如果是三层交换机）维护着路由表，这些路由表可以通过静态配置或动态路由协议（如外部网络中可能使用的 OSPF 或 BGP）来填充。转发决策完全基于数据包头中的目的 IP 地址。在一个基于 Linux 的路由器或交换机上，你可以使用 <code>ip route show</code> 命令来查看路由表。</p>
<h2 id="步骤-2-进入-K8s-节点与-Kube-Proxy-IPVS-处理"><a href="#步骤-2-进入-K8s-节点与-Kube-Proxy-IPVS-处理" class="headerlink" title="步骤 2: 进入 K8s 节点与 Kube-Proxy&#x2F;IPVS 处理"></a>步骤 2: 进入 K8s 节点与 Kube-Proxy&#x2F;IPVS 处理</h2><p>数据包到达 <strong>K8S 节点</strong>的 <code>eth0</code> 网络接口（<code>10.0.2.46</code>）。Linux 内核的网络协议栈开始处理这个入站数据包。数据包的目的 IP 是 <code>10.0.0.1</code>。</p>
<p>此时，Kubernetes 的服务抽象机制介入。IP 地址 <code>10.0.0.1</code> 很可能是一个 <strong>ClusterIP</strong>（如果它是集群内部的服务 IP）或者是 Kubernetes Service 管理的一个外部 IP。<strong>Kube-proxy</strong>，作为在每个节点上运行的守护进程，负责实现 Service 的功能。在此图中，kube-proxy 被配置为使用 <strong>IPVS (IP Virtual Server)</strong> 模式。</p>
<p><strong>Kube-proxy&#x2F;IPVS 解决了什么问题？</strong> 它拦截发往虚拟 Service IP 的流量，并将其负载均衡到支持该服务的实际 Pod IP 列表中的某一个。<strong>IPVS</strong> 是 Linux 内核内置的一个高性能 L4 负载均衡器。相比于旧的 iptables 模式，IPVS 通常能为拥有大量服务的集群提供更好的<strong>性能</strong>和<strong>可扩展性</strong>，因为它使用更高效的数据结构（如哈希表）来查找服务和后端，并且其规则更新的开销也相对较低。</p>
<p><strong>IPVS 如何工作？</strong> Kube-proxy 持续监视 Kubernetes API 服务器，获取 Service 和 Endpoints（与 Service 关联的 Pod IP）的变更。一旦有变化，它就会调用 Linux 内核的 Netlink 接口来配置 IPVS 规则。当目的地址为 <code>10.0.0.1:80</code> 的数据包到达时，内核的 <code>netfilter</code> 钩子（具体来说是 <code>PREROUTING</code> 或 <code>INPUT</code> 链，取决于 Service 类型和流量来源）会将数据包导向 IPVS 子系统。IPVS 查找其为 <code>10.0.0.1:80</code> 配置的规则，根据设定的负载均衡算法（例如轮询、最少连接等）选择一个后端 Pod IP（在此例中是 <code>10.0.3.48</code>）。</p>
<p>图示中特别标注了 <code>TCP 10.0.0.1:80 -&gt; 10.0.3.48:80 Tunnel</code>。这表明 IPVS 不仅仅执行了 DNAT（目标网络地址转换）将目的 IP 改为 Pod IP，它还知道到达 <code>10.0.3.48</code> 的路径需要通过<strong>隧道</strong>。这通常发生在 IPVS 意识到目标 Pod 位于另一个节点上，并且根据 CNI（如 Calico）的配置，跨节点通信需要封装。IPVS 此时可能只是将数据包标记并导向隧道处理逻辑，或者直接与隧道接口交互进行转发。</p>
<p><strong>内核交互</strong>: IPVS 完全在 Linux 内核中运行。你可以使用 <code>ipvsadm</code> 命令行工具来查看和管理 IPVS 规则：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 列出当前的 IPVS 规则</span><br><span class="hljs-built_in">sudo</span> ipvsadm -Ln<br></code></pre></td></tr></table></figure>
<p>这个命令会显示配置的虚拟服务（如 <code>TCP 10.0.0.1:80</code>）以及它们关联的真实服务器（Pod IP，如 <code>10.0.3.48:80</code>），包括转发方式（例如 <code>Tunnel</code>, <code>Masq</code> 代表 NAT, <code>Route</code> 代表 DR）。</p>
<h2 id="步骤-3-封装与转发-Calico-BGP-IP-in-IP"><a href="#步骤-3-封装与转发-Calico-BGP-IP-in-IP" class="headerlink" title="步骤 3: 封装与转发 (Calico BGP&#x2F;IP-in-IP)"></a>步骤 3: 封装与转发 (Calico BGP&#x2F;IP-in-IP)</h2><p>当前所在的 K8S 节点（<code>10.0.2.46</code>）现在需要将数据包发送给目标 <strong>Gateway Pod</strong>（<code>10.0.3.48</code>）。注意，这个 Pod 位于不同的子网（<code>10.0.3.x</code>），而当前节点在 <code>10.0.2.x</code> 子网。这就是 <strong>Calico</strong> 网络配置发挥关键作用的地方。</p>
<p><strong>Calico 与 BGP</strong>: Calico 经常使用 <strong>BGP（边界网关协议）</strong> 在 Kubernetes 集群的节点之间分发 Pod CIDR（分配给 Pod 的 IP 地址段）的路由信息。每个节点上通常运行一个 BGP 客户端（在此图中是 <strong>Bird</strong>，它包含在名为 <strong>Controller Pod</strong> 的组件里，这通常指 <code>calico-node</code> 这个 DaemonSet Pod 的一部分）。Bird 与集群中的其他节点（直接或通过 Route Reflector）建立 BGP 对等连接。通过 BGP，当前节点（Node A）学习到 Pod IP <code>10.0.3.48</code> 是可以通过托管该 Pod 的节点（我们称之为 Node B，连接到 TOR <code>10.0.3.1</code>）来访问的。节点上的路由表（如图中 <code>路由表: 10.0.0.1 dev cali704c5adc642</code> 这条路由可能与 Calico 内部管理有关，但更重要的是）会有一条指示如何到达 <code>10.0.3.x</code> Pod 子网的路由，这条路由是由 Bird 通过 BGP 学习并注入内核的。</p>
<p><strong>IP-in-IP 封装</strong>: 由于源节点（<code>10.0.2.x</code>）和目标节点（托管 <code>10.0.3.48</code> 的 Node B，在 <code>10.0.3.x</code> 子网）位于不同的二层网络域（由不同的 TOR 和子网暗示），此场景下的 Calico 配置使用了 <strong>IP-in-IP 隧道</strong>模式。这种模式常用于底层物理网络不支持直接路由 Pod IP（例如，存在二层隔离边界、防火墙策略限制等）的情况。</p>
<p><strong>IP-in-IP 解决了什么问题？</strong> 它允许发往远程节点上 Pod 的数据包能够穿越那些可能不知道如何路由 Pod IP 的中间网络。它通过将原始 IP 数据包<strong>封装</strong>在一个新的 IP 数据包内部来实现。</p>
<p><strong>IP-in-IP 如何工作？</strong> 源节点（Node A）的 Linux 内核获取原始 IP 数据包（或者经过 IPVS 修改后，目标为 <code>10.0.3.48</code> 的数据包），并为其添加一个新的 IP 头部：</p>
<ul>
<li><strong>外部 IP 头源 IP (Outer Source IP)</strong>: <code>10.0.2.24</code> (这很可能是 Node A 上用于 IP-in-IP 隧道的 <strong><code>tunl0</code> 接口</strong>的 IP 地址，作为隧道的源端点)。</li>
<li><strong>外部 IP 头目的 IP (Outer Destination IP)</strong>: <code>10.0.3.48</code> (在此特定的 Calico IP-in-IP 配置模式下，外部目标 IP 直接就是<strong>目标 Pod 的 IP 地址</strong>)。</li>
<li><strong>外部 IP 头协议号 (Outer Protocol)</strong>: <code>4</code> (代表内部协议是 IP，即 IP-in-IP)。</li>
<li><strong>内部数据包 (Inner Packet)</strong>: 原始的 IP 数据包 (<code>s:66.0.0.1</code>, <code>d:10.0.0.1</code>)。</li>
</ul>
<p>这个封装后的<strong>封装包 (Encapsulated packet)</strong> (<code>s:10.0.2.24</code>, <code>d:10.0.3.48</code>, 内部负载是原始包) 随后会根据其<em>外部</em>目的 IP (<code>10.0.3.48</code>) 进行常规的 IP 路由。节点的路由表（由 BGP 更新）知道要到达 <code>10.0.3.48</code>（或者更准确地说，是托管它的 Node B 的 IP），下一跳可能是 Node B 的节点 IP（例如 <code>10.0.3.X</code>），数据包经由 TOR <code>10.0.2.1</code> 发出。</p>
<p><strong>内核交互</strong>: IP-in-IP 隧道依赖于 Linux 内核的 <code>ipip</code> 模块。内核路由表和规则决定了哪些流量需要进入隧道进行封装。你可以使用 <code>ip tunnel show</code> 查看隧道接口，并用 <code>ip route show</code> 查看相关的路由。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 查看 Calico IP Pool 配置，可能包含 IPIP 模式设置</span><br><span class="hljs-comment"># (通常通过 Calico API/calicoctl 管理)</span><br><span class="hljs-comment"># calicoctl get ippool default-ipv4-ippool -o yaml</span><br><span class="hljs-comment"># 查找 ipipMode: Always 或 CrossSubnet</span><br><br><span class="hljs-comment"># 查看内核中的隧道接口 (由 Calico 创建和管理)</span><br>ip tunnel show <span class="hljs-comment"># 可能显示名为 tunl0 或类似名称的接口</span><br>ip route show | grep tunl0 <span class="hljs-comment"># 显示使用该隧道接口的路由</span><br></code></pre></td></tr></table></figure>

<h2 id="步骤-4-到达网关-Pod-与解封装"><a href="#步骤-4-到达网关-Pod-与解封装" class="headerlink" title="步骤 4: 到达网关 Pod 与解封装"></a>步骤 4: 到达网关 Pod 与解封装</h2><p>封装后的数据包跨越网络（经过 TOR <code>10.0.2.1</code>，路由器，TOR <code>10.0.3.1</code>），最终到达托管<strong>网关 Pod</strong>的目标 K8S 节点（Node B）。</p>
<p>数据包到达 Node B 的 <code>eth0</code> 接口。Node B 的 Linux 内核检查外部 IP 头。它发现外部目的 IP 是一个本地的 Pod IP (<code>10.0.3.48</code>)，并且协议号是 <code>4</code> (IP-in-IP)。</p>
<p><strong>解封装 (Decapsulation)</strong>: 内核的 IP-in-IP 驱动程序处理此数据包。它<strong>移除外部 IP 头部</strong> (<code>s:10.0.2.24</code>, <code>d:10.0.3.48</code>)。这使得<strong>原始的内部数据包</strong> (<code>s:66.0.0.1</code>, <code>d:10.0.0.1</code>) 被暴露出来。</p>
<p><strong>数据包递送至 Pod</strong>: 图中在 <strong>Gateway Pod</strong> 的网络命名空间内显示了一个 <code>tunl0</code> 接口，其 IP 为 <code>10.0.0.1</code>，并且解封装后的数据包似乎直接送达这里。这暗示了一种特定的 Calico 配置：<strong>隧道的端点和解封装过程可能发生在 Pod 自己的网络命名空间内</strong>，并与这个 <code>tunl0</code> 接口相关联。原始数据包 (<code>s:66.0.0.1</code>, <code>d:10.0.0.1</code>) 现在位于 Pod 的网络栈中，可供应用程序（Envoy）处理。<em>（另一种可能性是：解封装发生在宿主机节点上，然后节点通过 veth pair 将内部数据包路由给 Pod。但鉴于图中 <code>tunl0</code> 的位置，前一种解释更符合图示。）</em></p>
<p><strong>Veth Pair</strong>: Kubernetes 中的 Pod 通常拥有自己独立的<strong>网络命名空间 (Network Namespace)</strong>。为了将这个命名空间连接到宿主机的网络，会使用一个<strong>虚拟以太网设备对 (veth pair)</strong>。这对设备的一端位于宿主机命名空间（通常名字类似 <code>caliXXXX</code>），另一端则位于 Pod 的命名空间内，通常命名为 <code>eth0</code>。图中显示 Gateway Pod 内部的 <code>eth0</code> 接口 IP 为 <code>10.0.3.48</code>。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 在宿主机节点上，查看 veth 设备可能会看到 caliXXX 端</span><br>ip <span class="hljs-built_in">link</span> show <span class="hljs-built_in">type</span> veth<br><br><span class="hljs-comment"># 如果能进入 Pod 内部执行命令</span><br><span class="hljs-comment"># kubectl exec -it &lt;gateway-pod-name&gt; -n &lt;namespace&gt; -- ip addr show eth0</span><br><span class="hljs-comment"># 可以看到 Pod 内的 eth0 接口及其 IP</span><br></code></pre></td></tr></table></figure>

<h2 id="步骤-5-Envoy-处理与响应路径"><a href="#步骤-5-Envoy-处理与响应路径" class="headerlink" title="步骤 5: Envoy 处理与响应路径"></a>步骤 5: Envoy 处理与响应路径</h2><p>解封装后的原始数据包 (<code>s:66.0.0.1</code>, <code>d:10.0.0.1</code>, 目标端口 80) 到达 Gateway Pod 内部的网络协议栈。</p>
<p><strong>Envoy 代理</strong>: 在这个 Pod 内部运行着一个 <strong>Envoy</strong> 进程，它被配置为在 <code>0.0.0.0:80</code> 上监听。Envoy 接收这个入站 TCP 连接和 HTTP 请求。</p>
<p><strong>Envoy 解决了什么问题？</strong> Envoy 是一个功能强大的 L4&#x2F;L7 代理。在这个“网关 Pod”的场景下，它很可能扮演<strong>入口网关 (Ingress Gateway)</strong> 的角色：终止客户端连接，应用各种策略（如基于路径的路由、速率限制、认证授权、TLS 卸载——尽管图中未显示 TLS），然后将请求转发给后端正确的应用服务（图中未画出后端服务）。</p>
<p><strong>Envoy 如何工作？</strong> Envoy 依赖一套复杂的配置（通常由控制平面如 Istio 或简单的 Ingress 控制器通过 xDS API 动态下发）来定义<strong>监听器 (listeners)</strong>、<strong>路由规则 (route rules)</strong> 和<strong>上游集群 (upstream clusters)</strong>（即后端服务）。它根据这些配置来处理请求。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># 简化的 Envoy Listener 配置示例片段 (YAML 格式)</span><br><span class="hljs-attr">static_resources:</span><br>  <span class="hljs-attr">listeners:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">listener_0</span><br>    <span class="hljs-attr">address:</span><br>      <span class="hljs-attr">socket_address:</span> &#123; <span class="hljs-attr">address:</span> <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span>, <span class="hljs-attr">port_value:</span> <span class="hljs-number">80</span> &#125; <span class="hljs-comment"># 监听所有接口的 80 端口</span><br>    <span class="hljs-attr">filter_chains:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-attr">filters:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">envoy.filters.network.http_connection_manager</span><br>        <span class="hljs-attr">typed_config:</span><br>          <span class="hljs-attr">&quot;@type&quot;:</span> <span class="hljs-string">type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager</span><br>          <span class="hljs-attr">stat_prefix:</span> <span class="hljs-string">ingress_http</span><br>          <span class="hljs-attr">route_config:</span><br>            <span class="hljs-attr">name:</span> <span class="hljs-string">local_route</span><br>            <span class="hljs-attr">virtual_hosts:</span><br>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">backend</span><br>              <span class="hljs-attr">domains:</span> [<span class="hljs-string">&quot;*&quot;</span>] <span class="hljs-comment"># 匹配所有域名</span><br>              <span class="hljs-attr">routes:</span><br>              <span class="hljs-bullet">-</span> <span class="hljs-attr">match:</span> &#123; <span class="hljs-attr">prefix:</span> <span class="hljs-string">&quot;/&quot;</span> &#125; <span class="hljs-comment"># 匹配所有路径</span><br>                <span class="hljs-attr">route:</span> &#123; <span class="hljs-attr">cluster:</span> <span class="hljs-string">my_backend_service</span> &#125; <span class="hljs-comment"># 路由到名为 my_backend_service 的集群</span><br>          <span class="hljs-attr">http_filters:</span><br>          <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">envoy.filters.http.router</span> <span class="hljs-comment"># 必须的 HTTP 路由过滤器</span><br>  <span class="hljs-attr">clusters:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">my_backend_service</span><br>    <span class="hljs-comment"># ... 此处定义 my_backend_service 集群的具体信息 (例如 Endpoints)</span><br></code></pre></td></tr></table></figure>

<p><strong>响应路径</strong>: 当 Envoy 从后端应用获取到响应后，它需要将响应发送回客户端。</p>
<ul>
<li>响应数据包的<strong>源 IP</strong> (Source IP) 通常是 Pod 的 IP (<code>10.0.3.48</code>)。但也可能是 Service IP (<code>10.0.0.1</code>)，这取决于是否配置了特定的 SNAT 规则，或者是否使用了 Pod 内 <code>tunl0</code> 接口的 IP (<code>10.0.0.1</code>，如图所示 <code>s:10.0.0.1</code>) 作为出站源 IP。</li>
<li>响应数据包的<strong>目的 IP</strong> (Destination IP) 是原始客户端的 IP (<code>d:66.0.0.1</code>)。</li>
</ul>
<p>这个响应包通过 Gateway Pod 的 <code>eth0</code> 接口发出。它会使用 Pod 内的路由表（图中显示 <code>默认路由: default via 10.211.48.1 dev eth0</code> - <code>10.211.48.1</code> 很可能是宿主机端 veth 接口或网桥的 IP）。数据包随后被路由回 Node B 的宿主机网络，经过 TOR <code>10.0.3.1</code>、路由器，最终穿越外部网络回到客户端。<strong>关键点：对于返回给外部客户端的流量，通常不需要再次进行 IP-in-IP 封装</strong>，因为标准的互联网路由足以将数据包送达公网上的客户端 IP。</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/kubernetes/" class="print-no-link">#kubernetes</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Ingress</div>
      <div>https://mfzzf.github.io/2025/04/22/Ingress/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Mzzf</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年4月22日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/04/24/kubernetes%E7%94%9F%E4%BA%A7%E5%8C%96%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/" title="kubernetes生产化集群管理">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">kubernetes生产化集群管理</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/04/17/docker-backup/" title="docker-backup">
                        <span class="hidden-mobile">docker-backup</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
