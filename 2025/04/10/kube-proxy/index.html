

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Mzzf">
  <meta name="keywords" content="">
  
    <meta name="description" content="Kube-proxy 的深度解析核心定位与设计目标kube-proxy 是 Kubernetes 网络体系的核心组件，本质是集群服务的智能流量调度器。它以 DaemonSet 形式部署于每个节点，通过监听 API Server 的 Service 和 Endpoints 对象变化，动态维护节点网络规则，实现两大核心功能：服务发现（将抽象服务名解析为具体后端实例）和负载均衡（智能分配服务流量）。其设">
<meta property="og:type" content="article">
<meta property="og:title" content="kube_proxy">
<meta property="og:url" content="https://mfzzf.github.io/2025/04/10/kube-proxy/index.html">
<meta property="og:site_name" content="Mzzf&#39;s Blog">
<meta property="og:description" content="Kube-proxy 的深度解析核心定位与设计目标kube-proxy 是 Kubernetes 网络体系的核心组件，本质是集群服务的智能流量调度器。它以 DaemonSet 形式部署于每个节点，通过监听 API Server 的 Service 和 Endpoints 对象变化，动态维护节点网络规则，实现两大核心功能：服务发现（将抽象服务名解析为具体后端实例）和负载均衡（智能分配服务流量）。其设">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/10/kube-proxy/image-20250410103718455.png">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/10/kube-proxy/image-20250412130128351.png">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/10/kube-proxy/image-20250412130500507.png">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/10/kube-proxy/image-20250412130714279.png">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/10/kube-proxy/1.jpg">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/10/kube-proxy/2.jpg">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/10/kube-proxy/3.jpg">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/10/kube-proxy/5.jpg">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/10/kube-proxy/image-20250412133507210.png">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/10/kube-proxy/image-20250415103628349.png">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/10/kube-proxy/image-20250415104221941.png">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/10/kube-proxy/image-20250415105305996.png">
<meta property="article:published_time" content="2025-04-10T02:23:46.000Z">
<meta property="article:modified_time" content="2025-04-15T02:57:45.177Z">
<meta property="article:author" content="Mzzf">
<meta property="article:tag" content="kubernetes">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://mfzzf.github.io/2025/04/10/kube-proxy/image-20250410103718455.png">
  
  
  
  <title>kube_proxy - Mzzf&#39;s Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"mfzzf.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Mzzf&#39;s blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="kube_proxy"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-04-10 10:23" pubdate>
          2025年4月10日 上午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          19k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          159 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">kube_proxy</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="Kube-proxy-的深度解析"><a href="#Kube-proxy-的深度解析" class="headerlink" title="Kube-proxy 的深度解析"></a>Kube-proxy 的深度解析</h1><h2 id="核心定位与设计目标"><a href="#核心定位与设计目标" class="headerlink" title="核心定位与设计目标"></a>核心定位与设计目标</h2><p>kube-proxy 是 Kubernetes 网络体系的核心组件，本质是<strong>集群服务的智能流量调度器</strong>。它以 <strong>DaemonSet 形式</strong>部署于每个节点，通过监听 API Server 的 Service 和 Endpoints 对象变化，动态维护节点网络规则，实现两大核心功能：<strong>服务发现（将抽象服务名解析为具体后端实例）和负载均衡（智能分配服务流量）</strong>。其设计演进史本质是追求高性能、低延迟、大规模场景适应性的过程。</p>
<h2 id="底层网络模型演进"><a href="#底层网络模型演进" class="headerlink" title="底层网络模型演进"></a>底层网络模型演进</h2><h3 id="1-Userspace-模式：初代方案的性能困局"><a href="#1-Userspace-模式：初代方案的性能困局" class="headerlink" title="1. Userspace 模式：初代方案的性能困局"></a>1. Userspace 模式：初代方案的性能困局</h3><p>在 userspace 模式下，kube-proxy 通过用户态进程监听随机端口（如 31453），配合 iptables 的 DNAT 规则将服务流量重定向到该端口。进程内部维护轮询队列进行负载均衡，最终通过二次连接建立到实际 Pod 的通道。</p>
<p><strong>性能缺陷根源</strong>：</p>
<ul>
<li>双重上下文切换：数据包需经历<strong>内核态-&gt;用户态-&gt;内核态</strong>的完整路径</li>
<li>内存拷贝开销：每次转发都需要完整复制数据包内容</li>
<li>单进程瓶颈：所有流量集中处理，无法利用多核优势</li>
</ul>
<p>典型场景下吞吐量下降 50% 以上，时延增加 2-3 倍，仅适用于小规模测试环境。</p>
<h3 id="2-Iptables-模式：内核加速的代价"><a href="#2-Iptables-模式：内核加速的代价" class="headerlink" title="2. Iptables 模式：内核加速的代价"></a>2. Iptables 模式：内核加速的代价</h3><p>通过完全基于 netfilter 框架实现，kube-proxy 生成 iptables 规则链处理流量。以 ClusterIP 类型服务为例，关键规则链包括：</p>
<ul>
<li>KUBE-SERVICES：服务入口匹配</li>
<li>KUBE-SVC-XXXX：服务负载均衡链</li>
<li>KUBE-SEP-XXXX：具体 endpoint 转发规则</li>
</ul>
<p><strong>性能挑战</strong>：</p>
<ul>
<li>规则爆炸问题：每个 Service&#x2F;Endpoint 对应多条规则，500 服务即可产生数万条规则</li>
<li>全量更新延迟：规则变更需要整体刷新，万级规则时更新延迟可达分钟级</li>
<li>Conntrack 压力：SNAT 会话跟踪消耗大量内存，极端情况导致丢包</li>
</ul>
<p>实测显示 10,000 个服务时，iptables 模式新增规则耗时可达 5 分钟，且存在 30% 以上的转发性能衰减。</p>
<h3 id="3-IPVS-模式：生产级负载均衡方案"><a href="#3-IPVS-模式：生产级负载均衡方案" class="headerlink" title="3. IPVS 模式：生产级负载均衡方案"></a>3. IPVS 模式：生产级负载均衡方案</h3><p>基于 Linux 内核的 LVS（Linux Virtual Server）实现，通过 netlink 接口操作 IPVS 规则。核心优势体现在：</p>
<ul>
<li>哈希表存储规则：O(1) 时间复杂度查找，万级服务无性能衰减</li>
<li>增量式更新：仅修改变化部分，规则更新毫秒级完成</li>
<li>丰富调度算法：支持 rr&#x2F;wrr&#x2F;lc&#x2F;wlc&#x2F;ip_hash 等 10 余种算法</li>
<li>连接保持优化：通过 sh-port 等 flags 实现会话保持</li>
</ul>
<p><strong>关键技术实现</strong>：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// pkg/proxy/ipvs/controller.go</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(ln *linuxNetworking)</span></span> setupIPVSService(svc *ipvs.Service, bindAddr <span class="hljs-type">string</span>, port <span class="hljs-type">int</span>, protocol <span class="hljs-type">string</span>) <span class="hljs-type">error</span> &#123;<br>    <span class="hljs-comment">// 使用 netlink 与内核 IPVS 子系统交互</span><br>    _, err := ln.ipvsHandle.NewService(svc)<br>    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>        <span class="hljs-keyword">return</span> fmt.Errorf(<span class="hljs-string">&quot;error creating IPVS service: %v&quot;</span>, err)<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>
<p>IPVS 模式在万级服务规模下仍能保持微秒级转发延迟，比 iptables 模式提升 10 倍以上的规则处理效率。 </p>
<h2 id="生产环境选型建议"><a href="#生产环境选型建议" class="headerlink" title="生产环境选型建议"></a>生产环境选型建议</h2><h3 id="性能基准对比"><a href="#性能基准对比" class="headerlink" title="性能基准对比"></a>性能基准对比</h3><table>
<thead>
<tr>
<th>模式</th>
<th>万规则更新时延</th>
<th>并发连接能力</th>
<th>CPU 消耗</th>
<th>内存占用</th>
</tr>
</thead>
<tbody><tr>
<td>Userspace</td>
<td>N&#x2F;A</td>
<td>1k</td>
<td>高</td>
<td>高</td>
</tr>
<tr>
<td>Iptables</td>
<td>5min</td>
<td>10k</td>
<td>中高</td>
<td>高</td>
</tr>
<tr>
<td>IPVS</td>
<td>100ms</td>
<td>100k+</td>
<td>低</td>
<td>低</td>
</tr>
</tbody></table>
<h3 id="实践建议"><a href="#实践建议" class="headerlink" title="实践建议"></a>实践建议</h3><ol>
<li>内核要求：确保 Linux 内核 ≥4.15 并加载 ip_vs 相关模块</li>
<li>算法选择：默认 rr 轮询，有状态服务建议 sh 源地址哈希</li>
<li>配合 CNI：Calico&#x2F;Cilium 等插件需开启 IPVS 兼容模式</li>
<li>监控重点：关注 ipvs_connections 等指标，防范 SNAT 端口耗尽</li>
</ol>
<h2 id="架构演进启示"><a href="#架构演进启示" class="headerlink" title="架构演进启示"></a>架构演进启示</h2><p>从 userspace 到 IPVS 的演进路径，体现了 Kubernetes 网络模型从功能实现到生产可用的成熟过程。IPVS 模式通过内核级数据路径、零拷贝转发、增量更新等机制，完美解决了大规模微服务架构下的四层负载均衡需求。未来 eBPF 技术的引入（如 Cilium 方案）可能带来新一轮性能革命，但 IPVS 仍是当前生产环境最稳健的选择。</p>
<h1 id="Netfilter-框架深度解析"><a href="#Netfilter-框架深度解析" class="headerlink" title="Netfilter 框架深度解析"></a>Netfilter 框架深度解析</h1><img src="/2025/04/10/kube-proxy/image-20250410103718455.png" srcset="/img/loading.gif" lazyload class="" title="image-20250410103718455">

<p>Netfilter 是 Linux 内核中一个强大而灵活的网络数据包处理框架。它不仅仅是防火墙的基础，更是许多高级网络功能（如网络地址转换 NAT、数据包修改、连接跟踪）的核心。在云原生时代，理解 Netfilter 对于深入掌握 Kubernetes 网络（例如 kube-proxy 的 iptables 模式、CNI 插件如 Calico 的网络策略实现）至关重要。</p>
<h2 id="Iptables-与-Netfilter-的关系：用户空间与内核空间的协作"><a href="#Iptables-与-Netfilter-的关系：用户空间与内核空间的协作" class="headerlink" title="Iptables 与 Netfilter 的关系：用户空间与内核空间的协作"></a>Iptables 与 Netfilter 的关系：用户空间与内核空间的协作</h2><img src="/2025/04/10/kube-proxy/image-20250412130128351.png" srcset="/img/loading.gif" lazyload class="" title="image-20250412130128351">

<p>首先需要明确，<code>iptables</code> 和 <code>Netfilter</code> 并非同一事物，而是紧密协作的用户空间工具与内核空间框架的关系。<code>iptables</code> 是一个运行在用户空间的命令行工具，它允许系统管理员定义<strong>网络数据包的处理规则（例如允许、拒绝、修改数据包）</strong>。然而，<code>iptables</code> 本身并不直接执行这些规则。它扮演的是一个“规则配置器”的角色，将用户定义的规则翻译并加载到 Linux 内核中。</p>
<p>真正负责在网络协议栈中执行这些规则的是 <code>Netfilter</code> 框架。<code>Netfilter</code> 在内核的网络协议栈的关键路径上预设了一系列“钩子”（Hooks）。当网络数据包流经协议栈时，会触发这些钩子。内核模块（包括 <code>iptables</code> 相关的内核模块，如 <code>ip_tables</code>, <code>iptable_filter</code>, <code>iptable_nat</code> 等）可以通过 <code>Netfilter</code> 提供的接口，在这些钩子上注册自己的处理函数。当钩子被触发时，注册在此钩子上的函数会按照预设的优先级顺序被调用，对数据包进行检查、修改、放行或丢弃等操作。因此，<code>iptables</code> 定义策略，<code>Netfilter</code> 提供执行点和机制，两者结合实现了对网络流量的精细控制。</p>
<h2 id="Netfilter-Hooks：内核网络栈的关键切入点"><a href="#Netfilter-Hooks：内核网络栈的关键切入点" class="headerlink" title="Netfilter Hooks：内核网络栈的关键切入点"></a>Netfilter Hooks：内核网络栈的关键切入点</h2><img src="/2025/04/10/kube-proxy/image-20250412130500507.png" srcset="/img/loading.gif" lazyload class="" title="image-20250412130500507">

<p><code>Netfilter</code> 为 IPv4 协议栈定义了五个核心的钩子点（Hooks）。这些钩子点战略性地分布在数据包处理流程的关键位置，允许内核模块在不同阶段介入数据包的处理。数据包触发哪个钩子，取决于它的流向（流入、流出、转发）、目的地（本机或其它主机）以及它在前一个钩子处理后的状态（例如是否被接受）。</p>
<p>以下是 Linux 内核中定义的五个主要 IPv4 Netfilter 钩子点（定义在 <code>&lt;linux/netfilter_ipv4.h&gt;</code>）：</p>
<ul>
<li><strong>NF_IP_PRE_ROUTING:</strong> 这是数据包进入网络协议栈后遇到的第一个钩子。它在数据包刚刚通过基本的完整性检查（如 IP 头校验和、版本号验证）之后，但在进行任何路由决策之前触发。这是**执行目的地址转换（DNAT）**或进行早期包过滤&#x2F;检查的理想位置。</li>
<li><strong>NF_IP_LOCAL_IN:</strong> 当数据包经过路由决策，确定其最终目的地是本机时，在将数据包传递给上层协议（如 TCP&#x2F;UDP）之前，会触发此钩子。这是对访问本机服务的数据包进行**过滤（Filter 表的 INPUT 链）**的主要场所。</li>
<li><strong>NF_IP_FORWARD:</strong> 如果数据包经过路由决策，发现其目的地并非本机，而是需要<strong>转发到其他网络接口</strong>，那么在转发操作实际执行前会触发此钩子。这是实现防火墙转发规则（Filter 表的 FORWARD 链）的核心位置。</li>
<li><strong>NF_IP_LOCAL_OUT:</strong> 由本机应用程序产生、准备发送出去的数据包，在进入网络协议栈进行处理时，会立即触发此钩子。这发生在路由决策之前，是进行输出过滤（Filter 表的 OUTPUT 链）或早期修改（如标记）的地方。</li>
<li><strong>NF_IP_POST_ROUTING:</strong> 无论是本机产生的需要发送的数据包，还是需要转发的数据包，在经过最终的路由决策、即将被发送到网络接口之前，都会触发此钩子。这是**执行源地址转换（SNAT）或进行最后的数据包修改（Mangle）**的理想位置。</li>
</ul>
<p>内核模块在向 <code>Netfilter</code> 注册处理函数时，必须指定一个优先级（priority）。这使得当同一个钩子点有多个模块注册函数时，<code>Netfilter</code> 能够按照明确的顺序（优先级数值越小，越先执行）调用它们。每个处理函数执行完毕后，会返回一个状态码（如 <code>NF_ACCEPT</code>, <code>NF_DROP</code>, <code>NF_STOLEN</code>, <code>NF_QUEUE</code>, <code>NF_REPEAT</code>）给 <code>Netfilter</code> 框架，指示框架应该如何处理当前的数据包（接受、丢弃、模块已接管、送入用户空间队列、重新处理等）。</p>
<img src="/2025/04/10/kube-proxy/image-20250412130714279.png" srcset="/img/loading.gif" lazyload class="" title="image-20250412130714279">

<h2 id="Hooks、Tables-与-Chains：规则的组织结构"><a href="#Hooks、Tables-与-Chains：规则的组织结构" class="headerlink" title="Hooks、Tables 与 Chains：规则的组织结构"></a>Hooks、Tables 与 Chains：规则的组织结构</h2><p><code>iptables</code> 为了更好地组织和管理规则，引入了“表”（Table）和“链”（Chain）的概念。</p>
<p><strong>Tables</strong> 代表了不同类型的规则处理逻辑，主要有以下五种：</p>
<ul>
<li><strong>Filter Table (<code>filter</code>):</strong> 这是最常用、也是默认的表。其核心功能是进行数据包过滤，即决定是否允许一个数据包通过。它包含了 <code>INPUT</code>, <code>FORWARD</code>, <code>OUTPUT</code> 三个内建链。</li>
<li><strong>NAT Table (<code>nat</code>):</strong> 用于网络地址转换（Network Address Translation）。当数据包首次进入协议栈（<code>PREROUTING</code> 链，用于 DNAT）或即将离开时（<code>POSTROUTING</code> 链，用于 SNAT），以及某些特定场景下的输出包（<code>OUTPUT</code> 链，用于本地进程访问外部服务的 SNAT&#x2F;DNAT），此表中的规则会被触发，用于修改数据包的源或目的 IP 地址和端口，常用于实现内网访问外网、端口映射等。</li>
<li><strong>Mangle Table (<code>mangle</code>):</strong> 用于修改 IP 数据包的头部字段（如 TOS&#x2F;DSCP 服务质量标记、TTL 生存时间）或设置内核内部使用的标记（<code>MARK</code>）。这些修改可以影响后续的路由决策、流量控制或其他基于标记的处理。它可以在所有五个钩子点进行操作。</li>
<li><strong>Raw Table (<code>raw</code>):</strong> 这个表优先级非常高（在 <code>PREROUTING</code> 和 <code>OUTPUT</code> 钩子触发），其主要目的是提供一个机制，允许数据包“绕过” <code>Netfilter</code> 的连接跟踪（Connection Tracking）系统。通过在 <code>raw</code> 表中设置 <code>NOTRACK</code> 目标，可以显著降低某些场景下（如处理大量无状态短连接）的系统负载。</li>
<li><strong>Security Table (<code>security</code>):</strong> 主要用于与 Linux 安全模块（如 SELinux）集成。它允许基于安全策略为数据包或连接打上安全标记（<code>SECMARK</code>, <code>CONNSECMARK</code>），这些标记可以被 SELinux 或其他支持安全上下文的系统用于强制访问控制。它在 <code>filter</code> 表之后执行。</li>
</ul>
<p>在每个 Table 内部，规则被组织成<strong>Chains</strong>。Chain 是一系列按顺序排列的规则。<code>iptables</code> 有两种类型的 Chain：内建链（Built-in Chains）和自定义链（User-defined Chains）。内建链直接与 <code>Netfilter</code> 的钩子相关联，当相应的钩子被触发时，内核会遍历该链中的规则。</p>
<p>内建链与 Netfilter 钩子的对应关系，以及各 Table 在哪些链上注册，构成了数据包处理的完整路径：</p>
<ul>
<li><strong>PREROUTING Chain:</strong> 由 <code>NF_IP_PRE_ROUTING</code> 钩子触发。流经此链的 Table (按优先级顺序)：<code>raw</code>, <code>mangle</code>, <code>nat</code> (主要用于 DNAT)。</li>
<li><strong>INPUT Chain:</strong> 由 <code>NF_IP_LOCAL_IN</code> 钩子触发。流经此链的 Table：<code>mangle</code>, <code>filter</code>, <code>security</code>, <code>nat</code> (较少见，用于特定场景的源 NAT)。</li>
<li><strong>FORWARD Chain:</strong> 由 <code>NF_IP_FORWARD</code> 钩子触发。流经此链的 Table：<code>mangle</code>, <code>filter</code>, <code>security</code>。</li>
<li><strong>OUTPUT Chain:</strong> 由 <code>NF_IP_LOCAL_OUT</code> 钩子触发。流经此链的 Table：<code>raw</code>, <code>mangle</code>, <code>nat</code> (用于本地进程发起的连接的 DNAT&#x2F;SNAT), <code>filter</code>, <code>security</code>。</li>
<li><strong>POSTROUTING Chain:</strong> 由 <code>NF_IP_POST_ROUTING</code> 钩子触发。流经此链的 Table：<code>mangle</code>, <code>nat</code> (主要用于 SNAT)。</li>
</ul>
<p>这个结构使得管理员能够精确控制在数据包处理流程的哪个阶段应用何种类型的策略。例如，NAT 操作通常需要在路由决策之前（DNAT）或之后（SNAT）进行，而过滤操作则主要发生在路由决策之后的目的地判断点（INPUT、FORWARD）或本地发出时（OUTPUT）。</p>
<img src="/2025/04/10/kube-proxy/1.jpg" srcset="/img/loading.gif" lazyload class="" title="image">
<p><em>图片展示了数据包流经不同 Hook 点时，会依次经过哪些 Table 的哪些 Chain。</em></p>
<h2 id="从-Linux-IP-协议栈深入理解-Netfilter"><a href="#从-Linux-IP-协议栈深入理解-Netfilter" class="headerlink" title="从 Linux IP 协议栈深入理解 Netfilter"></a>从 Linux IP 协议栈深入理解 Netfilter</h2><p>要真正掌握 <code>Netfilter</code> 的工作原理，我们需要深入理解 Linux 内核处理 IP 数据包的流程。<code>Netfilter</code> 的钩子正是嵌入在这个流程中的。</p>
<img src="/2025/04/10/kube-proxy/2.jpg" srcset="/img/loading.gif" lazyload class="" title="image">
<p><em>简化的 Linux IP 协议栈数据包接收路径示意图</em></p>
<h3 id="数据包接收流程概述"><a href="#数据包接收流程概述" class="headerlink" title="数据包接收流程概述"></a>数据包接收流程概述</h3><ol>
<li><p><strong>硬件接收与中断:</strong> 当网卡（NIC）接收到一个目的 MAC 地址匹配本机或为广播&#x2F;多播地址的以太网帧时，它会将数据通过 DMA (Direct Memory Access) 传输到内存中的预分配缓冲区（Ring Buffer）。传输完成后，网卡会向 CPU 发出一个硬件中断信号。</p>
</li>
<li><p><strong>中断处理程序 (ISR - Interrupt Service Routine):</strong> CPU 响应该中断，暂停当前任务，跳转执行该网卡驱动注册的中断处理程序。ISR 的主要工作是快速响应硬件，通常会：</p>
<ul>
<li>禁用网卡中断（防止中断风暴）。</li>
<li>分配一个内核数据结构 <code>sk_buff</code> (Socket Buffer) 来表示这个数据包。<code>sk_buff</code> 是 Linux 网络栈中表示网络数据包的核心结构，包含了数据本身以及大量的元数据（如协议类型、接口信息、时间戳、路由结果等）。</li>
<li>调用网卡驱动的特定函数，将 DMA 缓冲区中的数据拷贝到 <code>sk_buff</code> 中，并更新 Ring Buffer 的状态。</li>
<li>调用与协议无关的网络设备接收函数 <code>netif_rx()</code> 或其变体（如 <code>napi_gro_receive()</code>）。</li>
</ul>
</li>
<li><p><strong><code>netif_rx()</code> 与 NAPI:</strong> <code>netif_rx()</code> 将 <code>sk_buff</code> 添加到 CPU 的 backlog 队列，并触发一个 <code>NET_RX_SOFTIRQ</code> 软中断。为了提高性能并避免中断风暴，现代驱动普遍使用 NAPI (New API)。在 NAPI 模式下，ISR 只需禁用中断并触发软中断，实际的数据包处理（分配 <code>sk_buff</code>、拷贝数据）被推迟到软中断上下文中，由 <code>napi_poll()</code> 函数批量处理。</p>
</li>
<li><p><strong>软中断处理 (<code>NET_RX_SOFTIRQ</code>):</strong> 内核调度器在适当的时候（通常是中断返回或内核线程调度时）会检查并执行挂起的软中断。<code>ksoftirqd</code> 内核线程也会在系统负载较高时帮助处理软中断。处理 <code>NET_RX_SOFTIRQ</code> 的核心函数是 <code>net_rx_action()</code>。它会从 backlog 队列或 NAPI 的 poll 列表中取出 <code>sk_buff</code>，然后根据 <code>sk_buff-&gt;protocol</code> 字段（由驱动根据以太网帧类型设置）将其分发给相应的 L3 协议处理函数。例如，IP 包会交给 <code>ip_rcv()</code> 处理，ARP 包交给 <code>arp_rcv()</code> 处理。</p>
</li>
</ol>
<h3 id="IPv4-数据包处理与-Netfilter-Hooks"><a href="#IPv4-数据包处理与-Netfilter-Hooks" class="headerlink" title="IPv4 数据包处理与 Netfilter Hooks"></a>IPv4 数据包处理与 Netfilter Hooks</h3><img src="/2025/04/10/kube-proxy/3.jpg" srcset="/img/loading.gif" lazyload class="" title="image">

<p><em>IP 层处理流程与 Netfilter Hooks 的嵌入点</em></p>
<ol>
<li><p><strong><code>ip_rcv()</code> - 初始处理与 <code>NF_IP_PRE_ROUTING</code>:</strong></p>
<ul>
<li><code>ip_rcv()</code> 函数是 IP 层处理接收到的数据包的入口。它首先会对 IP 头部进行基本的验证，如版本号、头部长度、总长度、校验和等。</li>
<li>如果验证通过，紧接着，数据包会经过 <strong><code>NF_IP_PRE_ROUTING</code></strong> 钩子。所有注册在此钩子上的 <code>Netfilter</code> 处理函数（来自 <code>raw</code>, <code>mangle</code>, <code>nat</code> 表）会依次执行。这是进行 DNAT 或早期过滤&#x2F;修改的关键点。</li>
<li>如果 <code>Netfilter</code> 函数返回 <code>NF_DROP</code>，数据包处理流程终止。如果返回 <code>NF_ACCEPT</code>，则继续。</li>
</ul>
</li>
<li><p><strong><code>ip_rcv_finish()</code> - 路由决策:</strong></p>
<ul>
<li>通过 <code>NF_IP_PRE_ROUTING</code> 钩子后，数据包进入 <code>ip_rcv_finish()</code>。此函数的核心任务是进行路由查找，决定数据包的下一跳。它会调用 <code>ip_route_input_slow()</code>（或其快速路径缓存）来查询路由表。</li>
<li>路由查找的结果会填充到 <code>skb-&gt;dst</code> (destination cache entry) 结构中，该结构包含了路由决策的全部信息，包括下一跳地址、输出设备、以及指向后续处理函数的指针（如 <code>dst-&gt;input</code> 和 <code>dst-&gt;output</code>）。</li>
</ul>
</li>
<li><p><strong>根据路由结果分发:</strong></p>
<ul>
<li><strong>目的为本机 (<code>dst-&gt;input == ip_local_deliver</code>):</strong> 如果路由查找确定数据包的目的 IP 是本机的某个地址，<code>dst_input(skb)</code> 最终会调用 <code>ip_local_deliver()</code>。<ul>
<li><strong><code>ip_local_deliver()</code> 与 <code>NF_IP_LOCAL_IN</code>:</strong> 在将数据包传递给更上层的协议（如 TCP 的 <code>tcp_v4_rcv</code> 或 UDP 的 <code>udp_rcv</code>）之前，<code>ip_local_deliver()</code> 会首先调用 <code>ip_local_deliver_finish()</code>，在这里会触发 <strong><code>NF_IP_LOCAL_IN</code></strong> 钩子。注册在此钩子上的 <code>Netfilter</code> 处理函数（来自 <code>mangle</code>, <code>filter</code>, <code>security</code>, <code>nat</code> 表）会被执行，主要用于对访问本机服务的数据包进行过滤。</li>
</ul>
</li>
<li><strong>需要转发 (<code>dst-&gt;input == ip_forward</code>):</strong> 如果路由查找确定数据包需要被转发到另一个网络接口，<code>dst_input(skb)</code> 最终会调用 <code>ip_forward()</code>。<ul>
<li><strong><code>ip_forward()</code> 与 <code>NF_IP_FORWARD</code>:</strong> <code>ip_forward()</code> 函数负责处理数据包的转发逻辑。在进行必要的检查（如 TTL 检查）之后，它会调用 <code>ip_forward_finish()</code>，在这里会触发 <strong><code>NF_IP_FORWARD</code></strong> 钩子。注册在此钩子上的 <code>Netfilter</code> 处理函数（来自 <code>mangle</code>, <code>filter</code>, <code>security</code> 表）会被执行，用于对转发的数据包进行过滤和修改。</li>
<li><strong>TTL 递减与 MTU 处理:</strong> 在 <code>ip_forward()</code> 过程中，IP 头部的 TTL 值会被减 1。如果 TTL 变为 0，数据包会被丢弃，并可能发送 ICMP Time Exceeded 消息。如果数据包大小超过了出口设备的 MTU 且允许分片，还会进行 IP 分片。</li>
</ul>
</li>
<li><strong>多播处理 (<code>dst-&gt;input == ip_mr_input</code>):</strong> 如果是多播数据包且本机配置了多播路由，会进入多播转发流程。</li>
</ul>
</li>
<li><p><strong>数据包发送路径与 <code>NF_IP_LOCAL_OUT</code> 和 <code>NF_IP_POST_ROUTING</code>:</strong></p>
<ul>
<li><strong>本地产生的数据包 (<code>ip_queue_xmit</code>):</strong> 当本地应用程序通过 Socket API 发送数据时，数据会逐层向下传递（例如 TCP -&gt; IP）。在 IP 层，<code>ip_queue_xmit()</code> 或类似函数负责构建 IP 头部并准备发送。<ul>
<li><strong><code>NF_IP_LOCAL_OUT</code>:</strong> 在 <code>ip_queue_xmit()</code> 内部，构建完 IP 头并进行初步路由查找（确定源地址和出口设备等）之后，会触发 <strong><code>NF_IP_LOCAL_OUT</code></strong> 钩子。注册在此钩子上的 <code>Netfilter</code> 处理函数（来自 <code>raw</code>, <code>mangle</code>, <code>nat</code>, <code>filter</code>, <code>security</code> 表）会被执行，用于对本地产生的数据包进行处理。</li>
</ul>
</li>
<li><strong>最终路由与 <code>NF_IP_POST_ROUTING</code>:</strong> 无论是本地产生的包还是需要转发的包，在最终确定所有 IP 头部字段（特别是经过了可能的 NAT 修改后）、选定出口网络设备，即将调用邻居子系统（ARP 或 NDP）解析下一跳 MAC 地址并将数据包传递给设备驱动程序之前，都会通过 <code>ip_output()</code>（单播）或 <code>ip_mc_output()</code>（多播）等函数，最终触发 <strong><code>NF_IP_POST_ROUTING</code></strong> 钩子。注册在此钩子上的 <code>Netfilter</code> 处理函数（来自 <code>mangle</code>, <code>nat</code> 表）会被执行。这是执行 SNAT 或进行最后修改的理想位置。</li>
<li><strong><code>dst_output()</code>:</strong> 最终，<code>dst_output(skb)</code> 函数会调用 <code>skb-&gt;dst-&gt;output</code> 指针指向的函数（如 <code>ip_output</code>），它会调用 <code>ip_finish_output()</code>，后者将 <code>sk_buff</code> 交给邻居子系统（<code>neigh_output</code>）和网络设备驱动进行 L2 封装和物理发送。</li>
</ul>
</li>
</ol>
<img src="/2025/04/10/kube-proxy/5.jpg" srcset="/img/loading.gif" lazyload class="" title="kube">

<p><em>TCP 层发送路径示意图，显示了数据包向下传递至 IP 层的过程，最终也会触发 Netfilter 的 OUT&#x2F;POSTROUTING 钩子。</em></p>
<h2 id="Netfilter-Hook-函数的注册与实现细节"><a href="#Netfilter-Hook-函数的注册与实现细节" class="headerlink" title="Netfilter Hook 函数的注册与实现细节"></a>Netfilter Hook 函数的注册与实现细节</h2><p><code>Netfilter</code> 框架的核心在于允许内核模块动态地注册和注销钩子处理函数。这使得内核的功能可以灵活扩展，而无需修改核心代码。</p>
<h3 id="注册和注销-Netfilter-Hook"><a href="#注册和注销-Netfilter-Hook" class="headerlink" title="注册和注销 Netfilter Hook"></a>注册和注销 Netfilter Hook</h3><p>注册一个钩子处理函数主要依赖于 <code>struct nf_hook_ops</code> 结构和 <code>nf_register_net_hook()</code> &#x2F; <code>nf_unregister_net_hook()</code> (或者针对特定网络命名空间的版本 <code>nf_register_hook</code> &#x2F; <code>nf_unregister_hook</code>) 函数。</p>
<p><code>struct nf_hook_ops</code> 结构定义在 <code>&lt;linux/netfilter.h&gt;</code> 中，其关键成员如下：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">nf_hook_ops</span> &#123;</span><br>    <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">list_head</span>    <span class="hljs-title">list</span>;</span>       <span class="hljs-comment">// 用于将 ops 链入 Netfilter 内部链表，内核管理</span><br><br>    <span class="hljs-comment">/* User fills in from here down. */</span><br>    nf_hookfn           *hook;      <span class="hljs-comment">// 指向实际处理数据包的函数指针</span><br>    <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">module</span>       *<span class="hljs-title">owner</span>;</span>     <span class="hljs-comment">// 指向拥有此 hook 的内核模块，用于引用计数</span><br>    <span class="hljs-type">u_int8_t</span>            pf;         <span class="hljs-comment">// 协议族 (Protocol Family)，如 PF_INET (IPv4), PF_INET6 (IPv6), PF_BRIDGE (桥接)</span><br>    <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span>        hooknum;    <span class="hljs-comment">// 指定要挂载的钩子点，如 NF_IP_PRE_ROUTING</span><br>    <span class="hljs-comment">/* Hooks are ordered in ascending priority. */</span><br>    <span class="hljs-type">int</span>                 priority;   <span class="hljs-comment">// 钩子函数的优先级，数值越小越先执行。内核定义了一些标准优先级，如 NF_IP_PRI_FIRST, NF_IP_PRI_FILTER, NF_IP_PRI_NAT_DST, NF_IP_PRI_NAT_SRC, NF_IP_PRI_LAST 等。</span><br>&#125;;<br></code></pre></td></tr></table></figure>

<ol>
<li><code>list</code>：此成员由Netfilter框架内部管理，用于将注册的<code>nf_hook_ops</code>组织成链表。对于在同一协议族（<code>pf</code>）和同一挂接点（<code>hooknum</code>）注册的多个hook函数，内核正是通过遍历这个链表来依次调用它们的。模块开发者在注册时无需关心此字段。</li>
<li><code>hook</code>：这是一个函数指针，指向类型为<code>nf_hookfn</code>的函数。这正是你的模块提供的核心处理逻辑，当匹配的网络数据包经过指定的<code>hooknum</code>时，内核将调用此函数。<code>nf_hookfn</code>的函数原型我们稍后会详细解析。</li>
<li><code>owner</code>：通常设置为<code>THIS_MODULE</code>宏，用于内核的模块引用计数管理。当模块被卸载时，内核可以通过这个指针自动注销其注册的hooks，防止出现悬挂指针导致系统崩溃。虽然示例代码中设置为<code>NULL</code>，但在生产级代码中，正确设置<code>owner</code>是保证系统稳定性的重要实践。</li>
<li><code>pf</code>：指定此hook函数适用的协议族（Protocol Family）。常见的协议族定义在<code>linux/socket.h</code>中，例如<code>PF_INET</code>代表IPv4协议栈，<code>PF_INET6</code>代表IPv6，<code>PF_BRIDGE</code>用于网桥等。你的hook函数只会处理属于指定协议族的数据包。</li>
<li><code>hooknum</code>：这明确了你的hook函数要挂载到Netfilter处理流程中的哪个具体位置。对于IPv4 (<code>PF_INET</code>)，这些挂接点（如<code>NF_INET_PRE_ROUTING</code>, <code>NF_INET_LOCAL_IN</code>, <code>NF_INET_FORWARD</code>, <code>NF_INET_LOCAL_OUT</code>, <code>NF_INET_POST_ROUTING</code>）定义在<code>linux/netfilter_ipv4.h</code>中，它们对应了数据包在内核中处理的不同阶段。</li>
<li><code>priority</code>：定义了在同一挂接点（<code>hooknum</code>）上注册的多个hook函数之间的执行优先级。优先级是一个整数，数值越小，优先级越高，越先被执行。内核提供了一系列预定义的优先级常量，例如<code>NF_IP_PRI_FIRST</code>（最高优先级）、<code>NF_IP_PRI_CONNTRACK</code>、<code>NF_IP_PRI_NAT_DST</code>、<code>NF_IP_PRI_FILTER</code>、<code>NF_IP_PRI_NAT_SRC</code>、<code>NF_IP_PRI_LAST</code>（最低优先级）等，定义在<code>linux/netfilter_ipv4.h</code>的<code>nf_ip_hook_priorities</code>枚举中。选择合适的优先级对于确保你的hook函数在正确的时机（例如，在NAT转换之前或之后）执行至关重要。</li>
</ol>
<p>要将你定义的<code>nf_hook_ops</code>结构体实例注册到Netfilter框架中，你需要调用<code>nf_register_net_hook()</code>函数（或者在较新内核中推荐使用针对特定netns的<code>nf_register_net_hook()</code>，如果你的模块需要感知网络命名空间的话；对于简单的全局hook，<code>nf_register_hook()</code>是一个历史接口，现在通常封装了<code>nf_register_net_hook(&amp;init_net, ops)</code>）。此函数接受一个指向<code>nf_hook_ops</code>结构体的指针作为参数。注册成功，你的hook函数就会成为内核网络处理流程的一部分。相应地，当你的内核模块卸载时，必须调用<code>nf_unregister_net_hook()</code>（或<code>nf_unregister_hook()</code>）并传入相同的<code>nf_hook_ops</code>结构体指针，以将其从Netfilter框架中移除，释放资源并避免潜在的错误。</p>
<p>下面的示例代码演示了这一注册与注销过程。它注册了一个简单的hook函数，挂载在IPv4协议栈的<code>NF_INET_LOCAL_OUT</code>挂接点（即本机进程发出的数据包在路由决策之后、发送到网络接口之前的位置），并赋予其最高优先级(<code>NF_IP_PRI_FIRST</code>)。该hook函数的实现极为简单粗暴：直接返回<code>NF_DROP</code>，这意味着所有经由此挂接点的IPv4数据包都将被丢弃。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;linux/kernel.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;linux/init.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;linux/module.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;linux/version.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;linux/skbuff.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;linux/netfilter.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;linux/netfilter_ipv4.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;linux/ip.h&gt;</span> <span class="hljs-comment">// Include for iphdr structure access if needed later</span></span><br><br>MODULE_LICENSE(<span class="hljs-string">&quot;GPL&quot;</span>);<br>MODULE_AUTHOR(<span class="hljs-string">&quot;xsc&quot;</span>);<br><br><span class="hljs-type">static</span> <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">nf_hook_ops</span> <span class="hljs-title">nfho</span>;</span><br><br><span class="hljs-comment">// The hook function itself</span><br><span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> <span class="hljs-title function_">hook_func</span><span class="hljs-params">(<span class="hljs-type">void</span> *priv, <span class="hljs-comment">// priv is unused in this simplified example, corresponds to nf_hook_ops</span></span><br><span class="hljs-params">                       <span class="hljs-keyword">struct</span> sk_buff *skb,</span><br><span class="hljs-params">                       <span class="hljs-type">const</span> <span class="hljs-keyword">struct</span> nf_hook_state *state)</span> <span class="hljs-comment">// Modern prototype uses nf_hook_state</span><br>&#123;<br>    <span class="hljs-comment">// In older kernels or simpler contexts, the prototype might be:</span><br>    <span class="hljs-comment">// unsigned int hook_func(unsigned int hooknum, struct sk_buff *skb,</span><br>    <span class="hljs-comment">//                        const struct net_device *in, const struct net_device *out,</span><br>    <span class="hljs-comment">//                        int (*okfn)(struct sk_buff *))</span><br><br>    printk(KERN_INFO <span class="hljs-string">&quot;Packet dropped by example hook!\n&quot;</span>); <span class="hljs-comment">// Good practice to log actions</span><br>    <span class="hljs-keyword">return</span> NF_DROP; <span class="hljs-comment">// Discard the packet</span><br>    <span class="hljs-comment">// Other possible return values:</span><br>    <span class="hljs-comment">// NF_ACCEPT: Continue processing the packet normally.</span><br>    <span class="hljs-comment">// NF_STOLEN: The hook function has taken ownership of the packet (e.g., queued it for userspace),</span><br>    <span class="hljs-comment">//            Netfilter should stop processing it.</span><br>    <span class="hljs-comment">// NF_QUEUE: Queue the packet for userspace processing (used by tools like iptables -j QUEUE).</span><br>    <span class="hljs-comment">// NF_REPEAT: Call this hook function again (use with caution).</span><br>&#125;<br><br><br><span class="hljs-type">static</span> <span class="hljs-type">int</span> __init <span class="hljs-title function_">kexec_test_init</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span><br>&#123;<br>    printk(KERN_INFO <span class="hljs-string">&quot;kexec test module loading...\n&quot;</span>);<br><br>    nfho.hook = hook_func;<br>    <span class="hljs-comment">// nfho.owner = THIS_MODULE; // Recommended practice</span><br>    nfho.pf = PF_INET;                   <span class="hljs-comment">// Target IPv4 protocol family</span><br>    nfho.hooknum = NF_INET_LOCAL_OUT;    <span class="hljs-comment">// Hook at the local output stage</span><br>    nfho.priority = NF_IP_PRI_FIRST;     <span class="hljs-comment">// Execute with the highest priority</span><br><br>    <span class="hljs-comment">// Register the hook for the initial network namespace</span><br><span class="hljs-meta">#<span class="hljs-keyword">if</span> LINUX_VERSION_CODE &gt;= KERNEL_VERSION(4, 13, 0)</span><br>    <span class="hljs-type">int</span> ret = nf_register_net_hook(&amp;init_net, &amp;nfho);<br><span class="hljs-meta">#<span class="hljs-keyword">else</span></span><br>    <span class="hljs-type">int</span> ret = nf_register_hook(&amp;nfho); <span class="hljs-comment">// Older kernel registration</span><br><span class="hljs-meta">#<span class="hljs-keyword">endif</span></span><br><br>    <span class="hljs-keyword">if</span> (ret &lt; <span class="hljs-number">0</span>) &#123;<br>        printk(KERN_ERR <span class="hljs-string">&quot;Failed to register Netfilter hook: %d\n&quot;</span>, ret);<br>        <span class="hljs-keyword">return</span> ret;<br>    &#125;<br><br>    printk(KERN_INFO <span class="hljs-string">&quot;Netfilter hook registered successfully.\n&quot;</span>);<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>; <span class="hljs-comment">// Success</span><br>&#125;<br><br><span class="hljs-type">static</span> <span class="hljs-type">void</span> __exit <span class="hljs-title function_">kexec_test_exit</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span><br>&#123;<br>    printk(KERN_INFO <span class="hljs-string">&quot;kexec test module unloading...\n&quot;</span>);<br><span class="hljs-meta">#<span class="hljs-keyword">if</span> LINUX_VERSION_CODE &gt;= KERNEL_VERSION(4, 13, 0)</span><br>    nf_unregister_net_hook(&amp;init_net, &amp;nfho);<br><span class="hljs-meta">#<span class="hljs-keyword">else</span></span><br>    nf_unregister_hook(&amp;nfho); <span class="hljs-comment">// Older kernel unregistration</span><br><span class="hljs-meta">#<span class="hljs-keyword">endif</span></span><br>    printk(KERN_INFO <span class="hljs-string">&quot;Netfilter hook unregistered.\n&quot;</span>);<br>&#125;<br><br>module_init(kexec_test_init); <span class="hljs-comment">// Register the init function</span><br>module_exit(kexec_test_exit); <span class="hljs-comment">// Register the exit function</span><br><br></code></pre></td></tr></table></figure>

<p><em>(Note: The provided example code was slightly adjusted for better practice (logging, <code>THIS_MODULE</code>, modern registration API), but kept the core logic as requested. The original prototype of the hook function is also mentioned for context.)</em></p>
<h3 id="Hook函数的实现细节"><a href="#Hook函数的实现细节" class="headerlink" title="Hook函数的实现细节"></a>Hook函数的实现细节</h3><p>理解<code>nf_hookfn</code>的原型对于编写有效的Netfilter hook至关重要。虽然原型在不同内核版本中略有演变，但核心传递的信息保持一致。我们来看一个常见的（略旧但更易于解释基础概念的）原型，定义在<code>linux/netfilter.h</code>：</p>
<C>

<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-keyword">typedef</span> <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> <span class="hljs-title function_">nf_hookfn</span><span class="hljs-params">(<span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> hooknum,<span class="hljs-keyword">struct</span> sk_buff *skb,<span class="hljs-type">const</span> <span class="hljs-keyword">struct</span> net_device *in,<span class="hljs-type">const</span> <span class="hljs-keyword">struct</span> net_device *out,<span class="hljs-type">int</span> (*okfn)(<span class="hljs-keyword">struct</span> sk_buff *))</span>;<br></code></pre></td></tr></table></figure>

<p><em>(较新内核倾向于使用 <code>nf_hookfn(void \*priv, struct sk_buff \*skb, const struct nf_hook_state \*state)</code>，其中<code>state</code>结构体封装了<code>hooknum</code>, <code>in</code>, <code>out</code>, <code>sk</code>等信息，<code>priv</code>通常指向注册时<code>nf_hook_ops</code>结构体本身)</em></p>
<p>让我们解析这个原型中的参数：</p>
<ul>
<li><code>hooknum</code>: 这个无符号整数明确告知hook函数当前是在哪个Netfilter挂接点被调用的（例如 <code>NF_INET_PRE_ROUTING</code>）。这使得一个函数可以服务于多个挂接点，并根据<code>hooknum</code>执行不同的逻辑。</li>
<li><code>skb</code>: 这是指向<code>struct sk_buff</code>的指针，是Linux内核中表示网络数据包的核心数据结构。<code>sk_buff</code>（Socket Buffer）不仅仅包含数据包的原始负载，还携带了大量的元数据，如协议头指针、路由信息、时间戳、关联的socket等。访问和操作<code>skb</code>是Netfilter hook函数进行包检查和修改的基础。</li>
<li><code>in</code>: 指向<code>struct net_device</code>的指针，代表数据包进入系统的网络接口。这个参数<strong>仅在数据包是接收路径上的特定挂接点</strong>（如<code>NF_INET_PRE_ROUTING</code>, <code>NF_INET_LOCAL_IN</code>）才有意义，此时它指向接收该数据包的物理或虚拟网络设备。在其他挂接点（如<code>NF_INET_LOCAL_OUT</code>, <code>NF_INET_POST_ROUTING</code>），<code>in</code>通常为<code>NULL</code>。</li>
<li><code>out</code>: 同样是指向<code>struct net_device</code>的指针，代表数据包计划离开系统的网络接口。这个参数<strong>仅在数据包是发送路径上的特定挂接点</strong>（如<code>NF_INET_LOCAL_OUT</code>, <code>NF_INET_POST_ROUTING</code>）才有意义，指向数据包将被发送出去的网络设备。在接收相关的挂接点，<code>out</code>通常为<code>NULL</code>。理解<code>in</code>和<code>out</code>的有效性取决于<code>hooknum</code>是至关重要的。</li>
<li><code>okfn</code>: 这是一个函数指针，原型为<code>int (*okfn)(struct sk_buff *)</code>。在某些复杂的Netfilter场景（尤其是在旧版本或特定子系统中），它指向下一个应该处理该数据包的函数或决策点。然而，在大多数现代的、简单的过滤场景下，hook函数的行为主要由其返回值（如 <code>NF_ACCEPT</code>, <code>NF_DROP</code>）决定，<code>okfn</code>的使用并不普遍，可以直接忽略。</li>
</ul>
<h3 id="Netfilter-报文过滤技术实现"><a href="#Netfilter-报文过滤技术实现" class="headerlink" title="Netfilter 报文过滤技术实现"></a>Netfilter 报文过滤技术实现</h3><p>Netfilter 构成了 Linux 内核网络栈的核心部分，它提供了一套强大的钩子（Hooks）机制，允许内核模块在数据包流经网络协议栈的关键路径点上注册回调函数，从而实现对数据包的检查、修改、丢弃或重新注入等操作。报文过滤是 Netfilter 最为经典和广泛的应用之一，诸如 iptables、nftables 等用户空间工具正是基于 Netfilter 框架来实现防火墙功能的。接下来，我们将深入探讨几种基于 Netfilter 实现报文过滤的具体技术途径。</p>
<h4 id="基于网络接口的过滤"><a href="#基于网络接口的过滤" class="headerlink" title="基于网络接口的过滤"></a>基于网络接口的过滤</h4><p>在网络数据包的处理流程中，识别数据包的来源或目的地网络接口是一项基本的过滤需求。Linux 内核中使用 <code>struct net_device</code> 结构来抽象表示一个网络接口（如 eth0, lo 等）。每个流经网络栈的数据包都由一个 <code>struct sk_buff</code> (skb) 结构体表示，该结构体内部包含了指向相关网络设备的指针，通常是 <code>skb-&gt;dev</code>，它指向数据包接收或即将发送的 <code>struct net_device</code>。因此，在 Netfilter 的钩子函数中（例如注册在 <code>NF_INET_PRE_ROUTING</code> 或 <code>NF_INET_LOCAL_IN</code> 钩子点），我们可以访问这个 <code>skb-&gt;dev</code> 指针，并进一步读取其 <code>name</code> 成员，这是一个包含了接口名称（如 “eth0”）的字符串。通过将此名称与我们预设的过滤规则中的接口名进行比较，就可以实现基于接口的过滤策略。例如，如果我们的策略是阻止所有进入 “eth0” 接口的数据包，那么在钩子函数中，当检测到 <code>skb-&gt;dev-&gt;name</code> 与 “eth0” 匹配时，函数直接返回 <code>NF_DROP</code> 即可。这个返回值会通知 Netfilter 框架立即丢弃该数据包，并释放相关的 <code>sk_buff</code> 资源，数据包将不再继续在网络栈中传递。</p>
<h4 id="基于-IP-地址的过滤"><a href="#基于-IP-地址的过滤" class="headerlink" title="基于 IP 地址的过滤"></a>基于 IP 地址的过滤</h4><p>基于源或目的 IP 地址进行过滤是防火墙最核心的功能之一。Netfilter 同样为此提供了便捷的实现方式。<code>sk_buff</code> 结构体中包含了指向各层协议头部的指针。网络层（IP层）的头部指针可以通过 <code>skb-&gt;network_header</code> 访问，或者更常用的方式是使用内核提供的辅助宏 <code>ip_hdr(skb)</code> 来获取指向 <code>struct iphdr</code>（定义于 <code>&lt;linux/ip.h&gt;</code>）的指针。这个结构体详细定义了 IPv4 报头的所有字段，包括源 IP 地址 (<code>saddr</code>) 和目的 IP 地址 (<code>daddr</code>)。需要注意的是，这些地址在内存中是以网络字节序（Big Endian）存储的。在 Netfilter 钩子函数中，我们可以提取出这些地址字段，并将其与过滤规则中定义的特定 IP 地址或地址范围进行比较（比较时通常需要使用 <code>ntohl()</code> 等函数将网络字节序转换为主机字节序，或者直接以网络字节序进行比较）。如果数据包的源地址或目的地址满足了我们设定的丢弃条件（例如，阻止来自某个特定恶意 IP 的所有连接请求），钩子函数便返回 <code>NF_DROP</code>，从而实现对该数据包的精确过滤。</p>
<h4 id="基于-TCP-端口的过滤"><a href="#基于-TCP-端口的过滤" class="headerlink" title="基于 TCP 端口的过滤"></a>基于 TCP 端口的过滤</h4><p>当需要进行更细粒度的控制，例如阻止对特定服务端口的访问时，就需要深入到传输层进行过滤。对于 TCP 协议而言，这意味着我们需要检查 TCP 头部中的源端口和目的端口。在 Netfilter 钩子函数中，这通常发生在确认了数据包是 IP 包（通过检查 <code>iphdr</code>）并且其协议字段 (<code>iph-&gt;protocol</code>) 指示为 <code>IPPROTO_TCP</code> 之后。要获取 TCP 头部的指针，我们需要知道 IP 头部的实际长度，因为 IP 头部可能包含选项，长度并非固定。<code>struct iphdr</code> 中的 <code>ihl</code> (Internet Header Length) 字段表示 IP 头部的长度，但其单位是 4 字节（32位字）。因此，TCP 头部的起始位置可以通过将 IP 头部指针（<code>iph</code>）加上 IP 头部的字节长度（<code>iph-&gt;ihl * 4</code>）来计算得到。获取到指向 <code>struct tcphdr</code>（定义于 <code>&lt;linux/tcp.h&gt;</code>）的指针后，我们就可以访问其成员，如源端口 (<code>source</code>) 和目的端口 (<code>dest</code>)。这些端口号同样是以网络字节序存储的。下面的代码片段展示了一个简单的示例，演示了如何在 Netfilter 钩子函数中检查 TCP 目的端口，并在端口号为 25 (SMTP 服务的默认端口) 时丢弃数据包：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;linux/ip.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;linux/tcp.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;linux/netfilter.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;linux/netfilter_ipv4.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;linux/skbuff.h&gt;</span></span><br><br><span class="hljs-type">static</span> <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> <span class="hljs-title function_">check_tcp_packet_hook</span><span class="hljs-params">(<span class="hljs-type">void</span> *priv,</span><br><span class="hljs-params">                                          <span class="hljs-keyword">struct</span> sk_buff *skb,</span><br><span class="hljs-params">                                          <span class="hljs-type">const</span> <span class="hljs-keyword">struct</span> nf_hook_state *state)</span><br>&#123;<br>    <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">tcphdr</span> *<span class="hljs-title">tcph</span> =</span> <span class="hljs-literal">NULL</span>;<br>    <span class="hljs-type">const</span> <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">iphdr</span> *<span class="hljs-title">iph</span> =</span> <span class="hljs-literal">NULL</span>;<br>    __be16 dport;<br><br>    <span class="hljs-comment">// skb 为空则直接接受，虽然理论上 Netfilter 不应传递空 skb</span><br>    <span class="hljs-keyword">if</span> (!skb) &#123;<br>        <span class="hljs-keyword">return</span> NF_ACCEPT;<br>    &#125;<br><br>    <span class="hljs-comment">// 获取 IP 头部指针</span><br>    iph = ip_hdr(skb);<br>    <span class="hljs-comment">// 检查是否为 NULL 以及是否为 IPv4 包（通常在注册钩子时已指定协议族）</span><br>    <span class="hljs-keyword">if</span> (!iph) &#123;<br>        <span class="hljs-keyword">return</span> NF_ACCEPT;<br>    &#125;<br><br>    <span class="hljs-comment">// 检查 IP 协议字段是否为 TCP</span><br>    <span class="hljs-keyword">if</span> (iph-&gt;protocol == IPPROTO_TCP) &#123;<br>        <span class="hljs-comment">// 计算 TCP 头部的起始位置</span><br>        <span class="hljs-comment">// 注意：这里直接使用 iph 指针，因为 ip_hdr(skb) 返回的就是 skb-&gt;network_header</span><br>        <span class="hljs-comment">// (void *)iph + iph-&gt;ihl * 4 是计算 TCP 头地址的标准方式</span><br>        tcph = (<span class="hljs-keyword">struct</span> tcphdr *)((__u8 *)iph + (iph-&gt;ihl * <span class="hljs-number">4</span>));<br><br>        <span class="hljs-comment">// skb-&gt;transport_header 可能未被设置，或者我们需要处理 IP 选项等复杂情况，</span><br>        <span class="hljs-comment">// 直接计算偏移量是更可靠的方式。</span><br>        <span class="hljs-comment">// 还需要确保 skb 足够长，包含完整的 TCP 头，这里为简化省略了长度检查。</span><br><br>        <span class="hljs-comment">// 获取 TCP 目的端口 (网络字节序)</span><br>        dport = tcph-&gt;dest;<br><br>        <span class="hljs-comment">// 将目的端口从网络字节序转换为主机字节序进行比较</span><br>        <span class="hljs-keyword">if</span> (ntohs(dport) == <span class="hljs-number">25</span>) &#123;<br>            printk(KERN_INFO <span class="hljs-string">&quot;Dropping TCP packet to port 25\n&quot;</span>); <span class="hljs-comment">// 日志记录，便于调试</span><br>            <span class="hljs-keyword">return</span> NF_DROP; <span class="hljs-comment">// 丢弃目标端口为 25 的 TCP 包</span><br>        &#125; <span class="hljs-keyword">else</span> &#123;<br>            <span class="hljs-comment">// 如果端口不是 25，则接受该 TCP 包</span><br>            <span class="hljs-keyword">return</span> NF_ACCEPT;<br>        &#125;<br>    &#125;<br><br>    <span class="hljs-comment">// 如果不是 TCP 包，则接受（根据具体策略可能需要调整）</span><br>    <span class="hljs-keyword">return</span> NF_ACCEPT;<br>&#125;<br><br><span class="hljs-comment">// 注意：实际使用中需要将此函数注册到 Netfilter 钩子点，例如：</span><br><span class="hljs-comment">// static struct nf_hook_ops tcp_filter_hook_ops = &#123;</span><br><span class="hljs-comment">//     .hook     = check_tcp_packet_hook,</span><br><span class="hljs-comment">//     .pf       = NFPROTO_IPV4,</span><br><span class="hljs-comment">//     .hooknum  = NF_INET_PRE_ROUTING, // 或者其他合适的钩子点</span><br><span class="hljs-comment">//     .priority = NF_IP_PRI_FIRST,    // 优先级</span><br><span class="hljs-comment">// &#125;;</span><br><span class="hljs-comment">// 在模块初始化时 nf_register_net_hook(&amp;init_net, &amp;tcp_filter_hook_ops);</span><br><span class="hljs-comment">// 在模块退出时 nf_unregister_net_hook(&amp;init_net, &amp;tcp_filter_hook_ops);</span><br></code></pre></td></tr></table></figure>

<p>在这个示例代码 <code>check_tcp_packet_hook</code>（调整了函数签名以匹配 Netfilter 钩子原型）中，我们首先通过 <code>ip_hdr(skb)</code> 获取 IP 头指针 <code>iph</code>。然后，检查 <code>iph-&gt;protocol</code> 是否等于 <code>IPPROTO_TCP</code>。如果是 TCP 包，就通过 IP 头长度 <code>iph-&gt;ihl * 4</code> 计算出 TCP 头 <code>tcphdr</code> 的地址。接着，提取目的端口 <code>tcph-&gt;dest</code>，并使用 <code>ntohs()</code> 将其从网络字节序转换为主机字节序。最后，将转换后的端口号与 25 进行比较，如果相等，则返回 <code>NF_DROP</code> 丢弃该包；否则，返回 <code>NF_ACCEPT</code> 允许该包继续传递。对于非 TCP 包或者不满足丢弃条件的 TCP 包，函数默认返回 <code>NF_ACCEPT</code>。这种基于传输层端口的过滤能力，使得我们可以对特定的网络服务进行精细化的访问控制。</p>
<h2 id="Kube-Proxy-工作原理"><a href="#Kube-Proxy-工作原理" class="headerlink" title="Kube-Proxy 工作原理"></a>Kube-Proxy 工作原理</h2><h4 id=""><a href="#" class="headerlink" title=""></a><img src="/2025/04/10/kube-proxy/image-20250412133507210.png" srcset="/img/loading.gif" lazyload class="" title="image-20250412133507210"></h4><h3 id="kube-proxy-的核心职责与解决的问题"><a href="#kube-proxy-的核心职责与解决的问题" class="headerlink" title="kube-proxy 的核心职责与解决的问题"></a>kube-proxy 的核心职责与解决的问题</h3><p>在 Kubernetes 集群中，<strong>Pod 是短暂的，它们的 IP 地址会随着 Pod 的创建和销毁而改变</strong>。然而，应用程序（无论是集群内部的客户端还是外部的客户端）需要一个稳定的方式来访问这些可能不断变化的 Pod 集合提供的服务。这就是 Kubernetes <code>Service</code> 对象解决的问题，它提供了一个稳定的虚拟 IP（ClusterIP）和端口，作为访问后端 Pod 的入口。</p>
<p><code>kube-proxy</code> 正是实现 <code>Service</code> 概念的关键组件。它并不是传统意义上的用户空间代理（尽管它曾经有过 userspace 模式，但现在已不常用且效率低下），而是一个运行在每个 Node 上的网络代理和负载均衡器。它的核心职责是 <strong>监视（watch）<code>kube-apiserver</code> 上关于 <code>Service</code> 和 <code>Endpoints</code> (或更现代的 <code>EndpointSlice</code>) 对象的变化，并将这些变化转化为节点本地的网络规则</strong>，从而确保发送到 Service IP 的流量能够被正确地路由和负载均衡到后端健康的 Pod 上。简而言之，<code>kube-proxy</code> 让 Service 的虚拟 IP 地址真正“可用”。</p>
<h3 id="基于图示的工作原理解析-iptables-模式"><a href="#基于图示的工作原理解析-iptables-模式" class="headerlink" title="基于图示的工作原理解析 (iptables 模式)"></a>基于图示的工作原理解析 (iptables 模式)</h3><p>这张图清晰地展示了 <code>kube-proxy</code> 在 <code>iptables</code> 模式下的工作流程（这也是最常见和默认的模式之一）。</p>
<ol>
<li><p><strong>信息获取</strong>: <code>kube-proxy</code> 作为一个客户端，持续地通过 List-Watch 机制与 <code>kube-apiserver</code> 通信。它关注 <code>Service</code> 对象的创建、更新和删除，以及与之关联的 <code>Endpoints</code> 或 <code>EndpointSlice</code> 对象的变化。<code>Endpoints</code> 对象记录了构成一个 <code>Service</code> 的所有后端 Pod 的实际 IP 地址和端口。</p>
</li>
<li><p><strong>规则生成</strong>: 当 <code>kube-proxy</code> 检测到变化时（例如，一个新的 Service 被创建，或者一个 Pod 变成 Ready 状态并被添加到 Service 的 Endpoints 中），它会在其所在的 Node 上修改网络规则。在 <code>iptables</code> 模式下，它主要利用 Linux 内核的 <code>netfilter</code> 框架，通过操作 <code>iptables</code> 规则来实现 Service 的虚拟 IP 和端口到后端 Pod IP 和端口的转换。</p>
</li>
<li><p><strong>规则内容</strong>: 图中的 <code>IPtables</code> 云状图形内部标注了 <code>ServiceIP rule</code>, <code>NodePort rule</code>, <code>LB IP rule</code>，这代表了 <code>kube-proxy</code> 生成的主要规则类型：</p>
<ul>
<li><strong>ServiceIP rule</strong>: 这是针对 <code>ClusterIP</code> 类型 Service 的核心规则。当一个数据包的目的地址是某个 Service 的 ClusterIP 和端口时，<code>iptables</code> 规则（通常位于 <code>nat</code> 表的 <code>PREROUTING</code> 链用于处理节点外部流量，<code>OUTPUT</code> 链用于处理节点内部发起的流量）会执行 <strong>DNAT (Destination Network Address Translation)</strong>。它会从该 Service 对应的健康 Endpoints 列表中选择一个 Pod 的 IP 和端口，并将数据包的目标地址修改为选中的 Pod IP 和端口。同时，它会利用 <code>iptables</code> 的模块（如 <code>statistic</code> 模式的 <code>random</code> 或 <code>nth</code>）来实现简单的负载均衡（默认是随机选择）。</li>
<li><strong>NodePort rule</strong>: 对于 <code>NodePort</code> 类型的 Service，除了上述 ClusterIP 的规则外，<code>kube-proxy</code> 还会配置 <code>iptables</code> 规则来监听在所有节点（或指定节点）上的特定端口（NodePort）。当外部流量访问 <code>NodeIP:NodePort</code> 时，<code>iptables</code> 规则（同样在 <code>nat</code> 表的 <code>PREROUTING</code> 链）会进行 DNAT，将流量转发给一个后端 Pod。</li>
<li><strong>LB IP rule</strong>: 对于 <code>LoadBalancer</code> 类型的 Service，云服务商提供的外部负载均衡器（图中的 <code>Load Balancer</code>）通常会将流量导向集群中节点的 <code>NodePort</code>。因此，<code>kube-proxy</code> 产生的 <code>NodePort</code> 规则间接支持了 <code>LoadBalancer</code> 服务。此外，<code>kube-proxy</code> 也会处理直接发送到分配给 Service 的外部 IP (LoadBalancer Ingress IP) 的流量（如果这个 IP 的流量被路由到了节点上），同样通过 DNAT 转发到后端 Pod。</li>
</ul>
</li>
<li><p><strong>流量转发</strong>:</p>
<ul>
<li><strong>Internal Client</strong>: 集群内部的 Pod（图中的 <code>Internal client</code>）访问 Service 时，流量的目的地址是 Service 的 ClusterIP。如果该 Pod 与目标 Pod 在同一节点，流量可能经过 <code>OUTPUT</code> 链的 <code>iptables</code> 规则进行 DNAT；如果不在同一节点，流量发出后到达目标 Pod 所在节点，经过 <code>PREROUTING</code> 链的 <code>iptables</code> 规则进行 DNAT，最终被转发到选定的 <code>Backend pod</code>（如 Pod 1, Pod 2, Pod 3）。</li>
<li><strong>External Client</strong>: 外部客户端访问 <code>NodePort</code> 或 <code>LoadBalancer</code> IP 时，流量首先到达某个 Node。节点上的 <code>iptables</code> 规则（<code>PREROUTING</code> 链）捕获该流量，执行 DNAT，将其重定向到后端 Pod。</li>
</ul>
</li>
</ol>
<h3 id="其他模式：IPVS"><a href="#其他模式：IPVS" class="headerlink" title="其他模式：IPVS"></a>其他模式：IPVS</h3><p>值得注意的是，除了 <code>iptables</code> 模式，<code>kube-proxy</code> 还支持 <code>IPVS</code> (IP Virtual Server) 模式。IPVS 也是构建在 <code>netfilter</code> 之上，但在 Linux 内核中提供了专门用于负载均衡的功能层。相比 <code>iptables</code>，IPVS 通常在大规模集群（大量 Service）下具有更好的性能和更低的延迟，因为它使用哈希表来查找服务，而不是像 <code>iptables</code> 那样需要线性遍历规则链。在 IPVS 模式下，<code>kube-proxy</code> 会使用 <code>netlink</code> 接口与内核的 IPVS 模块交互，创建虚拟服务器（对应 Service）和真实服务器（对应 Endpoints），并提供更丰富的负载均衡算法（如轮询、最少连接等）。工作流程的核心思想类似：监听 API Server，将 Service&#x2F;Endpoints 信息转化为内核中的 IPVS 规则。</p>
<h2 id="Kube-proxy-与-iptables-模式概述"><a href="#Kube-proxy-与-iptables-模式概述" class="headerlink" title="Kube-proxy 与 iptables 模式概述"></a>Kube-proxy 与 iptables 模式概述</h2><p>在 Kubernetes 集群中，Service 提供了一个稳定的访问入口（虚拟 IP 地址 ClusterIP 和端口），用于访问后端一组动态变化的 Pod。<code>kube-proxy</code> 是运行在每个 Node 上的一个关键组件，它的核心职责之一就是实现 Service 的概念。当 <code>kube-proxy</code> 配置为 <code>iptables</code> 模式时（这是早期和广泛使用的一种模式），它会监听 Kubernetes API Server 关于 Service 和 Endpoint (Pod IP + Port) 的变化，并将这些信息转化为 Linux 内核 Netfilter 子系统中的 <code>iptables</code> 规则。这些规则共同作用，拦截发往 Service IP 的流量，并将其负载均衡地转发到后端健康的 Pod 上。其核心思想是利用 <code>iptables</code> 的 DNAT（Destination Network Address Translation）和 SNAT（Source Network Address Translation，通常通过 MASQUERADE 实现）功能，结合用户自定义的 chain 来管理复杂的转发逻辑。</p>
<h3 id="来自其他主机的请求：PREROUTING-链的处理"><a href="#来自其他主机的请求：PREROUTING-链的处理" class="headerlink" title="来自其他主机的请求：PREROUTING 链的处理"></a>来自其他主机的请求：PREROUTING 链的处理</h3><p>当一个网络数据包从集群外部或者集群内的另一个节点（“其他主机”）发送到当前节点，并且其目标是本节点上的某个 Service 时，这个数据包首先会经过 Linux 内核网络栈的 <code>PREROUTING</code> 链。这是 Netfilter 框架在进行路由决策 <em>之前</em> 处理数据包的第一个钩子点。<code>kube-proxy</code> 在 <code>nat</code> 表的 <code>PREROUTING</code> 链中插入了一条或多条规则。</p>
<p>这些规则会检查数据包的目标 IP 和端口。如果数据包的目标是某个 Service 的 ClusterIP 和端口，或者目标是本节点的 IP 加上某个 Service 暴露的 NodePort，那么这个数据包就会被匹配到。根据你图示的逻辑，入口通常会跳转（<code>JUMP</code>）到 <code>KUBE-SERVICES</code> 这个自定义链。<code>KUBE-SERVICES</code> 链是 <code>kube-proxy</code> 管理 Service 规则的总入口。</p>
<h3 id="KUBE-SERVICES-链：Service-的分发中心"><a href="#KUBE-SERVICES-链：Service-的分发中心" class="headerlink" title="KUBE-SERVICES 链：Service 的分发中心"></a>KUBE-SERVICES 链：Service 的分发中心</h3><p><code>KUBE-SERVICES</code> 链扮演着一个分发中心的角色。它包含了一系列的规则，每一条规则对应一个 Kubernetes Service。</p>
<ul>
<li>如果数据包的目标是某个 Service 的 ClusterIP (<code>-d clusterIP</code>)，<code>iptables</code> 规则会直接将这个数据包跳转到该 Service 对应的具体处理链，通常命名为 <code>KUBE-SVC-&lt;service_hash&gt;</code>。</li>
<li>如果数据包的目标是当前节点的 IP 且目标端口是某个 Service 暴露的 NodePort (<code>--dport nodePort</code>)，<code>iptables</code> 规则会先跳转到 <code>KUBE-NODEPORTS</code> 链。</li>
</ul>
<h3 id="KUBE-NODEPORTS-链：处理-NodePort-类型的-Service"><a href="#KUBE-NODEPORTS-链：处理-NodePort-类型的-Service" class="headerlink" title="KUBE-NODEPORTS 链：处理 NodePort 类型的 Service"></a>KUBE-NODEPORTS 链：处理 NodePort 类型的 Service</h3><p><code>KUBE-NODEPORTS</code> 链专门处理通过 NodePort 访问的流量。它内部也包含对应各个 NodePort Service 的规则。对于匹配到的 NodePort 流量，通常会执行两个关键动作：</p>
<ol>
<li>跳转到 <code>KUBE-MARK-MASQ</code> 链。这个链的作用是给数据包打上一个特殊的 Netfilter 标记（mark），通常是 <code>0x4000</code>。这个标记非常重要，它向后续的 <code>POSTROUTING</code> 链表明这个数据包需要进行源地址伪装（SNAT&#x2F;Masquerade）。这么做的原因是，当数据包经过 DNAT 转发给 Pod 后，如果不修改源 IP，Pod 回复数据包时会直接发给原始客户端 IP。如果客户端在集群外部，且 Pod IP 是私有地址，这个回复可能无法正确路由回去。通过 Masquerade 将源 IP 改为 Node 的 IP，可以确保回复流量能正确返回到 Node，Node 再根据连接跟踪记录转发给原始客户端。</li>
<li>跳转到该 NodePort Service 对应的 ClusterIP Service 处理链，即 <code>KUBE-SVC-&lt;service_hash&gt;</code>。这样，无论是通过 ClusterIP 还是 NodePort 访问，最终都会汇聚到同一个 Service 处理逻辑中。</li>
</ol>
<h3 id="KUBE-SVC-hash-链：实现负载均衡"><a href="#KUBE-SVC-hash-链：实现负载均衡" class="headerlink" title="KUBE-SVC-hash 链：实现负载均衡"></a>KUBE-SVC-hash 链：实现负载均衡</h3><p><code>KUBE-SVC-&lt;service_hash&gt;</code> 链负责将流量分发到 Service 后端的多个 Pod (Endpoints)。这里的 <code>&lt;service_hash&gt;</code> 是根据 Service 名称和命名空间等信息计算出的唯一标识。<br>在此链中，<code>kube-proxy</code> 会为每个健康的 Endpoint 创建一条对应的规则，并根据 Service 的配置（例如 <code>sessionAffinity</code>）以及 <code>kube-proxy</code> 的负载均衡策略（图中显示为 <code>--mode random</code>）来决定如何选择 Endpoint。<br>在 <code>random</code> 模式下，会使用 <code>statistic</code> 模块和概率（<code>--probability</code>）来实现随机负载均衡。如果有 N 个 Endpoints，每个 Endpoint 对应的规则会有大约 <code>1/N</code> 的概率被选中。选中的规则会跳转到该 Endpoint 对应的处理链，通常命名为 <code>KUBE-SEP-&lt;endpoint_hash&gt;</code>（SEP 代表 Service EndPoint）。</p>
<h3 id="KUBE-SEP-hash-链：执行-DNAT"><a href="#KUBE-SEP-hash-链：执行-DNAT" class="headerlink" title="KUBE-SEP-hash 链：执行 DNAT"></a>KUBE-SEP-hash 链：执行 DNAT</h3><p><code>KUBE-SEP-&lt;endpoint_hash&gt;</code> 链是最终执行地址转换的地方。这里的 <code>&lt;endpoint_hash&gt;</code> 是根据具体 Pod IP 和端口计算的哈希。<br>此链包含的关键规则是：</p>
<ol>
<li>（可选但常见）再次跳转到 <code>KUBE-MARK-MASQ</code> 链。这是为了确保即使流量是直接访问 ClusterIP（例如来自集群内部其他 Pod），如果这个 Service 配置了某些需要 SNAT 的策略（比如 <code>externalTrafficPolicy: Cluster</code> 时，来自外部的流量被转发到其他节点上的 Pod），或者某些 CNI 插件需要，也可能需要打上 Masquerade 标记。</li>
<li>执行 <code>DNAT</code> 操作。这条规则使用 <code>DNAT</code> target，将数据包的目标 IP 和端口修改为选定 Endpoint 的实际 Pod IP 和 <code>targetPort</code> (<code>--to-destination &lt;pod_ip&gt;:&lt;pod_port&gt;</code>)。</li>
</ol>
<p>完成 DNAT 后，数据包的目标地址就变成了具体的 Pod IP。内核会根据新的目标地址重新进行路由决策，并将数据包转发给目标 Pod。</p>
<h3 id="本地-Pod-发起的请求：OUTPUT-链的处理"><a href="#本地-Pod-发起的请求：OUTPUT-链的处理" class="headerlink" title="本地 Pod 发起的请求：OUTPUT 链的处理"></a>本地 Pod 发起的请求：OUTPUT 链的处理</h3><p>当集群内的一个 Pod（本地 Pod）尝试访问同一个节点上的或其他节点上的 Service 时，其发出的数据包会经过该节点内核网络栈的 <code>OUTPUT</code> 链。这是在路由决策之后，针对本地进程产生的数据包的钩子点。<code>kube-proxy</code> 同样在 <code>nat</code> 表的 <code>OUTPUT</code> 链中插入了规则。这些规则与 <code>PREROUTING</code> 中的类似，会检查数据包的目标是否为 Service 的 ClusterIP 和端口。如果匹配，数据包同样会被跳转到 <code>KUBE-SERVICES</code> 链，然后经历与外部请求类似的处理流程：<code>KUBE-SERVICES</code> -&gt; <code>KUBE-SVC-&lt;hash&gt;</code> -&gt; <code>KUBE-SEP-&lt;hash&gt;</code> -&gt; <code>DNAT</code>。</p>
<h3 id="POSTROUTING-链与-Masquerade：确保返回路径"><a href="#POSTROUTING-链与-Masquerade：确保返回路径" class="headerlink" title="POSTROUTING 链与 Masquerade：确保返回路径"></a>POSTROUTING 链与 Masquerade：确保返回路径</h3><p>当数据包经过路由决策，确定要从哪个网络接口发出时，它会经过 <code>POSTROUTING</code> 链。这是 Netfilter 在数据包即将离开本机前的最后一个钩子点。<code>kube-proxy</code> 在 <code>nat</code> 表的 <code>POSTROUTING</code> 链中插入规则，通常是跳转到 <code>KUBE-POSTROUTING</code> 链。<br><code>KUBE-POSTROUTING</code> 链的核心作用是处理之前被打上标记的数据包。它包含一条关键规则：</p>
<ul>
<li>匹配之前在 <code>KUBE-MARK-MASQ</code> 链中设置的标记 (<code>-m mark --mark 0x4000/0x4000</code>)。</li>
<li>对匹配的数据包执行 <code>MASQUERADE</code> target。<code>MASQUERADE</code> 是一种特殊的 SNAT，它会自动将数据包的源 IP 地址替换为数据包发出时选择的网络接口的 IP 地址。这对于源 IP 地址是动态分配（如 Node IP 可能变化）或者需要隐藏内部 Pod IP 的场景非常有用。</li>
</ul>
<p>这样，经过 DNAT 转发给 Pod 的数据包，在离开节点（无论是发往外部客户端还是集群内其他节点上的 Pod）之前，其源 IP 会被修改为节点的 IP。这确保了响应数据包能够正确地路由回该节点，节点再根据其连接跟踪（conntrack）表将响应转发给原始的请求者（无论是外部客户端还是集群内的 Pod）。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>Kubernetes 利用 <code>iptables</code> 实现 Service 的机制，本质上是将 Linux 内核强大的 Netfilter&#x2F;iptables 功能自动化、规模化地应用于服务发现和负载均衡场景。通过在 <code>PREROUTING</code>、<code>OUTPUT</code>、<code>POSTROUTING</code> 等关键 Netfilter 钩子点注入精心设计的规则链 (<code>KUBE-SERVICES</code>, <code>KUBE-NODEPORTS</code>, <code>KUBE-SVC-*</code>, <code>KUBE-SEP-*</code>, <code>KUBE-MARK-MASQ</code>, <code>KUBE-POSTROUTING</code>)，<code>kube-proxy</code> 实现了将对稳定虚拟 Service IP 的请求，透明地、负载均衡地（例如随机）重定向（DNAT）到后端动态变化的 Pod IP，并通过标记（mark）和地址伪装（Masquerade&#x2F;SNAT）确保了网络连接在各种复杂场景下（如跨节点访问、NodePort 访问）的双向通信路径正确无误。这套机制虽然规则数量可能庞大，性能在超大规模集群下有挑战，但它稳定、可靠，并且不依赖任何特殊的网络硬件或协议，具有良好的兼容性。</p>
<h2 id="IPVS-模式"><a href="#IPVS-模式" class="headerlink" title="IPVS 模式"></a>IPVS 模式</h2><img src="/2025/04/10/kube-proxy/image-20250415103628349.png" srcset="/img/loading.gif" lazyload class="" title="image-20250415103628349">

<h3 id="服务发现与负载均衡的挑战"><a href="#服务发现与负载均衡的挑战" class="headerlink" title="服务发现与负载均衡的挑战"></a>服务发现与负载均衡的挑战</h3><p>在 Kubernetes 这样动态的环境中，Pod 的生命周期是短暂的，它们的 IP 地址会随着创建和销毁而改变。如果让客户端直接连接 Pod IP，那么当 Pod 发生变化时，客户端就需要感知并更新连接信息，这极大地增加了系统的复杂性。Service 的引入正是为了解决这个问题，它提供了一个稳定的访问点。<code>kube-proxy</code> 则需要将这个稳定的访问点（Service IP）动态地映射到当前提供服务的、健康的 Pod IP 列表上，并进行负载均衡。早期的 <code>userspace</code> 模式性能较差，后来广泛使用的 <code>iptables</code> 模式虽然功能稳定，但在集群规模（Service 数量、Endpoint 数量）非常大时，会因为 <code>iptables</code> 规则链的线性查找特性而遇到性能瓶颈。</p>
<h3 id="IPVS-模式：基于内核的-L4-负载均衡"><a href="#IPVS-模式：基于内核的-L4-负载均衡" class="headerlink" title="IPVS 模式：基于内核的 L4 负载均衡"></a>IPVS 模式：基于内核的 L4 负载均衡</h3><p>IPVS（IP Virtual Server）是 Linux 内核的一部分，属于 LVS（Linux Virtual Server）项目。它实现了一个高性能的、工作在 OSI 模型第四层（传输层）的负载均衡器。与 <code>iptables</code> 基于规则链匹配、逐条检查的方式不同，IPVS 使用哈希表（Hash Table）来存储和查找虚拟服务器（Virtual Server）和真实服务器（Real Server）的映射关系。当有数据包到达时，IPVS 可以通过高效的哈希查找（接近 O(1) 复杂度）快速确定目标真实服务器，这使得它在处理大量并发连接和规则时，性能远超 <code>iptables</code>（其复杂度可能接近 O(n)，n 为规则数量）。IPVS 被设计用来处理大规模的服务器负载均衡场景，因此非常适合用在大型 Kubernetes 集群中。</p>
<h3 id="kube-proxy-IPVS-模式工作原理"><a href="#kube-proxy-IPVS-模式工作原理" class="headerlink" title="kube-proxy IPVS 模式工作原理"></a>kube-proxy IPVS 模式工作原理</h3><img src="/2025/04/10/kube-proxy/image-20250415104221941.png" srcset="/img/loading.gif" lazyload class="" title="image-20250415104221941">

<p>当 <code>kube-proxy</code> 配置为 IPVS 模式时，它的工作流程大致如下：</p>
<ol>
<li><p><strong>监听 API Server:</strong> <code>kube-proxy</code> 通过 watch 机制持续监听 API Server 中 Service 和 Endpoints&#x2F;EndpointSlices 对象的创建、更新和删除事件。</p>
</li>
<li><p><strong>调用内核 IPVS 接口:</strong> 当检测到 Service 或 Endpoints 变化时，<code>kube-proxy</code> 不再是去生成大量的 <code>iptables</code> 规则，而是通过 <code>netlink</code> 接口与 Linux 内核的 IPVS 模块进行交互。<code>netlink</code> 是一种用于用户空间程序与内核空间模块之间进行通信的机制。</p>
</li>
<li><p><strong>创建 IPVS 规则:</strong></p>
<ul>
<li>对于每一个 Kubernetes Service（目前主要支持 ClusterIP 和 NodePort 类型的 Service），<code>kube-proxy</code> 会在宿主机内核中创建一个对应的 IPVS <em>虚拟服务器</em>（Virtual Server）。这个虚拟服务器的地址和端口就是 Service 的 ClusterIP 和 Port（或者 NodePort 的监听地址和端口）。</li>
<li>对于该 Service 关联的每一个健康的 Endpoint（即 Pod 的 IP 和 Port），<code>kube-proxy</code> 会为对应的 IPVS 虚拟服务器添加一个 <em>真实服务器</em>（Real Server）。</li>
<li><code>kube-proxy</code> 还会根据 Service 的配置（例如 <code>sessionAffinity</code>）和 <code>kube-proxy</code> 自身的启动参数，为这个 IPVS 虚拟服务器设置相应的负载均衡调度算法（如 Round Robin (rr), Least Connection (lc), Weighted Round Robin (wrr), Source Hashing (sh) 等）。IPVS 支持比 <code>iptables</code> 模式（通常是基于概率的随机或轮询）更丰富的调度算法。</li>
</ul>
</li>
<li><p><strong>流量转发:</strong> 当一个数据包到达宿主机，其目标地址和端口匹配某个 IPVS 虚拟服务器时，内核的 IPVS 模块会接管这个数据包。它会根据选定的调度算法，从该虚拟服务器关联的真实服务器列表中选择一个健康的 Pod。然后，IPVS 执行目标网络地址转换（DNAT），将数据包的目标 IP 和端口修改为选定 Pod 的 IP 和端口，并将数据包转发出去。</p>
</li>
<li><p><strong>网络接口与 IP 配置:</strong> 为了让 Service ClusterIP 在节点上可达，<code>kube-proxy</code> 通常会创建一个虚拟网络接口（如 <code>kube-ipvs0</code>），并将所有 Service ClusterIPs 配置在这个接口上。这样可以确保发送到 ClusterIP 的流量能够被节点正确地路由到 <code>INPUT</code> 链，进而被 IPVS 钩子捕获。</p>
</li>
<li><p><strong>SNAT 处理:</strong> 需要注意的是，虽然 IPVS 负责主要的负载均衡和 DNAT，但它通常不直接处理源网络地址转换（SNAT）。当 Pod 响应请求时，为了保证响应数据包能够正确返回给原始客户端（而不是直接返回，导致客户端收到一个来自 Pod IP 的非预期响应），通常还是需要进行 SNAT，将源 IP 地址改为节点的 IP 地址。这部分功能，<code>kube-proxy</code> 在 IPVS 模式下，仍然依赖 <code>iptables</code>（通常是 <code>MASQUERADE</code> 规则）来完成。<code>kube-proxy</code> 会配置相应的 <code>iptables</code> 规则来标记需要进行 SNAT 的数据包（例如，对从 IPVS 转发出去的、源 IP 是 Pod IP 但目标 IP 不在本地 Pod CIDR 范围内的包进行标记），然后在 <code>POSTROUTING</code> 链中对这些标记的包执行 <code>MASQUERADE</code>。</p>
</li>
</ol>
<h3 id="IPVS-模式的优势"><a href="#IPVS-模式的优势" class="headerlink" title="IPVS 模式的优势"></a>IPVS 模式的优势</h3><p>IPVS 模式相比 <code>iptables</code> 模式，主要优势在于：</p>
<ul>
<li><strong>性能和可扩展性:</strong> 如前所述，基于哈希表的查找机制使得 IPVS 在 Service 和 Endpoints 数量巨大时，依然能保持高性能和较低的 CPU 开销。这对于大规模集群至关重要，可以显著降低网络延迟，提高吞吐量。</li>
<li><strong>更丰富的负载均衡算法:</strong> IPVS 内核模块原生支持多种成熟的负载均衡算法，为 Service 流量分发提供了更多选择和优化空间，例如可以根据后端 Pod 的连接数进行负载均衡（Least Connection）。</li>
<li><strong>连接状态同步优化 (理论上):</strong> IPVS 本身有更完善的连接跟踪机制。在 <code>kube-proxy</code> 更新真实服务器列表时（例如 Pod 扩缩容或滚动更新），IPVS 处理现有连接（特别是长连接）通常比 <code>iptables</code> 更平滑，能更好地避免连接中断，但这具体也取决于 <code>kube-proxy</code> 的实现细节。</li>
</ul>
<h2 id="CoreDNS"><a href="#CoreDNS" class="headerlink" title="CoreDNS"></a>CoreDNS</h2><h3 id="Kubernetes-服务发现的挑战"><a href="#Kubernetes-服务发现的挑战" class="headerlink" title="Kubernetes 服务发现的挑战"></a>Kubernetes 服务发现的挑战</h3><p>在 Kubernetes 集群中，Service 提供了一种抽象，允许我们将一组运行相同应用的 Pod 暴露为一个网络服务。Kubernetes 会为类型为 <code>ClusterIP</code> 的 Service 分配一个虚拟 IP 地址（ClusterIP），为 <code>NodePort</code> 类型的 Service 在每个节点上开放一个静态端口，为 <code>LoadBalancer</code> 类型的 Service（在支持的云环境中）分配一个外部负载均衡器的 IP 地址。然而，正如你提到的，这些 IP 地址和端口（除了 NodePort 本身的端口号）本质上是动态的。当你删除并重新创建一个 Service 时，即使 Service 的名称和定义相同，其分配到的 ClusterIP 或 LoadBalancerIP 通常会发生变化。这种动态性使得直接使用 IP 地址作为服务的稳定访问入口变得不可靠，特别是对于服务间通信或者需要对外暴露的稳定端点而言。如果一个应用硬编码了另一个服务的 ClusterIP，那么当目标服务重建后，源应用就会连接失败。</p>
<h3 id="Kubernetes-DNS-的解决方案"><a href="#Kubernetes-DNS-的解决方案" class="headerlink" title="Kubernetes DNS 的解决方案"></a>Kubernetes DNS 的解决方案</h3><p>为了解决服务 IP 地址动态变化带来的问题，Kubernetes 引入了集群内部的 DNS 服务。这个 DNS 服务与 Kubernetes API Server 集成，能够自动为集群内的 Service 和 Pod 创建 DNS 记录。其核心思想是：<strong>用稳定的、人类可读的 DNS 名称来代替不稳定的 IP 地址</strong>。当一个 Service 被创建时，Kubernetes DNS 服务会为其自动生成一个形如 <code>&lt;service-name&gt;.&lt;namespace-name&gt;.svc.&lt;cluster-domain&gt;</code> 的完全限定域名（FQDN）。例如，一个名为 <code>my-app</code> 的 Service 在 <code>default</code> 命名空间下，其默认的集群域为 <code>cluster.local</code>，那么它的 FQDN 就是 <code>my-app.default.svc.cluster.local</code>。</p>
<p>这个 DNS 名称是稳定的。只要 Service 的名称 (<code>my-app</code>) 和其所在的命名空间 (<code>default</code>) 不变，即使这个 Service 被删除后重新创建，导致其 ClusterIP 发生变化，它的 DNS 名称始终保持不变。集群内的其他 Pod 可以通过这个 DNS 名称来访问该 Service。Pod 内的 DNS 解析器（通常由 Kubelet 根据 Pod 的 <code>dnsPolicy</code> 配置 <code>/etc/resolv.conf</code> 文件）会将这个 DNS 名称查询请求发送给集群的 DNS 服务，DNS 服务则会查询最新的 Service 信息，并将当前有效的 ClusterIP 返回给请求的 Pod。这样，应用代码中只需要使用服务名（或者 FQDN），就可以透明地连接到目标服务，无需关心其底层 IP 地址的变化。这极大地简化了服务发现和微服务间的通信。</p>
<h3 id="CoreDNS-是什么？"><a href="#CoreDNS-是什么？" class="headerlink" title="CoreDNS 是什么？"></a>CoreDNS 是什么？</h3><p>CoreDNS 是一个灵活、可扩展、插件化的 DNS 服务器，使用 Go 语言编写。自 Kubernetes v1.11 起，CoreDNS 成为了 Kubernetes 推荐且默认的集群 DNS 服务，逐渐取代了之前的 <code>kube-dns</code>（由 <code>dnsmasq</code>、<code>kube-dns</code> 和 <code>sidecar</code> 三个容器组成）解决方案。CoreDNS 的设计哲学是 “Do one thing and do it well”（通过插件链）。它的核心非常轻量，大部分功能，包括处理 DNS 查询、缓存、转发、监控指标暴露等，都是通过一系列可插拔的插件（Plugins）来实现的。这种架构使得 CoreDNS 非常容易定制和扩展，可以根据特定需求启用、禁用或配置不同的插件。</p>
<p>在 Kubernetes 中，CoreDNS 通常以 Deployment 的形式运行（例如，在 <code>kube-system</code> 命名空间下），并暴露为一个名为 <code>kube-dns</code> 的 Service（为了兼容性，Service 名称沿用了旧方案的名称）。这个 Service 的 ClusterIP 会被 Kubelet 配置到集群中每个 Pod 的 <code>/etc/resolv.conf</code> 文件中作为 <code>nameserver</code>，使得 Pod 内发起的 DNS 查询默认都流向 CoreDNS。</p>
<h3 id="CoreDNS-在-Kubernetes-中的工作原理"><a href="#CoreDNS-在-Kubernetes-中的工作原理" class="headerlink" title="CoreDNS 在 Kubernetes 中的工作原理"></a>CoreDNS 在 Kubernetes 中的工作原理</h3><img src="/2025/04/10/kube-proxy/image-20250415105305996.png" srcset="/img/loading.gif" lazyload class="" title="image-20250415105305996">

<p>CoreDNS 通过其 <code>kubernetes</code> 插件与 Kubernetes API Server 进行交互，实现了对集群内部服务和 Pod 的 DNS 解析。这个插件会 “watch”（监听）Kubernetes API Server 上的 Service 和 Endpoint (或 EndpointSlice，一种更高效的资源) 对象的变化。</p>
<p>当一个 Pod 发起对某个 Service FQDN（如 <code>my-service.my-ns.svc.cluster.local</code>）的 A 记录查询时：</p>
<ol>
<li>查询请求根据 Pod 的 <code>/etc/resolv.conf</code> 配置，被发送到 CoreDNS Pod 的 IP 地址和端口（通常是 <code>kube-dns</code> Service 的 ClusterIP 上的 53 端口）。</li>
<li>CoreDNS 接收到请求，根据其配置文件（Corefile）中的指令，将请求传递给 <code>kubernetes</code> 插件进行处理。</li>
<li><code>kubernetes</code> 插件检查查询的域名是否符合其配置的集群域（如 <code>cluster.local</code>）。</li>
<li>如果匹配，插件会解析出 Service 名称 (<code>my-service</code>) 和命名空间 (<code>my-ns</code>)。</li>
<li>插件在其内部维护的、从 API Server 同步来的缓存中查找对应的 Service 信息。</li>
<li>如果找到了 Service，它会返回该 Service 的 ClusterIP 作为 A 记录的响应。</li>
</ol>
<p>对于 Headless Service（定义了 <code>clusterIP: None</code> 的 Service），<code>kubernetes</code> 插件不会返回 ClusterIP，而是会返回该 Service 关联的所有 Ready状态 Pod 的 IP 地址作为多个 A 记录。这对于需要直接与 Pod 通信的场景（如 StatefulSet 的 Pod 间发现）非常有用。</p>
<p>除了 Service，<code>kubernetes</code> 插件还可以配置为解析 Pod 的 DNS 记录（通常格式为 <code>&lt;pod-ip-address&gt;.&lt;namespace-name&gt;.pod.&lt;cluster-domain&gt;</code>，其中 IP 地址中的点 <code>.</code> 被替换为短横线 <code>-</code>）。</p>
<p>如果查询的域名不属于集群内部域（例如 <code>www.google.com</code>），<code>kubernetes</code> 插件通常会配置 <code>fallthrough</code> 选项。这意味着它不会处理这个请求，而是让请求流转到插件链中的下一个插件，通常是 <code>forward</code> 插件。<code>forward</code> 插件会将请求转发给上游 DNS 服务器（例如，配置为 CoreDNS Pod 所在节点的 <code>/etc/resolv.conf</code> 中定义的 DNS 服务器，或者直接配置的公共 DNS 服务器如 8.8.8.8）。这样，CoreDNS 不仅能解析集群内部域名，也能代理外部域名的解析。</p>
<p>CoreDNS 还通过其他插件提供了重要功能：</p>
<ul>
<li><code>cache</code>: 缓存 DNS 记录，减少对 API Server 的查询压力和外部 DNS 查询的延迟。</li>
<li><code>forward</code>: 将无法在本地解析（非集群域或 <code>fallthrough</code> 的请求）的查询转发给上游 DNS 服务器。</li>
<li><code>prometheus</code>: 暴露 Prometheus 格式的监控指标，用于监控 CoreDNS 的性能和健康状况。</li>
<li><code>health</code>: 提供 HTTP 健康检查端点（<code>/health</code>），供 Kubelet 进行 Liveness Probe。</li>
<li><code>ready</code>: 提供 HTTP 就绪检查端点（<code>/ready</code>），供 Kubelet 进行 Readiness Probe，确保插件都已准备好服务。</li>
<li><code>reload</code>: 允许在不中断服务的情况下动态加载更新后的 Corefile 配置。</li>
<li><code>loop</code>: 检测并阻止 DNS 查询转发循环。</li>
<li><code>rewrite</code>: 在查询被处理之前或之后修改查询的名称。</li>
</ul>
<h3 id="CoreDNS-配置示例-Corefile"><a href="#CoreDNS-配置示例-Corefile" class="headerlink" title="CoreDNS 配置示例 (Corefile)"></a>CoreDNS 配置示例 (Corefile)</h3><p>CoreDNS 的配置通过一个名为 <code>Corefile</code> 的文件进行管理，这个文件通常存储在 Kubernetes 的 ConfigMap 对象（如 <code>coredns</code> ConfigMap in <code>kube-system</code>）中，并挂载到 CoreDNS Pod 里。<code>Corefile</code> 的语法是基于 Caddy 服务器的 Caddyfile 格式。它由一个或多个服务器块（Server Blocks）组成，每个块定义了 CoreDNS 监听的区域（Zone）、端口和使用的插件链。</p>
<p>以下是一个典型的 Kubernetes 环境下 CoreDNS 的 <code>Corefile</code> 示例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs coredns">.:53 &#123;                                                  # 定义一个服务器块，监听所有域 (.) 在 UDP/TCP 的 53 端口<br>    errors                                              # 启用错误日志记录插件<br>    health &#123;                                            # 启用健康检查插件<br>       lameduck 5s                                      # 在关闭前等待 5 秒，处理完现有请求<br>    &#125;<br>    ready                                               # 启用就绪检查插件，所有插件加载完毕后才报告 ready<br>    kubernetes cluster.local in-addr.arpa ip6.arpa &#123;    # 启用 Kubernetes 插件<br>       pods insecure                                    # 解析 Pod A 记录 (e.g., 1-2-3-4.default.pod.cluster.local)<br>       upstream                                         # 如果需要，从上游解析 headless service 的外部名称 CNAME 记录<br>       fallthrough in-addr.arpa ip6.arpa                # 如果查询不是 K8s 服务/Pod 或反向记录，则传递给下一个插件<br>    &#125;<br>    prometheus :9153                                    # 在 9153 端口暴露 Prometheus 指标<br>    forward . /etc/resolv.conf &#123;                        # 启用转发插件<br>       max_concurrent 1000                              # 设置最大并发转发查询数<br>    &#125;<br>    cache 30                                            # 启用缓存插件，缓存 TTL 为 30 秒<br>    loop                                                # 启用循环检测插件<br>    reload 5s                                           # 每 5 秒检查一次 Corefile 是否更新，并自动重新加载<br>    loadbalance                                         # 对返回的 A/AAAA/MX 记录进行轮询负载均衡<br>&#125;<br></code></pre></td></tr></table></figure>

<p><strong>配置解读:</strong></p>
<ul>
<li><code>.:53 &#123; ... &#125;</code>: 定义了一个处理所有 DNS 查询（<code>.</code> 代表根区域）的核心块，监听标准 DNS 端口 53。</li>
<li><code>errors</code>: 捕获并记录处理过程中的错误。</li>
<li><code>health &#123; lameduck 5s &#125;</code>: 在 <code>:8080/health</code> (默认端口) 提供健康检查接口。<code>lameduck</code> 确保在 CoreDNS 准备关闭时，会先保持 5 秒的运行状态以完成正在处理的请求，然后才报告不健康。</li>
<li><code>ready</code>: 在 <code>:8181/ready</code> (默认端口) 提供就绪检查接口，只有当 CoreDNS 完全准备好处理请求时才返回成功。</li>
<li><code>kubernetes cluster.local in-addr.arpa ip6.arpa &#123; ... &#125;</code>: 这是核心。<ul>
<li><code>cluster.local</code>: 指定 CoreDNS 负责解析的 Kubernetes 集群域。所有 <code>*.cluster.local</code> 的查询将由这个插件处理。</li>
<li><code>in-addr.arpa</code> 和 <code>ip6.arpa</code>: 使插件能够处理 PTR 反向查询，将 IP 地址解析回对应的 Kubernetes 服务或 Pod 名称。</li>
<li><code>pods insecure</code>: 允许解析 Pod 的 IP 地址到 DNS 名称的 A 记录。<code>insecure</code> 表示即使 Pod 未处于 Ready 状态也可能被解析（通常还支持 <code>verified</code> 和 <code>disabled</code> 选项）。</li>
<li><code>upstream</code>: 对于指向外部名称 (ExternalName) 的 Service，如果需要解析，会使用上游解析器。</li>
<li><code>fallthrough in-addr.arpa ip6.arpa</code>: 如果查询的名称不匹配 <code>cluster.local</code> 域，或者不匹配反向查询域，则将请求传递给插件链中的下一个插件（在这里是 <code>forward</code>）。</li>
</ul>
</li>
<li><code>prometheus :9153</code>: 在 9153 端口暴露 <code>/metrics</code> 端点，供 Prometheus 抓取监控数据。</li>
<li><code>forward . /etc/resolv.conf</code>: 对于不由 <code>kubernetes</code> 插件处理的查询（由于 <code>fallthrough</code>），使用此插件转发。<code>.</code> 表示转发所有域。<code>/etc/resolv.conf</code> 表示使用 CoreDNS Pod 所在 Node 的 DNS 配置（通常来自宿主机或云提供商的 DHCP）作为上游解析器。也可以直接指定 IP 地址，如 <code>forward . 8.8.8.8 1.1.1.1</code>。</li>
<li><code>cache 30</code>: 对 DNS 记录进行缓存，默认缓存时间 30 秒。这能显著提高性能并减少对上游 DNS 的依赖。</li>
<li><code>loop</code>: 防止 DNS 查询在转发过程中形成循环。</li>
<li><code>reload 5s</code>: CoreDNS 会定期检查 <code>Corefile</code> 的修改时间。如果文件发生变化，它会在不中断服务的情况下平滑地重新加载配置。</li>
<li><code>loadbalance</code>: 对 DNS 响应中的多个 A&#x2F;AAAA&#x2F;MX 记录进行轮询分发，提供简单的客户端负载均衡。</li>
</ul>
<p>通过这套机制，CoreDNS 为 Kubernetes 集群提供了一个健壮、高效且可配置的内部 DNS 解析服务，完美解决了因 Service IP 动态变化而导致的服务发现难题，是构建稳定可靠微服务架构的关键基础设施之一。理解 CoreDNS 的工作原理和配置方式，对于深入掌握 Kubernetes 网络和服务发现至关重要。</p>
<h3 id="CoreDNS-的核心架构：内存态-DNS-与控制器模式"><a href="#CoreDNS-的核心架构：内存态-DNS-与控制器模式" class="headerlink" title="CoreDNS 的核心架构：内存态 DNS 与控制器模式"></a>CoreDNS 的核心架构：内存态 DNS 与控制器模式</h3><p>你提到的 “内存态 DNS” 和 “控制器” 是理解 CoreDNS 工作方式的关键。</p>
<p><strong>内存态 DNS</strong>：CoreDNS 本身可以配置为权威 DNS 服务器或转发器&#x2F;缓存。在 Kubernetes 环境中，通过其 <code>kubernetes</code> 插件，CoreDNS 能够动态地维护集群内部服务的 DNS 记录。这些记录主要存储在<strong>内存</strong>中，以便快速响应查询请求。这意味着 CoreDNS 不需要依赖传统的、基于文件的区域（Zone）文件来存储 K8s 服务的记录。这种内存态存储对于应对 K8s 中 Service 和 Pod 的频繁变化至关重要，可以提供低延迟的查询响应。当然，CoreDNS 也包含 <code>cache</code> 插件，用于缓存内部和外部查询结果，进一步提高性能并减少对上游 DNS 服务器的负载。</p>
<p><strong>控制器模式</strong>：这是 CoreDNS 与 Kubernetes 集成运作的核心机制。CoreDNS（通过其 <code>kubernetes</code> 插件）扮演着一个标准的 <strong>Kubernetes 控制器</strong>角色。它通过 Kubernetes API <strong>监听（Watch）</strong> <code>Service</code> 和 <code>EndpointSlice</code>（或者旧版本的 <code>Endpoints</code>）资源的变化。</p>
<ul>
<li>当一个新的 <code>Service</code> 被创建时，控制器会收到通知，并根据 Service 的类型和配置，在内存中生成相应的 DNS 记录（A、AAAA、SRV、PTR 等）。</li>
<li>当一个 <code>Service</code> 关联的 Pod 实例发生变化（例如，Pod 启动就绪、Pod 被删除、Pod IP 改变）时，对应的 <code>EndpointSlice</code> 会更新。控制器监听到 <code>EndpointSlice</code> 的变化，并相应地更新内存中的 DNS 记录（特别是对于 Headless Service 的 A&#x2F;AAAA 记录）。</li>
<li>当 <code>Service</code> 被删除时，控制器也会移除相关的 DNS 记录。</li>
</ul>
<p>这个过程是<strong>持续、自动</strong>的。CoreDNS 不需要手动配置每个服务的 DNS 条目，它通过与 Kubernetes API 的实时交互，动态地维护了一份准确反映集群当前状态的 DNS 视图。这种机制与其他的 Kubernetes 控制器（如 Deployment Controller、ReplicaSet Controller）的工作原理类似，都是通过 <strong>Watch API -&gt; 对比期望状态与实际状态 -&gt; 执行调整动作</strong> 的循环来确保系统达到期望的状态。</p>
<p>从 Linux 内核或网络角度看，当一个 Pod 内的进程发起 DNS 查询时（通常是调用 libc 的 <code>gethostbyname</code> 或 <code>getaddrinfo</code> 等函数），最终会根据 <code>/etc/resolv.conf</code> 的配置，将 UDP 或 TCP 请求发送到 CoreDNS Pod 的 IP 地址和 53 端口。CoreDNS Pod 接收到请求后，其内部的插件链开始处理。<code>kubernetes</code> 插件会检查请求的域名是否匹配集群内部的域（如 <code>cluster.local</code>），如果匹配，则在内存数据中查找相应的记录并返回；如果不匹配，则可能由 <code>forward</code> 或 <code>proxy</code> 插件将请求转发给上游 DNS 服务器（例如节点宿主机的 DNS 或公共 DNS）。</p>
<h3 id="不同类型服务的-DNS-记录详解"><a href="#不同类型服务的-DNS-记录详解" class="headerlink" title="不同类型服务的 DNS 记录详解"></a>不同类型服务的 DNS 记录详解</h3><p>CoreDNS 如何为不同类型的 Kubernetes Service 生成 DNS 记录是服务发现的核心细节。</p>
<h4 id="普通-Service-ClusterIP-NodePort-LoadBalancer"><a href="#普通-Service-ClusterIP-NodePort-LoadBalancer" class="headerlink" title="普通 Service (ClusterIP, NodePort, LoadBalancer)"></a>普通 Service (ClusterIP, NodePort, LoadBalancer)</h4><p>这类 Service 都有一个由 Kubernetes API Server 分配的、稳定的虚拟 IP，即 <strong>ClusterIP</strong>。这个 IP 并不是绑定在某个具体的网络设备上，而是由 kube-proxy（或等效的网络组件如 Cilium eBPF）在数据平面上实现负载均衡和转发。</p>
<ul>
<li><strong>A&#x2F;AAAA 记录</strong>：CoreDNS 会为这类 Service 创建一个 <strong>FQDN（完全限定域名）</strong>，格式通常是 <strong><code>&lt;service-name&gt;.&lt;namespace&gt;.svc.&lt;cluster-domain&gt;</code></strong> (例如 <code>my-service.default.svc.cluster.local</code>)。这个 FQDN 解析为一个 A 记录（IPv4）或 AAAA 记录（IPv6），其值就是该 Service 的 <strong>ClusterIP</strong>。客户端 Pod 查询这个域名时，会得到 ClusterIP，然后发往这个 IP 的请求会被 kube-proxy 拦截并转发到该 Service 背后某个健康的 Pod IP 上。</li>
<li><strong>SRV 记录</strong>：如果 Service 定义了端口（Ports），CoreDNS 还会为每个命名端口（Named Port）创建 SRV 记录。格式通常是 <strong><code>_&lt;port-name&gt;._&lt;protocol&gt;.&lt;service-name&gt;.&lt;namespace&gt;.svc.&lt;cluster-domain&gt;</code></strong> (例如 <code>_http._tcp.my-service.default.svc.cluster.local</code>)。SRV 记录包含了端口号、优先级和权重，允许客户端发现服务提供的具体端口信息，而不仅仅是 IP 地址。这对于需要知道特定服务端口的应用（如 gRPC、LDAP）非常有用。</li>
<li><strong>PTR 记录</strong>：用于反向 DNS 查询，将 ClusterIP 解析回其对应的 FQDN。</li>
</ul>
<h4 id="Headless-Service"><a href="#Headless-Service" class="headerlink" title="Headless Service"></a>Headless Service</h4><p>当 Service 的 <code>.spec.clusterIP</code> 字段被显式设置为 <code>None</code> 时，就创建了一个 <strong>Headless Service</strong>。顾名思义，它<strong>没有 ClusterIP</strong>。API Server 不会为其分配虚拟 IP，kube-proxy 也不会处理到这个 Service 的流量。</p>
<ul>
<li><strong>A&#x2F;AAAA 记录 (多条)</strong>：对于 Headless Service，当查询其 FQDN <strong><code>&lt;service-name&gt;.&lt;namespace&gt;.svc.&lt;cluster-domain&gt;</code></strong> 时，CoreDNS <strong>不会返回 ClusterIP</strong>（因为它不存在）。相反，它会返回多个 A&#x2F;AAAA 记录，每个记录对应一个**当前就绪（Ready）**状态的、属于该 Service 的 <strong>Pod 的 IP 地址</strong>。这意味着客户端直接解析到后端 Pod 的真实 IP 列表。这对于需要直接连接到特定 Pod 实例的应用（如数据库集群、StatefulSet 管理的有状态应用）或者希望自己实现客户端负载均衡策略的场景非常有用。</li>
<li><strong>Pod FQDN 记录</strong>：对于与 Headless Service 关联的每个 Pod（特别是那些由 StatefulSet 创建的、具有稳定网络标识符的 Pod），CoreDNS 还会创建一个特定的 FQDN，格式通常是 <strong><code>&lt;pod-hostname&gt;.&lt;service-name&gt;.&lt;namespace&gt;.svc.&lt;cluster-domain&gt;</code></strong> (如果 Service 指定了 <code>subdomain</code> 字段，格式会是 <code>&lt;pod-hostname&gt;.&lt;subdomain&gt;.&lt;namespace&gt;.svc.&lt;cluster-domain&gt;</code>) 或者对于普通 Pod 可能简化为类似 <strong><code>&lt;pod-ip-dashed&gt;.&lt;namespace&gt;.pod.&lt;cluster-domain&gt;</code></strong>。这个记录直接解析为该 <strong>Pod 的 IP 地址</strong>。这允许直接寻址到某个具体的 Pod 实例。</li>
</ul>
<h4 id="ExternalName-Service"><a href="#ExternalName-Service" class="headerlink" title="ExternalName Service"></a>ExternalName Service</h4><p>这种类型的 Service 比较特殊，它<strong>不涉及 Pod 选择器或 IP 地址分配</strong>。它的目的是在集群内部为<strong>外部的一个 DNS 域名</strong>创建一个别名。</p>
<ul>
<li><strong>CNAME 记录</strong>：CoreDNS 会为 ExternalName Service 的 FQDN <strong><code>&lt;service-name&gt;.&lt;namespace&gt;.svc.&lt;cluster-domain&gt;</code></strong> 创建一个 <strong>CNAME 记录</strong>。这个 CNAME 记录的值是 Service 定义中 <code>.spec.externalName</code> 字段指定的外部域名。当集群内部的 Pod 查询这个 Service FQDN 时，CoreDNS 会返回 CNAME 记录，指示客户端应该去查询 <code>externalName</code> 指定的那个域名。这相当于在集群 DNS 内部做了一个“符号链接”或“别名”，将内部服务名映射到集群外部的某个实际服务上，而无需关心外部服务的 IP 是否变化。</li>
</ul>
<h3 id="Pod-的-DNS-配置-dnsPolicy-和-etc-resolv-conf"><a href="#Pod-的-DNS-配置-dnsPolicy-和-etc-resolv-conf" class="headerlink" title="Pod 的 DNS 配置 (dnsPolicy 和 /etc/resolv.conf)"></a>Pod 的 DNS 配置 (<code>dnsPolicy</code> 和 <code>/etc/resolv.conf</code>)</h3><p>Kubernetes 通过 <code>kubelet</code> 组件管理每个 Pod 的 DNS 配置，确保 Pod 能够正确地使用 CoreDNS 进行服务发现。</p>
<p><strong><code>dnsPolicy</code></strong> 属性定义了 Pod 的 DNS 解析行为。默认值是 <strong><code>ClusterFirst</code></strong>。这意味着：</p>
<ol>
<li>Pod 内发起的 DNS 查询，如果请求的域名<strong>不包含点（dots）或者包含的点数少于 <code>options ndots</code> 指定的值</strong>（通常默认为 5），并且<strong>配置了 <code>search</code> 域</strong>，那么系统会依次尝试将 <code>search</code> 域附加到查询域名后面进行解析。例如，查询 <code>my-service</code> 时，会依次尝试查询 <code>my-service.my-namespace.svc.cluster.local</code>，然后 <code>my-service.svc.cluster.local</code>，然后 <code>my-service.cluster.local</code>。</li>
<li>如果第一步没有找到，或者查询的域名本身就包含足够的点数（大于等于 <code>ndots</code>），系统会将其视为一个完全限定域名（FQDN）直接进行查询。</li>
<li>所有这些查询首先被发送到 <code>/etc/resolv.conf</code> 中 <code>nameserver</code> 指令指定的 DNS 服务器，也就是 <strong>CoreDNS 的 Service IP</strong>。</li>
<li>如果 CoreDNS 无法解析（例如，查询的是外部域名），并且 CoreDNS 配置了上游转发器（<code>forward</code> 插件），它会将查询转发给上游 DNS 服务器。</li>
</ol>
<p><strong><code>/etc/resolv.conf</code></strong> 文件在 Pod 启动时由 <code>kubelet</code> 根据 Pod 的 <code>dnsPolicy</code> 和集群配置动态生成并挂载到容器内部。一个典型的 <code>ClusterFirst</code> 策略下的 <code>/etc/resolv.conf</code> 文件内容如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 由 kubelet 生成</span><br>search &lt;namespace&gt;.svc.&lt;cluster-domain&gt; svc.&lt;cluster-domain&gt; &lt;cluster-domain&gt; [其他搜索域, 如宿主机配置的域]<br>nameserver &lt;coredns-cluster-ip&gt;<br>options ndots:5 <span class="hljs-built_in">timeout</span>:1 attempts:2<br></code></pre></td></tr></table></figure>

<ul>
<li><strong><code>search</code></strong>: 定义了 DNS 查询的搜索域列表。当查询一个不包含域名的短名称（如 <code>my-service</code>）时，系统会按列表顺序尝试将这些后缀附加到短名称后面进行解析。这使得在同一个 Namespace 内可以直接使用服务名（<code>my-service</code>），在不同 Namespace 可以使用 <code>&lt;service-name&gt;.&lt;namespace&gt;</code>。</li>
<li><strong><code>nameserver</code></strong>: 指定了 DNS 服务器的 IP 地址。在 K8s 中，这通常是 <strong>CoreDNS Service 的 ClusterIP</strong>。所有发往此 IP:53 的 DNS 请求都会被路由到某个 CoreDNS Pod 实例。</li>
<li><strong><code>options ndots:5</code></strong>: 这个选项非常关键。它告诉本地 DNS 解析器（通常是 glibc）：如果一个域名包含的点（dots）<strong>少于</strong> 5 个，那么在直接查询这个域名本身（视为绝对域名）之前，<strong>优先</strong>尝试将 <code>search</code> 列表中的后缀依次附加到该域名后面进行查询。如果域名包含的点数<strong>等于或多于</strong> 5 个，则解析器会认为它是一个 FQDN，直接查询它，不再尝试附加搜索后缀。这个默认值 5 是为了优化集群内部的服务发现（大部分内部服务 FQDN 包含 4 个点，如 <code>svc.cluster.local</code>），但有时可能导致对外部短域名（如 <code>www.google.com</code>，只有 2 个点）的解析变慢，因为它会先尝试附加搜索路径。</li>
<li><code>timeout</code> 和 <code>attempts</code> 控制 DNS 查询的超时时间和重试次数。</li>
</ul>
<p><strong>环境变量的影响</strong>：你提到了环境变量。确实，在 Kubernetes 的早期版本或者特定配置下，<code>kubelet</code> 会将<strong>当前 Namespace</strong> 中所有 Service 的信息（如 <code>SERVICE_HOST</code> 和 <code>SERVICE_PORT</code>）作为环境变量注入到新创建的 Pod 中。然而，这种方式有<strong>显著的缺点</strong>：</p>
<ol>
<li><strong>静态性</strong>：环境变量在 Pod 创建时注入，<strong>之后不会更新</strong>。如果 Service 的 ClusterIP 发生变化（虽然不常见，但可能发生），或者你希望 Pod 能够发现后来才创建的 Service，环境变量就无能为力了。</li>
<li><strong>污染环境变量空间</strong>：如果一个 Namespace 中有很多 Service，这会注入大量的环境变量，可能导致混乱或冲突。</li>
<li><strong>无法反映 Headless Service 的 Pod IP 列表</strong>。</li>
</ol>
<p>由于这些限制，<strong>基于 DNS 的服务发现（CoreDNS）是 Kubernetes 中推荐和主流的方式</strong>，它更加动态、灵活和标准。依赖环境变量的方式通常被认为是不推荐的做法。</p>
<h3 id="DNS-的落地实践：与企业-DNS-集成"><a href="#DNS-的落地实践：与企业-DNS-集成" class="headerlink" title="DNS 的落地实践：与企业 DNS 集成"></a>DNS 的落地实践：与企业 DNS 集成</h3><p>将 Kubernetes 集群的服务发现与企业现有的 DNS 基础架构集成是一个常见的需求，目的是让集群外部的用户或其他系统也能访问 Kubernetes 中部署的服务，并保持服务名称的一致性。</p>
<p><strong>挑战与需求</strong>：</p>
<ul>
<li>集群内部使用 CoreDNS 解析服务名到 ClusterIP 或 Pod IP。</li>
<li>集群外部（例如，用户的笔记本电脑、其他数据中心的服务器）需要通过企业内部的 DNS 服务器（如 BIND, Microsoft DNS, 或其他 DNS 服务）解析同一个服务名，但通常需要解析到服务的<strong>入口点 IP</strong>，比如 <strong>LoadBalancer Service 的外部 IP (VIP)</strong> 或 Ingress Controller 的外部 IP。</li>
<li>对于 Headless Service，如果其 Pod IP 在企业网络中是可路由的，并且需要从外部直接访问特定实例，可能也需要在企业 DNS 中创建相应的记录。</li>
</ul>
<p><strong>解决方案：定制 DNS 控制器 (如 ExternalDNS)</strong></p>
<p>为了自动化这个过程，社区开发了 <strong>ExternalDNS</strong> 这个项目。ExternalDNS 也是一个 Kubernetes 控制器，它的工作方式是：</p>
<ol>
<li><strong>监听</strong> Kubernetes 中的 <code>Service</code> 和&#x2F;或 <code>Ingress</code> 资源。</li>
<li>根据资源的**注解（Annotations）**或特定配置，判断哪些服务需要发布到外部 DNS。</li>
<li>读取需要发布的服务信息（例如，<code>LoadBalancer</code> Service 的 <code>status.loadBalancer.ingress[0].ip</code> 或 <code>hostname</code>，或者 <code>Ingress</code> 的主机名和 IP）。</li>
<li>使用相应的<strong>云服务商 API 或标准 DNS 更新协议 (RFC 2136)</strong>，在<strong>外部的企业 DNS 或公共 DNS</strong>（如 AWS Route 53, Google Cloud DNS, Azure DNS, BIND 等）中<strong>自动创建、更新或删除</strong>相应的 DNS 记录（通常是 A, AAAA, 或 CNAME 记录）。</li>
</ol>
<p><strong>配置示例 (ExternalDNS 部署片段 - 示意)</strong>：<br>部署 ExternalDNS 时，需要配置它连接到哪个 K8s 集群、监听哪些资源、使用哪个 DNS 提供商以及如何认证。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># deployment.yaml (部分)</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">external-dns</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span> <span class="hljs-comment"># 或者其他专用 namespace</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">strategy:</span><br>    <span class="hljs-attr">type:</span> <span class="hljs-string">Recreate</span><br>  <span class="hljs-attr">selector:</span><br>    <span class="hljs-attr">matchLabels:</span><br>      <span class="hljs-attr">app:</span> <span class="hljs-string">external-dns</span><br>  <span class="hljs-attr">template:</span><br>    <span class="hljs-attr">metadata:</span><br>      <span class="hljs-attr">labels:</span><br>        <span class="hljs-attr">app:</span> <span class="hljs-string">external-dns</span><br>    <span class="hljs-attr">spec:</span><br>      <span class="hljs-attr">serviceAccountName:</span> <span class="hljs-string">external-dns</span> <span class="hljs-comment"># 需要配置 RBAC 权限访问 Service/Ingress</span><br>      <span class="hljs-attr">containers:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">external-dns</span><br>        <span class="hljs-attr">image:</span> <span class="hljs-string">registry.k8s.io/external-dns/external-dns:v0.13.x</span> <span class="hljs-comment"># 使用合适的版本</span><br>        <span class="hljs-attr">args:</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-string">--source=service</span> <span class="hljs-comment"># 监听 Service 资源</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-string">--source=ingress</span> <span class="hljs-comment"># 同时监听 Ingress 资源</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-string">--domain-filter=my-company.com</span> <span class="hljs-comment"># 只管理这个域下的记录</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-string">--provider=aws</span> <span class="hljs-comment"># 指定 DNS 提供商 (例如 AWS Route 53)</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-string">--policy=upsert-only</span> <span class="hljs-comment"># DNS 记录管理策略 (安全起见，防止误删)</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-string">--aws-zone-type=public</span> <span class="hljs-comment"># 指定 Route 53 Zone 类型</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-string">--registry=txt</span> <span class="hljs-comment"># 使用 TXT 记录来跟踪所有权</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-string">--txt-owner-id=my-k8s-cluster-id</span> <span class="hljs-comment"># 唯一的 Owner ID 防止冲突</span><br>        <span class="hljs-comment"># 可能还需要配置 AWS 凭证，通常通过 Service Account + IAM Role (IRSA) 或挂载 Secret</span><br>      <span class="hljs-attr">securityContext:</span><br>        <span class="hljs-attr">fsGroup:</span> <span class="hljs-number">65534</span> <span class="hljs-comment"># 非 root 用户运行</span><br></code></pre></td></tr></table></figure>

<p><strong>处理 Headless Service 的注意事项</strong>：<br>如你所指出，将 Headless Service 的<strong>所有 Pod IP</strong> 都发布到企业 DNS 需要非常谨慎。</p>
<ul>
<li><strong>前提</strong>：这些 Pod IP 必须在企业网络中是<strong>可全局路由</strong>的，并且外部客户端有访问这些 IP 的网络策略许可。</li>
<li><strong>风险</strong>：如果 Pod 是动态扩缩容或频繁更新的，会导致大量的 DNS 记录频繁创建和删除，给企业 DNS 服务器带来<strong>巨大压力</strong>（冲击），也可能导致客户端 DNS 缓存失效和解析延迟。</li>
<li><strong>建议</strong>：<strong>按需创建 (On-demand creation)</strong>。通常只为那些需要稳定、可预测的外部访问的 Headless Service Pod（例如，StatefulSet 中的特定实例）创建 DNS 记录，并且可能需要更长的 TTL（Time-To-Live）来减少 DNS 流量。这通常通过在 Pod 或 Service 上添加特定的注解，让 ExternalDNS 知道要发布哪些记录。</li>
</ul>
<p><strong>最终目标</strong>：通过 CoreDNS（内部）和 ExternalDNS（外部）的协同工作，实现<strong>服务在集群内外拥有统一的标识</strong>（例如，<code>billing-api.my-company.com</code>）。内部 Pod 查询时，CoreDNS 可能返回 ClusterIP；外部用户查询时，企业 DNS 可能返回 LoadBalancer VIP。应用层使用统一的服务名，底层 DNS 负责解析到正确的、适合访问者位置的 IP 地址。</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/kubernetes/" class="print-no-link">#kubernetes</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>kube_proxy</div>
      <div>https://mfzzf.github.io/2025/04/10/kube-proxy/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Mzzf</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年4月10日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/04/03/kubernetes-Service%E5%AF%B9%E8%B1%A1/" title="Service对象">
                        <span class="hidden-mobile">Service对象</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
