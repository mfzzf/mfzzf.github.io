

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Mzzf">
  <meta name="keywords" content="">
  
    <meta name="description" content="Kubernetes 网络核心：kube-proxy 与 CoreDNS 深度解析引言：Kubernetes 网络面临的挑战在 Kubernetes (K8s) 集群中，应用程序以 Pod 的形式运行。Pod 是短暂的，它们的 IP 地址会随着创建、销毁和调度而动态变化。这给服务间的通信带来了挑战：客户端如何才能可靠地发现并连接到提供特定服务的 Pod 集合？ 为了解决这个问题，Kubernete">
<meta property="og:type" content="article">
<meta property="og:title" content="Kubernetes 网络核心：kube-proxy 与 CoreDNS 深度解析">
<meta property="og:url" content="https://mfzzf.github.io/2025/04/10/kube-proxy/index.html">
<meta property="og:site_name" content="Mzzf&#39;s Blog">
<meta property="og:description" content="Kubernetes 网络核心：kube-proxy 与 CoreDNS 深度解析引言：Kubernetes 网络面临的挑战在 Kubernetes (K8s) 集群中，应用程序以 Pod 的形式运行。Pod 是短暂的，它们的 IP 地址会随着创建、销毁和调度而动态变化。这给服务间的通信带来了挑战：客户端如何才能可靠地发现并连接到提供特定服务的 Pod 集合？ 为了解决这个问题，Kubernete">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/10/kube-proxy/image-20250410103718455.png">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/10/kube-proxy/image-20250412130128351.png">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/10/kube-proxy/image-20250412130500507.png">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/10/kube-proxy/image-20250412130714279.png">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/10/kube-proxy/1.jpg">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/10/kube-proxy/2.jpg">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/10/kube-proxy/3.jpg">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/10/kube-proxy/5.jpg">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/10/kube-proxy/image-20250412133507210.png">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/10/kube-proxy/image-20250415103628349.png">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/10/kube-proxy/image-20250415104221941.png">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/10/kube-proxy/image-20250415105305996.png">
<meta property="article:published_time" content="2025-04-10T02:23:46.000Z">
<meta property="article:modified_time" content="2025-04-27T09:47:33.890Z">
<meta property="article:author" content="Mzzf">
<meta property="article:tag" content="kubernetes">
<meta property="article:tag" content="networking">
<meta property="article:tag" content="kube-proxy">
<meta property="article:tag" content="coredns">
<meta property="article:tag" content="netfilter">
<meta property="article:tag" content="ipvs">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://mfzzf.github.io/2025/04/10/kube-proxy/image-20250410103718455.png">
  
  
  
  <title>Kubernetes 网络核心：kube-proxy 与 CoreDNS 深度解析 - Mzzf&#39;s Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"mfzzf.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Mzzf&#39;s blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="Kubernetes 网络核心：kube-proxy 与 CoreDNS 深度解析"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-04-10 10:23" pubdate>
          2025年4月10日 上午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          18k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          153 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">Kubernetes 网络核心：kube-proxy 与 CoreDNS 深度解析</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="Kubernetes-网络核心：kube-proxy-与-CoreDNS-深度解析"><a href="#Kubernetes-网络核心：kube-proxy-与-CoreDNS-深度解析" class="headerlink" title="Kubernetes 网络核心：kube-proxy 与 CoreDNS 深度解析"></a>Kubernetes 网络核心：kube-proxy 与 CoreDNS 深度解析</h1><h2 id="引言：Kubernetes-网络面临的挑战"><a href="#引言：Kubernetes-网络面临的挑战" class="headerlink" title="引言：Kubernetes 网络面临的挑战"></a>引言：Kubernetes 网络面临的挑战</h2><p>在 Kubernetes (K8s) 集群中，应用程序以 Pod 的形式运行。Pod 是短暂的，它们的 IP 地址会随着创建、销毁和调度而动态变化。这给服务间的通信带来了挑战：客户端如何才能可靠地发现并连接到提供特定服务的 Pod 集合？</p>
<p>为了解决这个问题，Kubernetes 引入了 <code>Service</code> 抽象。Service 提供了一个稳定的虚拟 IP (ClusterIP) 和端口，作为访问一组后端 Pod 的统一入口。然而，Service 本身只是一个 API 对象，需要有组件将这个抽象概念转化为实际的网络规则，实现<strong>服务发现</strong>和<strong>负载均衡</strong>。</p>
<p>本文将深入探讨 Kubernetes 网络体系中两个至关重要的组件：<code>kube-proxy</code> 和 <code>CoreDNS</code>，它们分别负责实现 Service 的数据平面转发和基于名称的服务发现。同时，我们也将深入剖析 <code>kube-proxy</code> iptables 模式所依赖的 Linux 内核 <code>Netfilter</code> 框架。</p>
<h2 id="kube-proxy：Service-的智能流量调度器"><a href="#kube-proxy：Service-的智能流量调度器" class="headerlink" title="kube-proxy：Service 的智能流量调度器"></a>kube-proxy：Service 的智能流量调度器</h2><p><code>kube-proxy</code> 是运行在 Kubernetes 每个 Node 上的网络代理和负载均衡器，通常以 DaemonSet 形式部署。它的核心职责是<strong>监视（watch）API Server 上 <code>Service</code> 和 <code>Endpoints</code> (或 <code>EndpointSlice</code>) 对象的变化，并将这些变化转化为节点本地的网络规则</strong>，确保发送到 Service IP 的流量能够被正确地路由和负载均衡到后端健康的 Pod 上。</p>
<p><code>kube-proxy</code> 的发展经历了几个模式的演进，本质上是不断追求更高性能、更低延迟和更好大规模集群适应性的过程。</p>
<h3 id="演进之路：从-Userspace-到-IPVS"><a href="#演进之路：从-Userspace-到-IPVS" class="headerlink" title="演进之路：从 Userspace 到 IPVS"></a>演进之路：从 Userspace 到 IPVS</h3><h4 id="1-Userspace-模式（已废弃）：初代方案的性能困局"><a href="#1-Userspace-模式（已废弃）：初代方案的性能困局" class="headerlink" title="1. Userspace 模式（已废弃）：初代方案的性能困局"></a>1. Userspace 模式（已废弃）：初代方案的性能困局</h4><p>最早期的模式。<code>kube-proxy</code> 在用户空间监听一个端口，通过 <code>iptables</code> 将 Service 流量重定向到这个端口。<code>kube-proxy</code> 进程内部维护 Pod 列表并进行轮询负载均衡，然后与选中的 Pod 建立新的连接。</p>
<ul>
<li><strong>工作流程:</strong> Client -&gt; ServiceIP:Port -&gt; (iptables DNAT) -&gt; NodeIP:ProxyPort -&gt; kube-proxy process -&gt; (Round Robin) -&gt; PodIP:TargetPort</li>
<li><strong>核心缺陷:</strong><ul>
<li><strong>内核态&#x2F;用户态切换开销大:</strong> 数据包路径长 (内核 -&gt; 用户 -&gt; 内核)。</li>
<li><strong>内存拷贝:</strong> 数据需要在内核和用户空间之间拷贝。</li>
<li><strong>单点瓶颈:</strong> 所有流量经过单个 <code>kube-proxy</code> 进程。</li>
</ul>
</li>
<li><strong>性能影响:</strong> 吞吐量低，延迟高，仅适用于小型测试环境。</li>
</ul>
<h4 id="2-Iptables-模式：基于-Netfilter-的内核级转发"><a href="#2-Iptables-模式：基于-Netfilter-的内核级转发" class="headerlink" title="2. Iptables 模式：基于 Netfilter 的内核级转发"></a>2. Iptables 模式：基于 Netfilter 的内核级转发</h4><p>这是长期以来的默认模式，也是理解 Kubernetes 网络规则的关键。<code>kube-proxy</code> 不再直接处理数据包，而是将 Service 和 Endpoints 信息<strong>翻译成大量的 <code>iptables</code> 规则</strong>，利用 Linux 内核的 Netfilter 框架在内核空间直接处理流量转发和负载均衡。</p>
<ul>
<li><strong>工作原理概述:</strong> 通过在 Netfilter 的特定钩子点（Hooks）上创建 <code>iptables</code> 规则链（Chains），实现 DNAT (目标地址转换) 和简单的负载均衡 (通常是随机或轮询)。</li>
<li><strong>核心优势:</strong> 相比 Userspace 模式，性能大幅提升，因为转发完全在内核态完成。</li>
<li><strong>面临挑战:</strong><ul>
<li><strong>规则数量爆炸:</strong> 每个 Service 和 Endpoint 都可能生成多条 <code>iptables</code> 规则。大规模集群（成千上万 Service&#x2F;Endpoint）下，规则数量可达数万甚至数十万条。</li>
<li><strong>性能随规模下降:</strong> <code>iptables</code> 规则匹配是线性查找，规则越多，匹配延迟越高，CPU 消耗越大。</li>
<li><strong>更新效率低:</strong> Service&#x2F;Endpoint 变更时，<code>kube-proxy</code> 需要更新 <code>iptables</code> 规则。全量刷新大量规则可能非常耗时（分钟级），导致服务更新延迟。</li>
<li><strong>连接跟踪 (Conntrack) 压力:</strong> NAT 操作依赖 conntrack 表记录连接状态，大量连接会消耗较多内存，极端情况可能导致 conntrack 表满而丢包。</li>
</ul>
</li>
</ul>
<p>为了深入理解 iptables 模式，我们必须先了解其底层的 Netfilter 框架。</p>
<hr>
<h3 id="深度解析-Netfilter-框架：Linux-网络数据包处理核心"><a href="#深度解析-Netfilter-框架：Linux-网络数据包处理核心" class="headerlink" title="[深度解析] Netfilter 框架：Linux 网络数据包处理核心"></a><strong>[深度解析] Netfilter 框架：Linux 网络数据包处理核心</strong></h3><img src="/2025/04/10/kube-proxy/image-20250410103718455.png" srcset="/img/loading.gif" lazyload class="" title="image-20250410103718455">

<p>Netfilter 是 Linux 内核中一个强大而灵活的网络数据包处理框架。它不仅仅是防火墙的基础，更是许多高级网络功能（如 NAT、数据包修改、连接跟踪）的核心。理解 Netfilter 对于掌握 <code>kube-proxy</code> 的 iptables 模式至关重要。</p>
<h4 id="Iptables-与-Netfilter-的关系：用户空间工具与内核框架"><a href="#Iptables-与-Netfilter-的关系：用户空间工具与内核框架" class="headerlink" title="Iptables 与 Netfilter 的关系：用户空间工具与内核框架"></a>Iptables 与 Netfilter 的关系：用户空间工具与内核框架</h4><img src="/2025/04/10/kube-proxy/image-20250412130128351.png" srcset="/img/loading.gif" lazyload class="" title="image-20250412130128351">

<ul>
<li><strong>Netfilter:</strong> 内核空间框架，在网络协议栈的关键路径上提供<strong>钩子 (Hooks)</strong>。内核模块可以在这些钩子上注册处理函数，对流经的数据包进行操作。</li>
<li><strong>iptables:</strong> 用户空间命令行工具，用于<strong>定义规则</strong>。这些规则被 <code>iptables</code> 工具加载到内核中，由 Netfilter 框架在相应的钩子上执行。</li>
</ul>
<p>简单说：<code>iptables</code> 定义策略，<code>Netfilter</code> 提供执行机制。</p>
<h4 id="Netfilter-Hooks：内核网络栈的关键切入点"><a href="#Netfilter-Hooks：内核网络栈的关键切入点" class="headerlink" title="Netfilter Hooks：内核网络栈的关键切入点"></a>Netfilter Hooks：内核网络栈的关键切入点</h4><img src="/2025/04/10/kube-proxy/image-20250412130500507.png" srcset="/img/loading.gif" lazyload class="" title="image-20250412130500507">

<p>Netfilter 为 IPv4 定义了 5 个核心钩子点，分布在数据包处理流程的关键位置：</p>
<ol>
<li><strong>NF_IP_PRE_ROUTING:</strong> 数据包进入网络栈后，<strong>路由决策之前</strong>。是执行 <strong>DNAT</strong> (目的地址转换，如 Service ClusterIP -&gt; Pod IP) 的主要位置。</li>
<li><strong>NF_IP_LOCAL_IN:</strong> 路由决策确定数据包<strong>目的地是本机</strong>后，传递给上层协议 (TCP&#x2F;UDP) 之前。用于过滤访问本机服务的数据包 (Filter 表 INPUT 链)。</li>
<li><strong>NF_IP_FORWARD:</strong> 路由决策确定数据包需要<strong>转发给其他主机</strong>时，实际转发之前。用于过滤转发的数据包 (Filter 表 FORWARD 链)。</li>
<li><strong>NF_IP_LOCAL_OUT:</strong> <strong>本机进程发出</strong>的数据包，进入网络栈，<strong>路由决策之前</strong>。用于过滤本机发出的数据包 (Filter 表 OUTPUT 链)，也是处理本机访问 Service ClusterIP 进行 DNAT 的地方。</li>
<li><strong>NF_IP_POST_ROUTING:</strong> 数据包<strong>即将离开本机</strong>发送到网络接口之前 (无论是本机发出还是转发)。是执行 <strong>SNAT</strong> (源地址转换，如 Pod IP -&gt; Node IP &#x2F; Masquerade) 的主要位置。</li>
</ol>
<img src="/2025/04/10/kube-proxy/image-20250412130714279.png" srcset="/img/loading.gif" lazyload class="" title="image-20250412130714279">
<p><em>数据包流经不同 Hook 点的示意图</em></p>
<h4 id="Hooks、Tables-与-Chains：规则的组织结构"><a href="#Hooks、Tables-与-Chains：规则的组织结构" class="headerlink" title="Hooks、Tables 与 Chains：规则的组织结构"></a>Hooks、Tables 与 Chains：规则的组织结构</h4><p><code>iptables</code> 使用 表 (Table) 和 链 (Chain) 来组织规则。</p>
<ul>
<li><p><strong>Tables (表):</strong> 代表不同的处理逻辑类型。主要有：</p>
<ul>
<li><code>raw</code>: 优先级最高，用于标记数据包跳过连接跟踪 (NOTRACK)。</li>
<li><code>mangle</code>: 用于修改 IP 头字段 (如 TOS, TTL, MARK)。</li>
<li><code>nat</code>: 用于网络地址转换 (DNAT, SNAT)。<strong>kube-proxy iptables 模式的核心</strong>。</li>
<li><code>filter</code>: 默认表，用于数据包过滤 (允许&#x2F;拒绝)。</li>
<li><code>security</code>: 用于与 SELinux 等安全模块集成。</li>
</ul>
</li>
<li><p><strong>Chains (链):</strong> 表内部规则的有序列表。</p>
<ul>
<li><strong>内建链 (Built-in Chains):</strong> 直接与 Netfilter Hooks 关联 (如 <code>PREROUTING</code>, <code>INPUT</code>, <code>FORWARD</code>, <code>OUTPUT</code>, <code>POSTROUTING</code>)。</li>
<li><strong>自定义链 (User-defined Chains):</strong> 用户创建，可以从内建链或其他自定义链跳转过来，用于组织复杂规则 (如 <code>kube-proxy</code> 创建的 <code>KUBE-SERVICES</code>, <code>KUBE-SVC-XXX</code> 等)。</li>
</ul>
</li>
</ul>
<p><strong>数据包处理流程与 Table&#x2F;Chain 关系:</strong></p>
<img src="/2025/04/10/kube-proxy/1.jpg" srcset="/img/loading.gif" lazyload class="" title="image">
<p><em>图片展示了数据包流经不同 Hook 点时，会依次经过哪些 Table 的哪些 Chain。</em></p>
<ul>
<li><strong>入口 (PRE_ROUTING Hook):</strong> <code>raw</code> -&gt; <code>mangle</code> -&gt; <code>nat</code> (DNAT)</li>
<li><strong>本机接收 (LOCAL_IN Hook):</strong> <code>mangle</code> -&gt; <code>filter</code> (INPUT) -&gt; <code>security</code> -&gt; <code>nat</code> (少用)</li>
<li><strong>转发 (FORWARD Hook):</strong> <code>mangle</code> -&gt; <code>filter</code> (FORWARD) -&gt; <code>security</code></li>
<li><strong>本机发出 (LOCAL_OUT Hook):</strong> <code>raw</code> -&gt; <code>mangle</code> -&gt; <code>nat</code> (DNAT&#x2F;SNAT) -&gt; <code>filter</code> (OUTPUT) -&gt; <code>security</code></li>
<li><strong>出口 (POST_ROUTING Hook):</strong> <code>mangle</code> -&gt; <code>nat</code> (SNAT)</li>
</ul>
<h4 id="从-Linux-IP-协议栈深入理解-Netfilter"><a href="#从-Linux-IP-协议栈深入理解-Netfilter" class="headerlink" title="从 Linux IP 协议栈深入理解 Netfilter"></a>从 Linux IP 协议栈深入理解 Netfilter</h4><img src="/2025/04/10/kube-proxy/2.jpg" srcset="/img/loading.gif" lazyload class="" title="image">
<p><em>简化的 Linux IP 协议栈数据包接收路径示意图</em></p>
<h3 id="数据包接收流程概述"><a href="#数据包接收流程概述" class="headerlink" title="数据包接收流程概述"></a>数据包接收流程概述</h3><ol>
<li><p><strong>硬件接收与中断:</strong> 当网卡（NIC）接收到一个目的 MAC 地址匹配本机或为广播&#x2F;多播地址的以太网帧时，它会将数据通过 DMA (Direct Memory Access) 传输到内存中的预分配缓冲区（Ring Buffer）。传输完成后，网卡会向 CPU 发出一个硬件中断信号。</p>
</li>
<li><p><strong>中断处理程序 (ISR - Interrupt Service Routine):</strong> CPU 响应该中断，暂停当前任务，跳转执行该网卡驱动注册的中断处理程序。ISR 的主要工作是快速响应硬件，通常会：</p>
<ul>
<li>禁用网卡中断（防止中断风暴）。</li>
<li>分配一个内核数据结构 <code>sk_buff</code> (Socket Buffer) 来表示这个数据包。<code>sk_buff</code> 是 Linux 网络栈中表示网络数据包的核心结构，包含了数据本身以及大量的元数据（如协议类型、接口信息、时间戳、路由结果等）。</li>
<li>调用网卡驱动的特定函数，将 DMA 缓冲区中的数据拷贝到 <code>sk_buff</code> 中，并更新 Ring Buffer 的状态。</li>
<li>调用与协议无关的网络设备接收函数 <code>netif_rx()</code> 或其变体（如 <code>napi_gro_receive()</code>）。</li>
</ul>
</li>
<li><p><strong><code>netif_rx()</code> 与 NAPI:</strong> <code>netif_rx()</code> 将 <code>sk_buff</code> 添加到 CPU 的 backlog 队列，并触发一个 <code>NET_RX_SOFTIRQ</code> 软中断。为了提高性能并避免中断风暴，现代驱动普遍使用 NAPI (New API)。在 NAPI 模式下，ISR 只需禁用中断并触发软中断，实际的数据包处理（分配 <code>sk_buff</code>、拷贝数据）被推迟到软中断上下文中，由 <code>napi_poll()</code> 函数批量处理。</p>
</li>
<li><p><strong>软中断处理 (<code>NET_RX_SOFTIRQ</code>):</strong> 内核调度器在适当的时候（通常是中断返回或内核线程调度时）会检查并执行挂起的软中断。<code>ksoftirqd</code> 内核线程也会在系统负载较高时帮助处理软中断。处理 <code>NET_RX_SOFTIRQ</code> 的核心函数是 <code>net_rx_action()</code>。它会从 backlog 队列或 NAPI 的 poll 列表中取出 <code>sk_buff</code>，然后根据 <code>sk_buff-&gt;protocol</code> 字段（由驱动根据以太网帧类型设置）将其分发给相应的 L3 协议处理函数。例如，IP 包会交给 <code>ip_rcv()</code> 处理，ARP 包交给 <code>arp_rcv()</code> 处理。</p>
</li>
</ol>
<h3 id="IPv4-数据包处理与-Netfilter-Hooks"><a href="#IPv4-数据包处理与-Netfilter-Hooks" class="headerlink" title="IPv4 数据包处理与 Netfilter Hooks"></a>IPv4 数据包处理与 Netfilter Hooks</h3><img src="/2025/04/10/kube-proxy/3.jpg" srcset="/img/loading.gif" lazyload class="" title="image">

<p><em>IP 层处理流程与 Netfilter Hooks 的嵌入点</em></p>
<ol>
<li><p><strong><code>ip_rcv()</code> - 初始处理与 <code>NF_IP_PRE_ROUTING</code>:</strong></p>
<ul>
<li><code>ip_rcv()</code> 函数是 IP 层处理接收到的数据包的入口。它首先会对 IP 头部进行基本的验证，如版本号、头部长度、总长度、校验和等。</li>
<li>如果验证通过，紧接着，数据包会经过 <strong><code>NF_IP_PRE_ROUTING</code></strong> 钩子。所有注册在此钩子上的 <code>Netfilter</code> 处理函数（来自 <code>raw</code>, <code>mangle</code>, <code>nat</code> 表）会依次执行。这是进行 DNAT 或早期过滤&#x2F;修改的关键点。</li>
<li>如果 <code>Netfilter</code> 函数返回 <code>NF_DROP</code>，数据包处理流程终止。如果返回 <code>NF_ACCEPT</code>，则继续。</li>
</ul>
</li>
<li><p><strong><code>ip_rcv_finish()</code> - 路由决策:</strong></p>
<ul>
<li>通过 <code>NF_IP_PRE_ROUTING</code> 钩子后，数据包进入 <code>ip_rcv_finish()</code>。此函数的核心任务是进行路由查找，决定数据包的下一跳。它会调用 <code>ip_route_input_slow()</code>（或其快速路径缓存）来查询路由表。</li>
<li>路由查找的结果会填充到 <code>skb-&gt;dst</code> (destination cache entry) 结构中，该结构包含了路由决策的全部信息，包括下一跳地址、输出设备、以及指向后续处理函数的指针（如 <code>dst-&gt;input</code> 和 <code>dst-&gt;output</code>）。</li>
</ul>
</li>
<li><p><strong>根据路由结果分发:</strong></p>
<ul>
<li><strong>目的为本机 (<code>dst-&gt;input == ip_local_deliver</code>):</strong> 如果路由查找确定数据包的目的 IP 是本机的某个地址，<code>dst_input(skb)</code> 最终会调用 <code>ip_local_deliver()</code>。<ul>
<li><strong><code>ip_local_deliver()</code> 与 <code>NF_IP_LOCAL_IN</code>:</strong> 在将数据包传递给更上层的协议（如 TCP 的 <code>tcp_v4_rcv</code> 或 UDP 的 <code>udp_rcv</code>）之前，<code>ip_local_deliver()</code> 会首先调用 <code>ip_local_deliver_finish()</code>，在这里会触发 <strong><code>NF_IP_LOCAL_IN</code></strong> 钩子。注册在此钩子上的 <code>Netfilter</code> 处理函数（来自 <code>mangle</code>, <code>filter</code>, <code>security</code>, <code>nat</code> 表）会被执行，主要用于对访问本机服务的数据包进行过滤。</li>
</ul>
</li>
<li><strong>需要转发 (<code>dst-&gt;input == ip_forward</code>):</strong> 如果路由查找确定数据包需要被转发到另一个网络接口，<code>dst_input(skb)</code> 最终会调用 <code>ip_forward()</code>。<ul>
<li><strong><code>ip_forward()</code> 与 <code>NF_IP_FORWARD</code>:</strong> <code>ip_forward()</code> 函数负责处理数据包的转发逻辑。在进行必要的检查（如 TTL 检查）之后，它会调用 <code>ip_forward_finish()</code>，在这里会触发 <strong><code>NF_IP_FORWARD</code></strong> 钩子。注册在此钩子上的 <code>Netfilter</code> 处理函数（来自 <code>mangle</code>, <code>filter</code>, <code>security</code> 表）会被执行，用于对转发的数据包进行过滤和修改。</li>
<li><strong>TTL 递减与 MTU 处理:</strong> 在 <code>ip_forward()</code> 过程中，IP 头部的 TTL 值会被减 1。如果 TTL 变为 0，数据包会被丢弃，并可能发送 ICMP Time Exceeded 消息。如果数据包大小超过了出口设备的 MTU 且允许分片，还会进行 IP 分片。</li>
</ul>
</li>
<li><strong>多播处理 (<code>dst-&gt;input == ip_mr_input</code>):</strong> 如果是多播数据包且本机配置了多播路由，会进入多播转发流程。</li>
</ul>
</li>
<li><p><strong>数据包发送路径与 <code>NF_IP_LOCAL_OUT</code> 和 <code>NF_IP_POST_ROUTING</code>:</strong></p>
<ul>
<li><strong>本地产生的数据包 (<code>ip_queue_xmit</code>):</strong> 当本地应用程序通过 Socket API 发送数据时，数据会逐层向下传递（例如 TCP -&gt; IP）。在 IP 层，<code>ip_queue_xmit()</code> 或类似函数负责构建 IP 头部并准备发送。<ul>
<li><strong><code>NF_IP_LOCAL_OUT</code>:</strong> 在 <code>ip_queue_xmit()</code> 内部，构建完 IP 头并进行初步路由查找（确定源地址和出口设备等）之后，会触发 <strong><code>NF_IP_LOCAL_OUT</code></strong> 钩子。注册在此钩子上的 <code>Netfilter</code> 处理函数（来自 <code>raw</code>, <code>mangle</code>, <code>nat</code>, <code>filter</code>, <code>security</code> 表）会被执行，用于对本地产生的数据包进行处理。</li>
</ul>
</li>
<li><strong>最终路由与 <code>NF_IP_POST_ROUTING</code>:</strong> 无论是本地产生的包还是需要转发的包，在最终确定所有 IP 头部字段（特别是经过了可能的 NAT 修改后）、选定出口网络设备，即将调用邻居子系统（ARP 或 NDP）解析下一跳 MAC 地址并将数据包传递给设备驱动程序之前，都会通过 <code>ip_output()</code>（单播）或 <code>ip_mc_output()</code>（多播）等函数，最终触发 <strong><code>NF_IP_POST_ROUTING</code></strong> 钩子。注册在此钩子上的 <code>Netfilter</code> 处理函数（来自 <code>mangle</code>, <code>nat</code> 表）会被执行。这是执行 SNAT 或进行最后修改的理想位置。</li>
<li><strong><code>dst_output()</code>:</strong> 最终，<code>dst_output(skb)</code> 函数会调用 <code>skb-&gt;dst-&gt;output</code> 指针指向的函数（如 <code>ip_output</code>），它会调用 <code>ip_finish_output()</code>，后者将 <code>sk_buff</code> 交给邻居子系统（<code>neigh_output</code>）和网络设备驱动进行 L2 封装和物理发送。</li>
</ul>
</li>
</ol>
<img src="/2025/04/10/kube-proxy/5.jpg" srcset="/img/loading.gif" lazyload class="" title="kube">
<p><em>TCP 层发送路径示意图，显示了数据包向下传递至 IP 层的过程，最终也会触发 Netfilter 的 OUT&#x2F;POSTROUTING 钩子。</em></p>
<h2 id="Netfilter-Hook-函数的注册与实现细节"><a href="#Netfilter-Hook-函数的注册与实现细节" class="headerlink" title="Netfilter Hook 函数的注册与实现细节"></a>Netfilter Hook 函数的注册与实现细节</h2><p><code>Netfilter</code> 框架的核心在于允许内核模块动态地注册和注销钩子处理函数。这使得内核的功能可以灵活扩展，而无需修改核心代码。</p>
<h3 id="注册和注销-Netfilter-Hook"><a href="#注册和注销-Netfilter-Hook" class="headerlink" title="注册和注销 Netfilter Hook"></a>注册和注销 Netfilter Hook</h3><p>注册一个钩子处理函数主要依赖于 <code>struct nf_hook_ops</code> 结构和 <code>nf_register_net_hook()</code> &#x2F; <code>nf_unregister_net_hook()</code> (或者针对特定网络命名空间的版本 <code>nf_register_hook</code> &#x2F; <code>nf_unregister_hook</code>) 函数。</p>
<p><code>struct nf_hook_ops</code> 结构定义在 <code>&lt;linux/netfilter.h&gt;</code> 中，其关键成员如下：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">nf_hook_ops</span> &#123;</span><br>    <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">list_head</span>    <span class="hljs-title">list</span>;</span>       <span class="hljs-comment">// 用于将 ops 链入 Netfilter 内部链表，内核管理</span><br><br>    <span class="hljs-comment">/* User fills in from here down. */</span><br>    nf_hookfn           *hook;      <span class="hljs-comment">// 指向实际处理数据包的函数指针</span><br>    <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">module</span>       *<span class="hljs-title">owner</span>;</span>     <span class="hljs-comment">// 指向拥有此 hook 的内核模块，用于引用计数</span><br>    <span class="hljs-type">u_int8_t</span>            pf;         <span class="hljs-comment">// 协议族 (Protocol Family)，如 PF_INET (IPv4), PF_INET6 (IPv6), PF_BRIDGE (桥接)</span><br>    <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span>        hooknum;    <span class="hljs-comment">// 指定要挂载的钩子点，如 NF_IP_PRE_ROUTING</span><br>    <span class="hljs-comment">/* Hooks are ordered in ascending priority. */</span><br>    <span class="hljs-type">int</span>                 priority;   <span class="hljs-comment">// 钩子函数的优先级，数值越小越先执行。内核定义了一些标准优先级，如 NF_IP_PRI_FIRST, NF_IP_PRI_FILTER, NF_IP_PRI_NAT_DST, NF_IP_PRI_NAT_SRC, NF_IP_PRI_LAST 等。</span><br>&#125;;<br></code></pre></td></tr></table></figure>

<ol>
<li><code>list</code>：此成员由Netfilter框架内部管理，用于将注册的<code>nf_hook_ops</code>组织成链表。对于在同一协议族（<code>pf</code>）和同一挂接点（<code>hooknum</code>）注册的多个hook函数，内核正是通过遍历这个链表来依次调用它们的。模块开发者在注册时无需关心此字段。</li>
<li><code>hook</code>：这是一个函数指针，指向类型为<code>nf_hookfn</code>的函数。这正是你的模块提供的核心处理逻辑，当匹配的网络数据包经过指定的<code>hooknum</code>时，内核将调用此函数。<code>nf_hookfn</code>的函数原型我们稍后会详细解析。</li>
<li><code>owner</code>：通常设置为<code>THIS_MODULE</code>宏，用于内核的模块引用计数管理。当模块被卸载时，内核可以通过这个指针自动注销其注册的hooks，防止出现悬挂指针导致系统崩溃。虽然示例代码中设置为<code>NULL</code>，但在生产级代码中，正确设置<code>owner</code>是保证系统稳定性的重要实践。</li>
<li><code>pf</code>：指定此hook函数适用的协议族（Protocol Family）。常见的协议族定义在<code>linux/socket.h</code>中，例如<code>PF_INET</code>代表IPv4协议栈，<code>PF_INET6</code>代表IPv6，<code>PF_BRIDGE</code>用于网桥等。你的hook函数只会处理属于指定协议族的数据包。</li>
<li><code>hooknum</code>：这明确了你的hook函数要挂载到Netfilter处理流程中的哪个具体位置。对于IPv4 (<code>PF_INET</code>)，这些挂接点（如<code>NF_INET_PRE_ROUTING</code>, <code>NF_INET_LOCAL_IN</code>, <code>NF_INET_FORWARD</code>, <code>NF_INET_LOCAL_OUT</code>, <code>NF_INET_POST_ROUTING</code>）定义在<code>linux/netfilter_ipv4.h</code>中，它们对应了数据包在内核中处理的不同阶段。</li>
<li><code>priority</code>：定义了在同一挂接点（<code>hooknum</code>）上注册的多个hook函数之间的执行优先级。优先级是一个整数，数值越小，优先级越高，越先被执行。内核提供了一系列预定义的优先级常量，例如<code>NF_IP_PRI_FIRST</code>（最高优先级）、<code>NF_IP_PRI_CONNTRACK</code>、<code>NF_IP_PRI_NAT_DST</code>、<code>NF_IP_PRI_FILTER</code>、<code>NF_IP_PRI_NAT_SRC</code>、<code>NF_IP_PRI_LAST</code>（最低优先级）等，定义在<code>linux/netfilter_ipv4.h</code>的<code>nf_ip_hook_priorities</code>枚举中。选择合适的优先级对于确保你的hook函数在正确的时机（例如，在NAT转换之前或之后）执行至关重要。</li>
</ol>
<p>要将你定义的<code>nf_hook_ops</code>结构体实例注册到Netfilter框架中，你需要调用<code>nf_register_net_hook()</code>函数（或者在较新内核中推荐使用针对特定netns的<code>nf_register_net_hook()</code>，如果你的模块需要感知网络命名空间的话；对于简单的全局hook，<code>nf_register_hook()</code>是一个历史接口，现在通常封装了<code>nf_register_net_hook(&amp;init_net, ops)</code>）。此函数接受一个指向<code>nf_hook_ops</code>结构体的指针作为参数。注册成功，你的hook函数就会成为内核网络处理流程的一部分。相应地，当你的内核模块卸载时，必须调用<code>nf_unregister_net_hook()</code>（或<code>nf_unregister_hook()</code>）并传入相同的<code>nf_hook_ops</code>结构体指针，以将其从Netfilter框架中移除，释放资源并避免潜在的错误。</p>
<p>下面的示例代码演示了这一注册与注销过程。它注册了一个简单的hook函数，挂载在IPv4协议栈的<code>NF_INET_LOCAL_OUT</code>挂接点（即本机进程发出的数据包在路由决策之后、发送到网络接口之前的位置），并赋予其最高优先级(<code>NF_IP_PRI_FIRST</code>)。该hook函数的实现极为简单粗暴：直接返回<code>NF_DROP</code>，这意味着所有经由此挂接点的IPv4数据包都将被丢弃。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;linux/kernel.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;linux/init.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;linux/module.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;linux/version.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;linux/skbuff.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;linux/netfilter.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;linux/netfilter_ipv4.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;linux/ip.h&gt;</span> <span class="hljs-comment">// Include for iphdr structure access if needed later</span></span><br><br>MODULE_LICENSE(<span class="hljs-string">&quot;GPL&quot;</span>);<br>MODULE_AUTHOR(<span class="hljs-string">&quot;xsc&quot;</span>);<br><br><span class="hljs-type">static</span> <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">nf_hook_ops</span> <span class="hljs-title">nfho</span>;</span><br><br><span class="hljs-comment">// The hook function itself</span><br><span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> <span class="hljs-title function_">hook_func</span><span class="hljs-params">(<span class="hljs-type">void</span> *priv, <span class="hljs-comment">// priv is unused in this simplified example, corresponds to nf_hook_ops</span></span><br><span class="hljs-params">                       <span class="hljs-keyword">struct</span> sk_buff *skb,</span><br><span class="hljs-params">                       <span class="hljs-type">const</span> <span class="hljs-keyword">struct</span> nf_hook_state *state)</span> <span class="hljs-comment">// Modern prototype uses nf_hook_state</span><br>&#123;<br>    <span class="hljs-comment">// In older kernels or simpler contexts, the prototype might be:</span><br>    <span class="hljs-comment">// unsigned int hook_func(unsigned int hooknum, struct sk_buff *skb,</span><br>    <span class="hljs-comment">//                        const struct net_device *in, const struct net_device *out,</span><br>    <span class="hljs-comment">//                        int (*okfn)(struct sk_buff *))</span><br><br>    printk(KERN_INFO <span class="hljs-string">&quot;Packet dropped by example hook!\n&quot;</span>); <span class="hljs-comment">// Good practice to log actions</span><br>    <span class="hljs-keyword">return</span> NF_DROP; <span class="hljs-comment">// Discard the packet</span><br>    <span class="hljs-comment">// Other possible return values:</span><br>    <span class="hljs-comment">// NF_ACCEPT: Continue processing the packet normally.</span><br>    <span class="hljs-comment">// NF_STOLEN: The hook function has taken ownership of the packet (e.g., queued it for userspace),</span><br>    <span class="hljs-comment">//            Netfilter should stop processing it.</span><br>    <span class="hljs-comment">// NF_QUEUE: Queue the packet for userspace processing (used by tools like iptables -j QUEUE).</span><br>    <span class="hljs-comment">// NF_REPEAT: Call this hook function again (use with caution).</span><br>&#125;<br><br><br><span class="hljs-type">static</span> <span class="hljs-type">int</span> __init <span class="hljs-title function_">kexec_test_init</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span><br>&#123;<br>    printk(KERN_INFO <span class="hljs-string">&quot;kexec test module loading...\n&quot;</span>);<br><br>    nfho.hook = hook_func;<br>    nfho.owner = THIS_MODULE; <span class="hljs-comment">// Recommended practice</span><br>    nfho.pf = PF_INET;                   <span class="hljs-comment">// Target IPv4 protocol family</span><br>    nfho.hooknum = NF_INET_LOCAL_OUT;    <span class="hljs-comment">// Hook at the local output stage</span><br>    nfho.priority = NF_IP_PRI_FIRST;     <span class="hljs-comment">// Execute with the highest priority</span><br><br>    <span class="hljs-comment">// Register the hook for the initial network namespace</span><br><span class="hljs-meta">#<span class="hljs-keyword">if</span> LINUX_VERSION_CODE &gt;= KERNEL_VERSION(4, 13, 0)</span><br>    <span class="hljs-type">int</span> ret = nf_register_net_hook(&amp;init_net, &amp;nfho);<br><span class="hljs-meta">#<span class="hljs-keyword">else</span></span><br>    <span class="hljs-type">int</span> ret = nf_register_hook(&amp;nfho); <span class="hljs-comment">// Older kernel registration</span><br><span class="hljs-meta">#<span class="hljs-keyword">endif</span></span><br><br>    <span class="hljs-keyword">if</span> (ret &lt; <span class="hljs-number">0</span>) &#123;<br>        printk(KERN_ERR <span class="hljs-string">&quot;Failed to register Netfilter hook: %d\n&quot;</span>, ret);<br>        <span class="hljs-keyword">return</span> ret;<br>    &#125;<br><br>    printk(KERN_INFO <span class="hljs-string">&quot;Netfilter hook registered successfully.\n&quot;</span>);<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>; <span class="hljs-comment">// Success</span><br>&#125;<br><br><span class="hljs-type">static</span> <span class="hljs-type">void</span> __exit <span class="hljs-title function_">kexec_test_exit</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span><br>&#123;<br>    printk(KERN_INFO <span class="hljs-string">&quot;kexec test module unloading...\n&quot;</span>);<br><span class="hljs-meta">#<span class="hljs-keyword">if</span> LINUX_VERSION_CODE &gt;= KERNEL_VERSION(4, 13, 0)</span><br>    nf_unregister_net_hook(&amp;init_net, &amp;nfho);<br><span class="hljs-meta">#<span class="hljs-keyword">else</span></span><br>    nf_unregister_hook(&amp;nfho); <span class="hljs-comment">// Older kernel unregistration</span><br><span class="hljs-meta">#<span class="hljs-keyword">endif</span></span><br>    printk(KERN_INFO <span class="hljs-string">&quot;Netfilter hook unregistered.\n&quot;</span>);<br>&#125;<br><br>module_init(kexec_test_init); <span class="hljs-comment">// Register the init function</span><br>module_exit(kexec_test_exit); <span class="hljs-comment">// Register the exit function</span><br></code></pre></td></tr></table></figure>

<p><em>(Note: The provided example code was slightly adjusted for better practice (logging, <code>THIS_MODULE</code>, modern registration API), but kept the core logic as requested. The original prototype of the hook function is also mentioned for context.)</em></p>
<h3 id="Hook函数的实现细节"><a href="#Hook函数的实现细节" class="headerlink" title="Hook函数的实现细节"></a>Hook函数的实现细节</h3><p>理解<code>nf_hookfn</code>的原型对于编写有效的Netfilter hook至关重要。虽然原型在不同内核版本中略有演变，但核心传递的信息保持一致。我们来看一个常见的（略旧但更易于解释基础概念的）原型，定义在<code>linux/netfilter.h</code>：</p>
<C>

<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-keyword">typedef</span> <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> <span class="hljs-title function_">nf_hookfn</span><span class="hljs-params">(<span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> hooknum,<span class="hljs-keyword">struct</span> sk_buff *skb,<span class="hljs-type">const</span> <span class="hljs-keyword">struct</span> net_device *in,<span class="hljs-type">const</span> <span class="hljs-keyword">struct</span> net_device *out,<span class="hljs-type">int</span> (*okfn)(<span class="hljs-keyword">struct</span> sk_buff *))</span>;<br></code></pre></td></tr></table></figure>

<p><em>(较新内核倾向于使用 <code>nf_hookfn(void \*priv, struct sk_buff \*skb, const struct nf_hook_state \*state)</code>，其中<code>state</code>结构体封装了<code>hooknum</code>, <code>in</code>, <code>out</code>, <code>sk</code>等信息，<code>priv</code>通常指向注册时<code>nf_hook_ops</code>结构体本身)</em></p>
<p>让我们解析这个原型中的参数：</p>
<ul>
<li><code>hooknum</code>: 这个无符号整数明确告知hook函数当前是在哪个Netfilter挂接点被调用的（例如 <code>NF_INET_PRE_ROUTING</code>）。这使得一个函数可以服务于多个挂接点，并根据<code>hooknum</code>执行不同的逻辑。</li>
<li><code>skb</code>: 这是指向<code>struct sk_buff</code>的指针，是Linux内核中表示网络数据包的核心数据结构。<code>sk_buff</code>（Socket Buffer）不仅包含数据包的原始负载，还携带了大量的元数据，如协议头指针、路由信息、时间戳、关联的socket等。访问和操作<code>skb</code>是Netfilter hook函数进行包检查和修改的基础。</li>
<li><code>in</code>: 指向<code>struct net_device</code>的指针，代表数据包进入系统的网络接口。这个参数<strong>仅在数据包是接收路径上的特定挂接点上</strong>（如<code>NF_INET_PRE_ROUTING</code>, <code>NF_INET_LOCAL_IN</code>）才有意义，此时它指向接收该数据包的物理或虚拟网络设备。在其他挂接点（如<code>NF_INET_LOCAL_OUT</code>, <code>NF_INET_POST_ROUTING</code>），<code>in</code>通常为<code>NULL</code>。</li>
<li><code>out</code>: 同样是指向<code>struct net_device</code>的指针，代表数据包计划离开系统的网络接口。这个参数<strong>仅在数据包是发送路径上的特定挂接点上</strong>（如<code>NF_INET_LOCAL_OUT</code>, <code>NF_INET_POST_ROUTING</code>）才有意义，指向数据包将被发送出去的网络设备。在接收相关的挂接点，<code>out</code>通常为<code>NULL</code>。理解<code>in</code>和<code>out</code>的有效性取决于<code>hooknum</code>是至关重要的。</li>
<li><code>okfn</code>: 这是一个函数指针，原型为<code>int (*okfn)(struct sk_buff *)</code>。在某些复杂的Netfilter场景（尤其是在旧版本或特定子系统中），它指向下一个应该处理该数据包的函数或决策点。然而，在大多数现代的、简单的过滤场景下，hook函数的行为主要由其返回值（如 <code>NF_ACCEPT</code>, <code>NF_DROP</code>）决定，<code>okfn</code>的使用并不普遍，可以直接忽略。</li>
</ul>
<h3 id="Netfilter-报文过滤技术实现"><a href="#Netfilter-报文过滤技术实现" class="headerlink" title="Netfilter 报文过滤技术实现"></a>Netfilter 报文过滤技术实现</h3><p>Netfilter 构成了 Linux 内核网络栈的核心部分，它提供了一套强大的钩子（Hooks）机制，允许内核模块在数据包流经网络协议栈的关键路径点上注册回调函数，从而实现对数据包的检查、修改、丢弃或重新注入等操作。报文过滤是 Netfilter 最为经典和广泛的应用之一，诸如 iptables、nftables 等用户空间工具正是基于 Netfilter 框架来实现防火墙功能的。接下来，我们将深入探讨几种基于 Netfilter 实现报文过滤的具体技术途径。</p>
<h4 id="基于网络接口的过滤"><a href="#基于网络接口的过滤" class="headerlink" title="基于网络接口的过滤"></a>基于网络接口的过滤</h4><p>在网络数据包的处理流程中，识别数据包的来源或目的地网络接口是一项基本的过滤需求。Linux 内核中使用 <code>struct net_device</code> 结构来抽象表示一个网络接口（如 eth0, lo 等）。每个流经网络栈的数据包都由一个 <code>struct sk_buff</code> (skb) 结构体表示，该结构体内部包含了指向相关网络设备的指针，通常是 <code>skb-&gt;dev</code>，它指向接收或发送该数据包的网络设备。通过在 Netfilter 钩子函数中访问这个 <code>skb-&gt;dev</code> 指针，并进一步读取其 <code>name</code> 成员（包含接口名称），就可以实现基于接口的过滤策略。例如，如果我们的策略是阻止所有进入 “eth0” 接口的数据包，那么在钩子函数中，当检测到 <code>skb-&gt;dev-&gt;name</code> 与 “eth0” 匹配时，函数直接返回 <code>NF_DROP</code> 即可。这个返回值会通知 Netfilter 框架立即丢弃该数据包，并释放相关的 <code>sk_buff</code> 资源，数据包将不再继续在网络栈中传递。</p>
<h4 id="基于-IP-地址的过滤"><a href="#基于-IP-地址的过滤" class="headerlink" title="基于 IP 地址的过滤"></a>基于 IP 地址的过滤</h4><p>基于源或目的 IP 地址进行过滤是防火墙最核心的功能之一。Netfilter 同样为此提供了便捷的实现方式。<code>sk_buff</code> 结构体中包含了指向各层协议头部的指针。网络层（IP层）的头部指针可以通过 <code>skb-&gt;network_header</code> 访问，或者更常用的方式是使用内核提供的辅助宏 <code>ip_hdr(skb)</code> 来获取指向 <code>struct iphdr</code>（定义于 <code>&lt;linux/ip.h&gt;</code>）的指针。这个结构体详细定义了 IPv4 报头的所有字段，包括源 IP 地址 (<code>saddr</code>) 和目的 IP 地址 (<code>daddr</code>)。需要注意的是，这些地址在内存中是以网络字节序（Big Endian）存储的。在 Netfilter 钩子函数中，我们可以提取出这些地址字段，并将其与过滤规则中定义的特定 IP 地址或地址范围进行比较（比较时通常需要使用 <code>ntohl()</code> 等函数将网络字节序转换为主机字节序，或者直接以网络字节序进行比较）。如果数据包的源地址或目的地址满足了我们设定的丢弃条件（例如，阻止来自某个特定恶意 IP 的所有连接请求），钩子函数便返回 <code>NF_DROP</code>，从而实现对该数据包的精确过滤。</p>
<h4 id="基于-TCP-端口的过滤"><a href="#基于-TCP-端口的过滤" class="headerlink" title="基于 TCP 端口的过滤"></a>基于 TCP 端口的过滤</h4><p>当需要进行更细粒度的控制，例如阻止对特定服务端口的访问时，就需要深入到传输层进行过滤。对于 TCP 协议而言，这意味着我们需要检查 TCP 头部中的源端口和目的端口。在 Netfilter 钩子函数中，这通常发生在确认了数据包是 IP 包（通过检查 <code>iphdr</code>）并且其协议字段 (<code>iph-&gt;protocol</code>) 指示为 <code>IPPROTO_TCP</code> 之后。要获取 TCP 头部的指针，我们需要知道 IP 头部的实际长度，因为 IP 头部可能包含选项，长度并非固定。<code>struct iphdr</code> 中的 <code>ihl</code> (Internet Header Length) 字段表示 IP 头部的长度，但其单位是 4 字节（32位字）。因此，TCP 头部的起始位置可以通过将 IP 头部指针（<code>iph</code>）加上 IP 头部的字节长度（<code>iph-&gt;ihl * 4</code>）来计算得到。获取到指向 <code>struct tcphdr</code>（定义于 <code>&lt;linux/tcp.h&gt;</code>）的指针后，我们就可以访问其成员，如源端口 (<code>source</code>) 和目的端口 (<code>dest</code>)。这些端口号同样是以网络字节序存储的。下面的代码片段展示了一个简单的示例，演示了如何在 Netfilter 钩子函数中检查 TCP 目的端口，并在端口号为 25 (SMTP 服务的默认端口) 时丢弃数据包：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;linux/ip.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;linux/tcp.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;linux/netfilter.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;linux/netfilter_ipv4.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;linux/skbuff.h&gt;</span></span><br><br><span class="hljs-type">static</span> <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> <span class="hljs-title function_">check_tcp_packet_hook</span><span class="hljs-params">(<span class="hljs-type">void</span> *priv,</span><br><span class="hljs-params">                                          <span class="hljs-keyword">struct</span> sk_buff *skb,</span><br><span class="hljs-params">                                          <span class="hljs-type">const</span> <span class="hljs-keyword">struct</span> nf_hook_state *state)</span><br>&#123;<br>    <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">tcphdr</span> *<span class="hljs-title">tcph</span> =</span> <span class="hljs-literal">NULL</span>;<br>    <span class="hljs-type">const</span> <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">iphdr</span> *<span class="hljs-title">iph</span> =</span> <span class="hljs-literal">NULL</span>;<br>    __be16 dport;<br><br>    <span class="hljs-comment">// skb 为空则直接接受，虽然理论上 Netfilter 不应传递空 skb</span><br>    <span class="hljs-keyword">if</span> (!skb) &#123;<br>        <span class="hljs-keyword">return</span> NF_ACCEPT;<br>    &#125;<br><br>    <span class="hljs-comment">// 获取 IP 头部指针</span><br>    iph = ip_hdr(skb);<br>    <span class="hljs-comment">// 检查是否为 NULL 以及是否为 IPv4 包（通常在注册钩子时已指定协议族）</span><br>    <span class="hljs-keyword">if</span> (!iph) &#123;<br>        <span class="hljs-keyword">return</span> NF_ACCEPT;<br>    &#125;<br><br>    <span class="hljs-comment">// 检查 IP 协议字段是否为 TCP</span><br>    <span class="hljs-keyword">if</span> (iph-&gt;protocol == IPPROTO_TCP) &#123;<br>        <span class="hljs-comment">// 计算 TCP 头部的起始位置</span><br>        <span class="hljs-comment">// 注意：这里直接使用 iph 指针，因为 ip_hdr(skb) 返回的就是 skb-&gt;network_header</span><br>        <span class="hljs-comment">// (void *)iph + iph-&gt;ihl * 4 是计算 TCP 头地址的标准方式</span><br>        <span class="hljs-comment">// 需要确保 skb 足够长，包含完整的 TCP 头</span><br>        <span class="hljs-keyword">if</span> (!pskb_may_pull(skb, iph-&gt;ihl*<span class="hljs-number">4</span> + <span class="hljs-keyword">sizeof</span>(<span class="hljs-keyword">struct</span> tcphdr))) &#123;<br>             <span class="hljs-comment">// 如果 skb 长度不足以包含 IP 头和 TCP 头，则接受或记录错误</span><br>             <span class="hljs-keyword">return</span> NF_ACCEPT; <br>        &#125;<br>        <span class="hljs-comment">// 重新获取 IP 头指针，因为 pskb_may_pull 可能改变 skb-&gt;data</span><br>        iph = ip_hdr(skb); <br>        tcph = (<span class="hljs-keyword">struct</span> tcphdr *)((__u8 *)iph + (iph-&gt;ihl * <span class="hljs-number">4</span>));<br><br><br>        <span class="hljs-comment">// 获取 TCP 目的端口 (网络字节序)</span><br>        dport = tcph-&gt;dest;<br><br>        <span class="hljs-comment">// 将目的端口从网络字节序转换为主机字节序进行比较</span><br>        <span class="hljs-keyword">if</span> (ntohs(dport) == <span class="hljs-number">25</span>) &#123;<br>            printk(KERN_INFO <span class="hljs-string">&quot;Dropping TCP packet to port 25\n&quot;</span>); <span class="hljs-comment">// 日志记录，便于调试</span><br>            <span class="hljs-keyword">return</span> NF_DROP; <span class="hljs-comment">// 丢弃目标端口为 25 的 TCP 包</span><br>        &#125; <span class="hljs-keyword">else</span> &#123;<br>            <span class="hljs-comment">// 如果端口不是 25，则接受该 TCP 包</span><br>            <span class="hljs-keyword">return</span> NF_ACCEPT;<br>        &#125;<br>    &#125;<br><br>    <span class="hljs-comment">// 如果不是 TCP 包，则接受（根据具体策略可能需要调整）</span><br>    <span class="hljs-keyword">return</span> NF_ACCEPT;<br>&#125;<br><br><span class="hljs-comment">// 注意：实际使用中需要将此函数注册到 Netfilter 钩子点，例如：</span><br><span class="hljs-comment">// static struct nf_hook_ops tcp_filter_hook_ops = &#123;</span><br><span class="hljs-comment">//     .hook     = check_tcp_packet_hook,</span><br><span class="hljs-comment">//     .pf       = NFPROTO_IPV4, // 或 PF_INET</span><br><span class="hljs-comment">//     .hooknum  = NF_INET_PRE_ROUTING, // 或者其他合适的钩子点</span><br><span class="hljs-comment">//     .priority = NF_IP_PRI_FILTER,    // 合适的优先级</span><br><span class="hljs-comment">//     .owner    = THIS_MODULE,</span><br><span class="hljs-comment">// &#125;;</span><br><span class="hljs-comment">// 在模块初始化时 nf_register_net_hook(&amp;init_net, &amp;tcp_filter_hook_ops);</span><br><span class="hljs-comment">// 在模块退出时 nf_unregister_net_hook(&amp;init_net, &amp;tcp_filter_hook_ops);</span><br></code></pre></td></tr></table></figure>

<hr>
<h4 id="kube-proxy-iptables-模式工作原理详解"><a href="#kube-proxy-iptables-模式工作原理详解" class="headerlink" title="kube-proxy iptables 模式工作原理详解"></a>kube-proxy iptables 模式工作原理详解</h4><img src="/2025/04/10/kube-proxy/image-20250412133507210.png" srcset="/img/loading.gif" lazyload class="" title="image-20250412133507210">

<p><code>kube-proxy</code> 在 iptables 模式下，通过在 <code>nat</code> 表和 <code>filter</code> 表（较少）中创建一系列自定义链和规则，巧妙地利用了 Netfilter 的钩子点来实现 Service 功能。</p>
<p><strong>核心 iptables 链 (位于 <code>nat</code> 表):</strong></p>
<ul>
<li><strong><code>KUBE-SERVICES</code>:</strong> 所有 Service 流量的总入口。规则根据目标 IP (ClusterIP) 或目标端口 (NodePort) 跳转到相应的 Service 处理链。挂载在 <code>PREROUTING</code> (外部&#x2F;跨节点流量) 和 <code>OUTPUT</code> (节点内部流量) 链上。</li>
<li><strong><code>KUBE-NODEPORTS</code>:</strong> 处理 NodePort 类型服务的入口。规则根据目标端口跳转到对应的 Service 处理链，并通常先跳转到 <code>KUBE-MARK-MASQ</code>。挂载在 <code>PREROUTING</code> 和 <code>OUTPUT</code> 链上（通常通过 <code>KUBE-SERVICES</code> 跳转过来）。</li>
<li><strong><code>KUBE-SVC-&lt;hash&gt;</code>:</strong> 每个 Service 对应一个此链。负责<strong>负载均衡</strong>。包含多条规则，每条规则对应一个健康的 Endpoint (Pod)。使用 <code>statistic</code> 模块（<code>--mode random --probability P</code> 或 <code>--mode nth</code>) 实现 Pod 选择，然后跳转到对应的 Endpoint 处理链。</li>
<li><strong><code>KUBE-SEP-&lt;hash&gt;</code>:</strong> 每个 Endpoint (Service EndPoint) 对应一个此链。负责<strong>执行 DNAT</strong>。包含一条 <code>-j DNAT --to-destination &lt;pod-ip&gt;:&lt;target-port&gt;</code> 规则，将数据包目标地址修改为实际 Pod IP 和端口。通常也会先跳转到 <code>KUBE-MARK-MASQ</code>。</li>
<li><strong><code>KUBE-MARK-MASQ</code>:</strong> 给需要进行源地址伪装 (SNAT&#x2F;Masquerade) 的数据包打上一个 Netfilter 标记 (通常是 <code>0x4000</code>)。例如，从 NodePort 进来的流量，或者 <code>externalTrafficPolicy: Cluster</code> 时跨节点转发的流量。</li>
<li><strong><code>KUBE-POSTROUTING</code>:</strong> 挂载在 <code>POSTROUTING</code> 链上。检查数据包是否有 <code>KUBE-MARK-MASQ</code> 设置的标记，如果有，则执行 <code>-j MASQUERADE</code>，将源 IP 修改为出接口的 IP 地址，确保响应流量能正确返回。</li>
</ul>
<p><strong>流量路径示例 (外部访问 NodePort):</strong></p>
<ol>
<li>外部请求 -&gt; NodeIP:NodePort</li>
<li>数据包到达 Node，进入 <code>PREROUTING</code> Hook。</li>
<li><code>nat</code> 表 <code>PREROUTING</code> 链 -&gt; <code>KUBE-SERVICES</code> 链。</li>
<li><code>KUBE-SERVICES</code> 链匹配 NodePort -&gt; <code>KUBE-NODEPORTS</code> 链。</li>
<li><code>KUBE-NODEPORTS</code> 链匹配端口 -&gt; <code>KUBE-MARK-MASQ</code> (打标记 <code>0x4000</code>) -&gt; <code>KUBE-SVC-&lt;hash&gt;</code> 链。</li>
<li><code>KUBE-SVC-&lt;hash&gt;</code> 链根据负载均衡策略 (e.g., random) -&gt; <code>KUBE-SEP-&lt;hash&gt;</code> 链。</li>
<li><code>KUBE-SEP-&lt;hash&gt;</code> 链 -&gt; <code>KUBE-MARK-MASQ</code> (可能重复打标，无害) -&gt; 执行 <code>DNAT --to-destination &lt;pod-ip&gt;:&lt;target-port&gt;</code>。</li>
<li>数据包目标地址变为 Pod IP，内核重新路由，可能在本机或转发到其他 Node。</li>
<li>数据包离开 Node，进入 <code>POSTROUTING</code> Hook。</li>
<li><code>nat</code> 表 <code>POSTROUTING</code> 链 -&gt; <code>KUBE-POSTROUTING</code> 链。</li>
<li><code>KUBE-POSTROUTING</code> 链匹配标记 <code>0x4000</code> -&gt; 执行 <code>MASQUERADE</code> (源 IP 变为 Node IP)。</li>
<li>数据包发送给目标 Pod。</li>
</ol>
<p><strong>流量路径示例 (Pod 访问 ClusterIP):</strong></p>
<ol>
<li>Pod A (Client) -&gt; ClusterIP:Port</li>
<li>数据包在本机产生，进入 <code>OUTPUT</code> Hook。</li>
<li><code>nat</code> 表 <code>OUTPUT</code> 链 -&gt; <code>KUBE-SERVICES</code> 链。</li>
<li><code>KUBE-SERVICES</code> 链匹配 ClusterIP -&gt; <code>KUBE-SVC-&lt;hash&gt;</code> 链。</li>
<li><code>KUBE-SVC-&lt;hash&gt;</code> 链 -&gt; <code>KUBE-SEP-&lt;hash&gt;</code> 链 (选择目标 Pod B)。</li>
<li><code>KUBE-SEP-&lt;hash&gt;</code> 链 -&gt; 执行 <code>DNAT --to-destination &lt;podB-ip&gt;:&lt;target-port&gt;</code>。</li>
<li>数据包目标地址变为 Pod B IP，内核重新路由。</li>
<li>如果 Pod B 在同一节点，数据包直接转发给 Pod B。</li>
<li>如果 Pod B 在不同节点，数据包进入 <code>POSTROUTING</code> Hook。</li>
<li><code>nat</code> 表 <code>POSTROUTING</code> 链 -&gt; <code>KUBE-POSTROUTING</code> 链。</li>
<li><strong>是否 MASQUERADE 取决于 CNI 和网络策略。</strong> 如果 Pod 网络 (如 Calico BGP 模式) 可以直接路由，则可能不需要 Masquerade。如果需要隐藏 Pod IP 或网络策略要求，<code>KUBE-SEP</code> 链中可能已包含 <code>KUBE-MARK-MASQ</code> 跳转，此时会执行 Masquerade。</li>
<li>数据包发送给目标 Pod B。</li>
</ol>
<p><strong>iptables 模式总结:</strong> 利用 Netfilter&#x2F;iptables 在内核中实现了 Service 的转发和负载均衡，性能优于 userspace，但受限于 iptables 自身的可扩展性瓶颈。</p>
<hr>
<h4 id="3-IPVS-模式：面向大规模集群的高性能方案"><a href="#3-IPVS-模式：面向大规模集群的高性能方案" class="headerlink" title="3. IPVS 模式：面向大规模集群的高性能方案"></a>3. IPVS 模式：面向大规模集群的高性能方案</h4><img src="/2025/04/10/kube-proxy/image-20250415103628349.png" srcset="/img/loading.gif" lazyload class="" title="image-20250415103628349">

<p>为了解决 iptables 模式在大规模集群下的性能问题，Kubernetes 引入了 IPVS (IP Virtual Server) 模式。IPVS 是 Linux 内核内置的高性能 L4 负载均衡模块，最初为 LVS (Linux Virtual Server) 项目开发。</p>
<ul>
<li><p><strong>核心优势:</strong></p>
<ul>
<li><strong>高效查找:</strong> IPVS 使用<strong>哈希表</strong>存储 Service (Virtual Server) 到 Endpoint (Real Server) 的映射，查找效率接近 O(1)，不受规则数量影响。</li>
<li><strong>性能优越:</strong> 在大量 Service&#x2F;Endpoint 场景下，转发性能远超 <code>iptables</code>，CPU 消耗和延迟更低。</li>
<li><strong>增量更新:</strong> Service&#x2F;Endpoint 变更时，只需通过 <code>netlink</code> 接口增量更新 IPVS 规则，速度快 (毫秒级)。</li>
<li><strong>丰富调度算法:</strong> IPVS 内核模块原生支持多种成熟的负载均衡算法，为 Service 流量分发提供了更多选择和优化空间，例如可以根据后端 Pod 的连接数进行负载均衡（Least Connection）。</li>
<li><strong>连接保持优化:</strong> 支持更好的会话保持机制。</li>
</ul>
</li>
<li><p><strong>工作原理:</strong></p>
<img src="/2025/04/10/kube-proxy/image-20250415104221941.png" srcset="/img/loading.gif" lazyload class="" title="image-20250415104221941">
<ol>
<li><strong>监听 API Server:</strong> 同 iptables 模式。</li>
<li><strong>调用 IPVS 内核接口:</strong> <code>kube-proxy</code> 通过 <code>netlink</code> 与内核 IPVS 模块通信。</li>
<li><strong>创建 IPVS 规则:</strong><ul>
<li>为每个 Kubernetes Service（目前主要支持 ClusterIP 和 NodePort 类型的 Service），<code>kube-proxy</code> 会在宿主机内核中创建一个对应的 IPVS <strong>虚拟服务器</strong>（Virtual Server）。这个虚拟服务器的地址和端口就是 Service 的 ClusterIP 和 Port（或者 NodePort 的监听地址和端口）。</li>
<li>对于该 Service 关联的每一个健康的 Endpoint（即 Pod 的 IP 和 Port），<code>kube-proxy</code> 会为对应的 IPVS 虚拟服务器添加一个 <strong>真实服务器</strong>（Real Server）。</li>
<li><code>kube-proxy</code> 还会根据 Service 的配置（例如 <code>sessionAffinity</code>）和 <code>kube-proxy</code> 的负载均衡策略（图中显示为 <code>--mode random</code>）来决定如何选择 Endpoint。</li>
</ul>
</li>
<li><strong>创建虚拟接口:</strong> <code>kube-proxy</code> 通常会创建一个虚拟接口 (如 <code>kube-ipvs0</code>) 并将所有 ClusterIP 配置在该接口上，确保发往 ClusterIP 的流量能被本地 IP 栈捕获，进而触发 IPVS 处理。</li>
<li><strong>流量转发:</strong> 当一个数据包到达宿主机，其目标地址和端口匹配某个 IPVS 虚拟服务器时，内核的 IPVS 模块会接管这个数据包。它会根据选定的调度算法，从该虚拟服务器关联的真实服务器列表中选择一个健康的 Pod。然后，IPVS 执行目标网络地址转换（DNAT），将数据包的目标 IP 和端口修改为选定 Pod 的 IP 和端口，并将数据包转发出去。</li>
<li><strong>SNAT&#x2F;Masquerade 仍依赖 iptables:</strong> 需要注意的是，虽然 IPVS 负责主要的负载均衡和 DNAT，但它通常不直接处理源网络地址转换（SNAT）。当 Pod 响应请求时，为了保证响应数据包能够正确返回给原始客户端（而不是直接返回，导致客户端收到一个来自 Pod IP 的非预期响应），通常还是需要进行 SNAT，将源 IP 地址改为节点的 IP 地址。这部分功能，<code>kube-proxy</code> 在 IPVS 模式下，仍然依赖 <code>iptables</code>（通常是 <code>MASQUERADE</code> 规则）来完成。<code>kube-proxy</code> 会配置简单的 <code>iptables</code> 规则来标记需要进行 SNAT 的数据包（例如，对从 IPVS 转发出去的、源 IP 是 Pod IP 但目标 IP 不在本地 Pod CIDR 范围内的包进行标记），然后在 <code>POSTROUTING</code> 链中对这些标记的包执行 <code>MASQUERADE</code>。</li>
</ol>
</li>
<li><p><strong>关键技术实现 (Go 代码片段示意):</strong></p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// pkg/proxy/ipvs/proxier.go (简化示意)</span><br><span class="hljs-comment">// ...</span><br><span class="hljs-comment">// syncProxyRules() -&gt; syncService()</span><br><span class="hljs-comment">// ...</span><br><span class="hljs-comment">// Create or Update IPVS Service (Virtual Server)</span><br>service := &amp;ipvs.Service&#123;<br>    Address:  net.ParseIP(clusterIP),<br>    Port:     <span class="hljs-type">uint16</span>(port),<br>    Protocol: ipvs.Protocol(protocol),<br>    Scheduler: scheduler, <span class="hljs-comment">// e.g., &quot;rr&quot;, &quot;lc&quot;</span><br>    Flags:    serviceFlags,<br>    Timeout:  timeout,<br>&#125;<br><span class="hljs-keyword">if</span> err := p.ipvs.AddService(service); err != <span class="hljs-literal">nil</span> &#123; ... &#125;<br><br><span class="hljs-comment">// ...</span><br><span class="hljs-comment">// syncEndpoint()</span><br><span class="hljs-comment">// ...</span><br><span class="hljs-comment">// Add IPVS Destination (Real Server)</span><br>dest := &amp;ipvs.Destination&#123;<br>    Address: net.ParseIP(endpointIP),<br>    Port:    <span class="hljs-type">uint16</span>(targetPort),<br>    Weight:  <span class="hljs-number">1</span>, <span class="hljs-comment">// Or based on annotations</span><br>    Flags:   destinationFlags,<br>&#125;<br><span class="hljs-keyword">if</span> err := p.ipvs.AddDestination(service, dest); err != <span class="hljs-literal">nil</span> &#123; ... &#125;<br><span class="hljs-comment">// ...</span><br><span class="hljs-comment">// Setup iptables rules for masquerading, hairpin, etc.</span><br>p.syncMasqueradeMarkRule()<br>p.syncHairpinIptablesRules()<br><span class="hljs-comment">// ...</span><br></code></pre></td></tr></table></figure></li>
</ul>
<p><strong>IPVS 模式总结:</strong> 通过利用内核 IPVS 模块，显著提升了大规模集群下的服务转发性能和可扩展性，是当前生产环境推荐的选择。但仍需 <code>iptables</code> 配合完成 SNAT 等功能。</p>
<h3 id="生产环境选型与实践建议"><a href="#生产环境选型与实践建议" class="headerlink" title="生产环境选型与实践建议"></a>生产环境选型与实践建议</h3><h4 id="性能基准对比-示意"><a href="#性能基准对比-示意" class="headerlink" title="性能基准对比 (示意)"></a>性能基准对比 (示意)</h4><table>
<thead>
<tr>
<th>模式</th>
<th>万规则更新时延</th>
<th>并发连接能力</th>
<th>CPU 消耗 (高负载)</th>
<th>内存占用 (规则)</th>
</tr>
</thead>
<tbody><tr>
<td>Userspace</td>
<td>N&#x2F;A</td>
<td>~1k</td>
<td>非常高</td>
<td>高</td>
</tr>
<tr>
<td>Iptables</td>
<td>分钟级</td>
<td>~10k-50k</td>
<td>中高</td>
<td>高 (规则+conntrack)</td>
</tr>
<tr>
<td>IPVS</td>
<td>毫秒级</td>
<td>100k+</td>
<td>低</td>
<td>低 (规则) + 中 (conntrack)</td>
</tr>
</tbody></table>
<h4 id="实践建议"><a href="#实践建议" class="headerlink" title="实践建议"></a>实践建议</h4><ol>
<li><strong>模式选择:</strong> 对于新集群或有性能需求的集群，<strong>强烈推荐使用 IPVS 模式</strong>。如果内核版本过低或有特殊原因无法使用 IPVS，iptables 模式仍是稳定选择。</li>
<li><strong>内核要求 (IPVS):</strong> 确保 Linux 内核版本 ≥ 4.1 (推荐 4.19+ 以获得更好性能和特性)，并已加载 <code>ip_vs</code>, <code>ip_vs_rr</code>, <code>ip_vs_wrr</code>, <code>ip_vs_sh</code>, <code>nf_conntrack</code> (或 <code>nf_conntrack_ipv4</code>) 等必要内核模块。</li>
<li><strong>切换模式:</strong><ul>
<li>编辑 <code>kube-proxy</code> 的 ConfigMap (<code>kubectl edit configmap kube-proxy -n kube-system</code>)。</li>
<li>修改 <code>mode</code> 字段为 <code>&quot;ipvs&quot;</code>。</li>
<li>重启 <code>kube-proxy</code> Pods (<code>kubectl delete pod -l k8s-app=kube-proxy -n kube-system</code>)。</li>
<li>验证 IPVS 规则: <code>sudo ipvsadm -Ln</code>。</li>
</ul>
</li>
<li><strong>调度算法选择 (IPVS):</strong><ul>
<li>默认 <code>rr</code> (Round Robin): 简单轮询，适用于无状态服务。</li>
<li><code>lc</code> (Least Connection): 将请求发往当前活动连接数最少的 Pod，适用于长连接或连接数不均的服务。</li>
<li><code>sh</code> (Source Hashing): 基于源 IP 哈希，可实现简单的会话保持 (同一客户端总是访问同一 Pod)，适用于需要会话保持的有状态服务。</li>
</ul>
</li>
<li><strong>配合 CNI:</strong> 确保使用的 CNI 插件与所选 <code>kube-proxy</code> 模式兼容。例如，某些 CNI 可能需要特定配置才能与 IPVS 协同工作。</li>
<li><strong>监控:</strong> 关注关键指标：<ul>
<li>iptables 模式: <code>iptables</code> 规则数量、<code>iptables-restore</code> 耗时、<code>conntrack</code> 表项数量 (<code>conntrack -L | wc -l</code>)、CPU 使用率。</li>
<li>IPVS 模式: <code>ipvsadm -Ln --stats</code> 查看连接数和速率、<code>ipvs_sync_daemon</code> 相关指标 (如果使用)、<code>conntrack</code> 表项数量、CPU 使用率。</li>
<li>通用: 网络延迟、丢包率、<code>kube-proxy</code> 自身资源消耗和错误日志。</li>
</ul>
</li>
<li><strong>SNAT 端口耗尽:</strong> 无论哪种模式，如果大量 Pod 需要访问集群外部，都可能遇到 SNAT 源端口耗尽的问题。考虑使用 CNI 的 IPAM 功能分配更多 IP、配置 Egress Gateway 或使用 IPv6。</li>
</ol>
<h3 id="架构演进启示"><a href="#架构演进启示" class="headerlink" title="架构演进启示"></a>架构演进启示</h3><p><code>kube-proxy</code> 从 userspace 到 iptables 再到 IPVS 的演进，清晰地展示了 Kubernetes 网络模型在性能、可扩展性和生产可用性方面的持续优化。IPVS 模式通过利用成熟的内核负载均衡技术，有效解决了大规模微服务场景下的 L4 负载均衡挑战。未来，基于 eBPF 的方案 (如 Cilium 的 Service 实现) 提供了绕过 <code>kube-proxy</code> 和 <code>iptables/IPVS</code> 的可能性，有望带来进一步的性能提升和灵活性，但这仍在发展中，IPVS 模式是当前广泛验证的生产级选择。</p>
<hr>
<h2 id="CoreDNS：集群服务发现的基石"><a href="#CoreDNS：集群服务发现的基石" class="headerlink" title="CoreDNS：集群服务发现的基石"></a>CoreDNS：集群服务发现的基石</h2><p>虽然 <code>kube-proxy</code> 解决了将 Service IP 流量转发到后端 Pod 的问题，但还有一个关键问题：<strong>应用程序如何知道要访问哪个 Service IP？</strong> 特别是当 Service 重建导致 ClusterIP 变化时。</p>
<p>答案是 <strong>DNS</strong>。Kubernetes 集群内部署了 DNS 服务，为 Service 和 Pod 自动创建 DNS 记录。应用程序只需要使用<strong>稳定、可读的服务名称</strong>，DNS 服务就能将其解析为<strong>当前有效的 ClusterIP 或 Pod IP</strong>。</p>
<p><code>CoreDNS</code> 是 Kubernetes 当前默认且推荐的集群 DNS 服务器。它是一个灵活、可扩展、基于插件的 DNS 服务器，使用 Go 语言编写。</p>
<h3 id="CoreDNS-的核心职责与架构"><a href="#CoreDNS-的核心职责与架构" class="headerlink" title="CoreDNS 的核心职责与架构"></a>CoreDNS 的核心职责与架构</h3><ul>
<li><strong>核心职责:</strong> 响应集群内部对 Service 和 Pod 名称的 DNS 查询请求，将其解析为对应的 IP 地址。</li>
<li><strong>部署形式:</strong> 通常作为 Deployment 部署在 <code>kube-system</code> 命名空间，并通过一个名为 <code>kube-dns</code> 的 Service (ClusterIP 类型) 暴露给集群内的其他 Pod。</li>
<li><strong>工作模式 (控制器模式):</strong> CoreDNS (通过其 <code>kubernetes</code> 插件) 扮演着 Kubernetes 控制器的角色。它<strong>监听 (Watch)</strong> API Server 上的 <code>Service</code> 和 <code>EndpointSlice</code> (或 <code>Endpoints</code>) 资源的变化。</li>
<li><strong>内存态 DNS:</strong> 当资源发生变化时，<code>kubernetes</code> 插件会<strong>实时更新其内存中的 DNS 记录</strong>。查询时直接从内存查找，速度快，无需读写磁盘 Zone 文件。</li>
<li><strong>插件化架构:</strong> CoreDNS 的核心非常轻量，功能由插件链提供。常用插件包括：<ul>
<li><code>kubernetes</code>: 核心插件，与 K8s API 交互，解析集群内部域名。</li>
<li><code>cache</code>: 缓存 DNS 记录，提高性能，降低上游负载。</li>
<li><code>forward</code>: 将无法本地解析的查询 (如外部域名) 转发给上游 DNS 服务器。</li>
<li><code>prometheus</code>: 暴露监控指标。</li>
<li><code>health</code>, <code>ready</code>: 提供健康检查和就绪检查端点。</li>
<li><code>reload</code>: 支持配置热加载。</li>
<li><code>loop</code>: 检测并阻止 DNS 转发循环。</li>
</ul>
</li>
</ul>
<h3 id="CoreDNS-工作原理解析"><a href="#CoreDNS-工作原理解析" class="headerlink" title="CoreDNS 工作原理解析"></a>CoreDNS 工作原理解析</h3><img src="/2025/04/10/kube-proxy/image-20250415105305996.png" srcset="/img/loading.gif" lazyload class="" title="image-20250415105305996">

<ol>
<li><strong>Pod 发起查询:</strong> Pod 内应用需要访问 <code>my-svc</code> 服务 -&gt; 调用 DNS 解析库 (如 glibc <code>getaddrinfo</code>) -&gt; 根据 <code>/etc/resolv.conf</code> 配置，向 <code>kube-dns</code> Service 的 ClusterIP (即 CoreDNS) 发送 DNS 查询请求 (e.g., 查询 <code>my-svc.my-namespace.svc.cluster.local</code> 的 A 记录)。</li>
<li><strong>CoreDNS 处理:</strong><ul>
<li>请求到达 CoreDNS Pod。</li>
<li>CoreDNS 根据 <code>Corefile</code> 配置，将请求交给插件链处理。</li>
<li><code>kubernetes</code> 插件检查域名是否匹配其负责的集群域 (e.g., <code>cluster.local</code>)。</li>
<li><strong>匹配集群域:</strong><ul>
<li>解析出 Service 名称 (<code>my-svc</code>) 和 Namespace (<code>my-namespace</code>)。</li>
<li>在内存缓存中查找对应的 Service 信息。</li>
<li><strong>根据 Service 类型返回记录:</strong><ul>
<li><strong>普通 Service (ClusterIP&#x2F;NodePort&#x2F;LoadBalancer):</strong> 返回 Service 的 <strong>ClusterIP</strong> 作为 A&#x2F;AAAA 记录。</li>
<li><strong>Headless Service (<code>clusterIP: None</code>):</strong> 返回所有**就绪 (Ready)**状态的后端 Pod 的 <strong>IP 地址列表</strong> 作为多个 A&#x2F;AAAA 记录。</li>
<li><strong>ExternalName Service:</strong> 返回一个 <strong>CNAME 记录</strong>，指向 <code>.spec.externalName</code> 定义的外部域名。</li>
</ul>
</li>
<li>(可选) 返回 SRV 记录 (用于命名端口) 或 PTR 记录 (用于反向查询)。</li>
</ul>
</li>
<li><strong>不匹配集群域 (e.g., <code>www.google.com</code>):</strong><ul>
<li><code>kubernetes</code> 插件配置了 <code>fallthrough</code>，将请求传递给下一个插件。</li>
<li><code>cache</code> 插件检查缓存。</li>
<li><code>forward</code> 插件将请求转发给上游 DNS 服务器 (通常是 Node 的 DNS 或公共 DNS)。</li>
</ul>
</li>
<li><strong>返回响应:</strong> CoreDNS 将解析结果 (或错误) 返回给 Pod。</li>
</ul>
</li>
</ol>
<h3 id="Pod-的-DNS-配置-dnsPolicy-etc-resolv-conf-dnsConfig"><a href="#Pod-的-DNS-配置-dnsPolicy-etc-resolv-conf-dnsConfig" class="headerlink" title="Pod 的 DNS 配置 (dnsPolicy, /etc/resolv.conf, dnsConfig)"></a>Pod 的 DNS 配置 (<code>dnsPolicy</code>, <code>/etc/resolv.conf</code>, <code>dnsConfig</code>)</h3><p><code>kubelet</code> 负责为每个 Pod 配置 DNS 解析环境。</p>
<ul>
<li><p><strong><code>dnsPolicy</code>:</strong> Pod Spec 中的字段，决定 DNS 配置策略。</p>
<ul>
<li><strong><code>ClusterFirst</code> (默认):</strong><ul>
<li>Pod 内 <code>/etc/resolv.conf</code> 由 Kubelet 生成。</li>
<li><code>nameserver</code> 指向 CoreDNS Service IP。</li>
<li><code>search</code> 包含 Pod 命名空间、svc 域、集群域以及 Node 的搜索域。</li>
<li>查询短名称 (点数 &lt; <code>ndots</code>) 时，会优先尝试附加 <code>search</code> 域进行解析。</li>
<li>无法解析的查询会通过 CoreDNS 转发给上游 DNS。</li>
</ul>
</li>
<li><strong><code>Default</code>:</strong> Pod 直接继承 Node 节点的 <code>/etc/resolv.conf</code> 配置。Pod 将无法解析集群内部 Service 名称，除非 Node 的 DNS 配置了指向 CoreDNS。</li>
<li><strong><code>None</code>:</strong> Kubelet <strong>不管理</strong> Pod 的 <code>/etc/resolv.conf</code>。Pod 使用镜像自带的 <code>/etc/resolv.conf</code>。通常需要配合 <code>dnsConfig</code> 字段使用。</li>
<li><strong><code>ClusterFirstWithHostNet</code>:</strong> 用于 <code>hostNetwork: true</code> 的 Pod，行为类似 <code>ClusterFirst</code>，但会考虑宿主机网络环境。</li>
</ul>
</li>
<li><p><strong><code>/etc/resolv.conf</code> (由 Kubelet 生成示例 - <code>ClusterFirst</code>):</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># search &lt;namespace&gt;.svc.&lt;cluster.local&gt; svc.&lt;cluster.local&gt; &lt;cluster.local&gt; [node&#x27;s search domains]</span><br>search default.svc.cluster.local svc.cluster.local cluster.local mycorp.com<br><span class="hljs-comment"># nameserver 指向 CoreDNS Service IP</span><br>nameserver 10.96.0.10<br><span class="hljs-comment"># options ndots:5 (关键选项)</span><br>options ndots:5 <span class="hljs-built_in">timeout</span>:1 attempts:2<br></code></pre></td></tr></table></figure>
<ul>
<li><code>search</code>: 域名搜索路径。查询 <code>my-svc</code> 时会尝试 <code>my-svc.default.svc.cluster.local</code>, <code>my-svc.svc.cluster.local</code>, …</li>
<li><code>nameserver</code>: DNS 服务器地址。</li>
<li><code>options ndots:5</code>: 如果查询的域名包含的点 <code>&lt; 5</code>，优先尝试附加 <code>search</code> 域；如果 <code>&gt;= 5</code>，则视为 FQDN 直接查询。</li>
</ul>
</li>
<li><p><strong><code>dnsConfig</code>:</strong> Pod Spec 中的字段，<strong>仅在 <code>dnsPolicy: &quot;None&quot;</code> 时或需要覆盖默认策略时使用</strong>。允许用户显式指定 <code>nameservers</code>, <code>searches</code>, <code>options</code>。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">dnsPolicy:</span> <span class="hljs-string">&quot;None&quot;</span> <span class="hljs-comment"># Kubelet 不自动配置</span><br>  <span class="hljs-attr">dnsConfig:</span>        <span class="hljs-comment"># 用户提供完整配置</span><br>    <span class="hljs-attr">nameservers:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;1.1.1.1&quot;</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;8.8.8.8&quot;</span><br>    <span class="hljs-attr">searches:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-string">my.custom.domain</span><br>    <span class="hljs-attr">options:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ndots</span><br>        <span class="hljs-attr">value:</span> <span class="hljs-string">&quot;1&quot;</span><br></code></pre></td></tr></table></figure>
<p><strong>注意:</strong> 如果使用 <code>dnsPolicy: None</code> 但仍需解析集群服务，必须在 <code>dnsConfig.nameservers</code> 中手动加入 CoreDNS 的 IP，并在 <code>dnsConfig.searches</code> 中加入集群搜索域。</p>
</li>
<li><p><strong>环境变量方式 (已不推荐):</strong> 早期 K8s 会将 Service 信息注入环境变量 (<code>&lt;SERVICE_NAME&gt;_SERVICE_HOST</code>, <code>&lt;SERVICE_NAME&gt;_SERVICE_PORT</code>)。这种方式是静态的，无法感知 Service 或 IP 变化，且污染环境，<strong>强烈建议使用 DNS 进行服务发现</strong>。</p>
</li>
</ul>
<h3 id="CoreDNS-配置示例-Corefile"><a href="#CoreDNS-配置示例-Corefile" class="headerlink" title="CoreDNS 配置示例 (Corefile)"></a>CoreDNS 配置示例 (<code>Corefile</code>)</h3><p>CoreDNS 的行为由 <code>Corefile</code> 控制，通常存储在 <code>coredns</code> ConfigMap 中。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs coredns"># Corefile 示例 (典型配置)<br>.:53 &#123;                                                  # 监听所有域 (.) 在 UDP/TCP 的 53 端口<br>    errors                                              # 启用错误日志<br>    health &#123;                                            # 健康检查 /health<br>       lameduck 5s<br>    &#125;<br>    ready                                               # 就绪检查 /ready<br>    kubernetes cluster.local in-addr.arpa ip6.arpa &#123;    # Kubernetes 插件<br>       pods insecure                                    # 解析 Pod A 记录<br>       upstream                                         # 解析 ExternalName Service 时使用上游<br>       fallthrough in-addr.arpa ip6.arpa                # 非 K8s 域查询传递给下一插件<br>    &#125;<br>    prometheus :9153                                    # 暴露 Prometheus 指标 /metrics<br>    forward . /etc/resolv.conf &#123;                        # 转发插件，使用 Node 的 DNS 作为上游<br>       max_concurrent 1000<br>    &#125;<br>    cache 30                                            # 缓存插件，TTL 30 秒<br>    loop                                                # 循环检测<br>    reload 5s                                           # 5 秒检查配置更新并热加载<br>    loadbalance                                         # 对 A/AAAA/MX 记录轮询负载均衡<br>&#125;<br></code></pre></td></tr></table></figure>
<p><strong>配置解读:</strong></p>
<ul>
<li><code>.:53 &#123; ... &#125;</code>: 定义了一个处理所有 DNS 查询（<code>.</code> 代表根区域），监听标准 DNS 端口 53。</li>
<li><code>errors</code>: 捕获并记录处理过程中的错误。</li>
<li><code>health &#123; lameduck 5s &#125;</code>: 在 <code>:8080/health</code> (默认端口) 提供健康检查接口。<code>lameduck</code> 确保在 CoreDNS 准备关闭时，会先保持 5 秒的运行状态以完成正在处理的请求，然后才报告不健康。</li>
<li><code>ready</code>: 在 <code>:8181/ready</code> (默认端口) 提供就绪检查接口，只有当 CoreDNS 完全准备好处理请求时才返回成功。</li>
<li><code>kubernetes cluster.local in-addr.arpa ip6.arpa &#123; ... &#125;</code>: 这是核心。<ul>
<li><code>cluster.local</code>: 指定 CoreDNS 负责解析的 Kubernetes 集群域。所有 <code>*.cluster.local</code> 的查询将由这个插件处理。</li>
<li><code>in-addr.arpa</code> 和 <code>ip6.arpa</code>: 使插件能够处理 PTR 反向查询，将 IP 地址解析回对应的 Kubernetes 服务或 Pod 名称。</li>
<li><code>pods insecure</code>: 允许解析 Pod 的 IP 地址到 DNS 名称的 A 记录。<code>insecure</code> 表示即使 Pod 未处于 Ready 状态也可能被解析（通常还支持 <code>verified</code> 和 <code>disabled</code> 选项）。</li>
<li><code>upstream</code>: 对于指向外部名称 (ExternalName) 的 Service，如果需要解析，会使用上游解析器。</li>
<li><code>fallthrough in-addr.arpa ip6.arpa</code>: 如果查询的名称不匹配 <code>cluster.local</code> 域，或者不匹配反向查询域，则将请求传递给插件链中的下一个插件（在这里是 <code>forward</code>）。</li>
</ul>
</li>
<li><code>prometheus :9153</code>: 在 9153 端口暴露 <code>/metrics</code> 端点，供 Prometheus 抓取监控数据。</li>
<li><code>forward . /etc/resolv.conf</code>: 对于不由 <code>kubernetes</code> 插件处理的查询（由于 <code>fallthrough</code>），使用此插件转发。<code>.</code> 表示转发所有域。<code>/etc/resolv.conf</code> 表示使用 CoreDNS Pod 所在 Node 的 DNS 配置作为上游解析器。也可以直接指定 IP 地址，如 <code>forward . 8.8.8.8 1.1.1.1</code>。</li>
<li><code>cache 30</code>: 对 DNS 记录进行缓存，默认缓存时间 30 秒。这能显著提高性能并减少对上游 DNS 的依赖。</li>
<li><code>loop</code>: 防止 DNS 查询在转发过程中形成循环。</li>
<li><code>reload 5s</code>: CoreDNS 会定期检查 <code>Corefile</code> 的修改时间。如果文件发生变化，它会在不中断服务的情况下平滑地重新加载配置。</li>
<li><code>loadbalance</code>: 对 DNS 响应中的多个 A&#x2F;AAAA&#x2F;MX 记录进行轮询分发，提供简单的客户端负载均衡。</li>
</ul>
<p>通过这套机制，CoreDNS 为 Kubernetes 集群提供了一个健壮、高效且可配置的内部 DNS 解析服务，完美解决了因 Service IP 动态变化而导致的服务发现难题，是构建稳定可靠微服务架构的关键基础设施之一。理解 CoreDNS 的工作原理和配置方式，对于深入掌握 Kubernetes 网络和服务发现至关重要。</p>
<h3 id="CoreDNS-的核心架构：内存态-DNS-与控制器模式"><a href="#CoreDNS-的核心架构：内存态-DNS-与控制器模式" class="headerlink" title="CoreDNS 的核心架构：内存态 DNS 与控制器模式"></a>CoreDNS 的核心架构：内存态 DNS 与控制器模式</h3><p>你提到的 “内存态 DNS” 和 “控制器” 是理解 CoreDNS 工作方式的关键。</p>
<p><strong>内存态 DNS</strong>：CoreDNS 本身可以配置为权威 DNS 服务器或转发器&#x2F;缓存。在 Kubernetes 环境中，通过其 <code>kubernetes</code> 插件，CoreDNS 能够动态地维护集群内部服务的 DNS 记录。这些记录主要存储在<strong>内存</strong>中，以便快速响应查询请求。这意味着 CoreDNS 不需要依赖传统的、基于文件的区域（Zone）文件来存储 K8s 服务的记录。这种内存态存储对于应对 K8s 中 Service 和 Pod 的频繁变化至关重要，可以提供低延迟的查询响应。当然，CoreDNS 也包含 <code>cache</code> 插件，用于缓存内部和外部查询结果，进一步提高性能并减少对上游 DNS 服务器的负载。</p>
<p><strong>控制器模式</strong>：这是 CoreDNS 与 Kubernetes 集成运作的核心机制。CoreDNS (通过其 <code>kubernetes</code> 插件) 扮演着一个标准的 <strong>Kubernetes 控制器</strong>角色。它通过 Kubernetes API <strong>监听（Watch）</strong> <code>Service</code> 和 <code>EndpointSlice</code>（或者旧版本的 <code>Endpoints</code>）资源的变化。</p>
<ul>
<li>当一个新的 <code>Service</code> 被创建时，控制器会收到通知，并根据 Service 的类型和配置，在内存中生成相应的 DNS 记录（A、AAAA、SRV、PTR 等）。</li>
<li>当一个 <code>Service</code> 关联的 Pod 实例发生变化（例如，Pod 启动就绪、Pod 被删除、Pod IP 改变）时，对应的 <code>EndpointSlice</code> 会更新。控制器监听到 <code>EndpointSlice</code> 的变化，并相应地更新内存中的 DNS 记录（特别是对于 Headless Service 的 A&#x2F;AAAA 记录）。</li>
<li>当 <code>Service</code> 被删除时，控制器也会移除相关的 DNS 记录。</li>
</ul>
<p>这个过程是<strong>持续、自动</strong>的。CoreDNS 不需要手动配置每个服务的 DNS 条目，它通过与 Kubernetes API 的实时交互，动态地维护了一份准确反映集群当前状态的 DNS 视图。这种机制与其他的 Kubernetes 控制器（如 Deployment Controller、ReplicaSet Controller）的工作原理类似，都是通过 <strong>Watch API -&gt; 对比期望状态与实际状态 -&gt; 执行调整动作</strong> 的循环来确保系统达到期望的状态。</p>
<p>从 Linux 内核或网络角度看，当一个 Pod 内的进程发起 DNS 查询时（通常是调用 libc 的 <code>gethostbyname</code> 或 <code>getaddrinfo</code> 等函数），最终会根据 <code>/etc/resolv.conf</code> 的配置，将 UDP 或 TCP 请求发送到 CoreDNS Pod 的 IP 地址和 53 端口。CoreDNS Pod 接收到请求后，其内部的插件链开始处理。<code>kubernetes</code> 插件会检查请求的域名是否匹配集群内部的域（如 <code>cluster.local</code>），如果匹配，则在内存数据中查找相应的记录并返回；如果不匹配，则可能由 <code>forward</code> 或 <code>proxy</code> 插件将请求转发给上游 DNS 服务器（例如节点宿主机的 DNS 或公共 DNS）。</p>
<h3 id="不同类型服务的-DNS-记录详解"><a href="#不同类型服务的-DNS-记录详解" class="headerlink" title="不同类型服务的 DNS 记录详解"></a>不同类型服务的 DNS 记录详解</h3><p>CoreDNS 如何为不同类型的 Kubernetes Service 生成 DNS 记录是服务发现的核心细节。</p>
<h4 id="普通-Service-ClusterIP-NodePort-LoadBalancer"><a href="#普通-Service-ClusterIP-NodePort-LoadBalancer" class="headerlink" title="普通 Service (ClusterIP, NodePort, LoadBalancer)"></a>普通 Service (ClusterIP, NodePort, LoadBalancer)</h4><p>这类 Service 都有一个由 Kubernetes API Server 分配的、稳定的虚拟 IP，即 <strong>ClusterIP</strong>。这个 IP 并不是绑定在某个具体的网络设备上，而是由 kube-proxy（或等效的网络组件如 Cilium eBPF）在数据平面上实现负载均衡和转发。</p>
<ul>
<li><strong>A&#x2F;AAAA 记录</strong>：CoreDNS 会为这类 Service 创建一个 <strong>FQDN（完全限定域名）</strong>，格式通常是 <strong><code>&lt;service-name&gt;.&lt;namespace-name&gt;.svc.&lt;cluster-domain&gt;</code></strong> (例如 <code>my-service.default.svc.cluster.local</code>)。这个 FQDN 解析为一个 A 记录（IPv4）或 AAAA 记录（IPv6），其值就是该 Service 的 <strong>ClusterIP</strong>。客户端 Pod 查询这个域名时，会得到 ClusterIP，然后发往这个 IP 的请求会被 kube-proxy 拦截并转发到该 Service 背后某个健康的 Pod IP 上。</li>
<li><strong>SRV 记录</strong>：如果 Service 定义了端口（Ports），CoreDNS 还会为每个命名端口（Named Port）创建 SRV 记录。格式通常是 <strong><code>_&lt;port-name&gt;._&lt;protocol&gt;.&lt;service-name&gt;.&lt;namespace-name&gt;.svc.&lt;cluster-domain&gt;</code></strong> (例如 <code>_http._tcp.my-service.default.svc.cluster.local</code>)。SRV 记录包含了端口号、优先级和权重，允许客户端发现服务提供的具体端口信息，而不仅仅是 IP 地址。这对于需要知道特定服务端口的应用（如 gRPC、LDAP）非常有用。</li>
<li><strong>PTR 记录</strong>：用于反向 DNS 查询，将 IP 地址解析回对应的 Kubernetes 服务或 Pod 名称。</li>
</ul>
<h4 id="Headless-Service"><a href="#Headless-Service" class="headerlink" title="Headless Service"></a>Headless Service</h4><p><strong>直接指向所有 Ready 的 Pod IP</strong>。</p>
<p>当 Service 的 <code>.spec.clusterIP</code> 字段被显式设置为 <code>None</code> 时，就创建了一个 <strong>Headless Service</strong>。顾名思义，它<strong>没有 ClusterIP</strong>。API Server 不会为其分配虚拟 IP，kube-proxy 也不会处理到这个 Service 的流量。</p>
<ul>
<li><strong>A&#x2F;AAAA 记录 (多条)</strong>：对于 Headless Service，当查询其 FQDN <strong><code>&lt;service-name&gt;.&lt;namespace-name&gt;.svc.&lt;cluster-domain&gt;</code></strong> 时，CoreDNS <strong>不会返回 ClusterIP</strong>（因为它不存在）。相反，它会返回多个 A&#x2F;AAAA 记录，每个记录对应一个**当前就绪（Ready）**状态的、属于该 Service 的 <strong>Pod 的 IP 地址</strong>。**这意味着客户端直接解析到后端 Pod 的真实 IP 列表。**这对于需要直接与 Pod 通信的场景（如 StatefulSet 的 Pod 间发现）或者希望自己实现客户端负载均衡策略的场景非常有用。</li>
<li><strong>Pod FQDN 记录</strong>：对于与 Headless Service 关联的每个 Pod（特别是那些由 StatefulSet 创建的、具有稳定网络标识符的 Pod），CoreDNS 还会创建一个特定的 FQDN，格式通常是 <strong><code>&lt;pod-hostname&gt;.&lt;service-name&gt;.&lt;namespace-name&gt;.svc.&lt;cluster-domain&gt;</code></strong> (如果 Service 指定了 <code>subdomain</code> 字段，格式会是 <code>&lt;pod-hostname&gt;.&lt;subdomain&gt;.&lt;namespace-name&gt;.svc.&lt;cluster-domain&gt;</code>) 或者对于普通 Pod 可能简化为类似 <strong><code>&lt;pod-ip-dashed&gt;.&lt;namespace-name&gt;.pod.&lt;cluster-domain&gt;</code></strong>。这个记录直接解析为该 <strong>Pod 的 IP 地址</strong>。这允许直接寻址到某个具体的 Pod 实例。</li>
</ul>
<h4 id="ExternalName-Service"><a href="#ExternalName-Service" class="headerlink" title="ExternalName Service"></a>ExternalName Service</h4><p>这种类型的 Service 比较特殊，它<strong>不涉及 Pod 选择器或 IP 地址分配</strong>。它的目的是在集群内部为<strong>外部的一个 DNS 域名</strong>创建一个别名。</p>
<ul>
<li><strong>CNAME 记录</strong>：CoreDNS 会为 ExternalName Service 的 FQDN <strong><code>&lt;service-name&gt;.&lt;namespace-name&gt;.svc.&lt;cluster-domain&gt;</code></strong> 创建一个 <strong>CNAME 记录</strong>。这个 CNAME 记录的值是 Service 定义中 <code>.spec.externalName</code> 字段指定的外部域名。当集群内部的 Pod 查询这个 Service FQDN 时，CoreDNS 会返回 CNAME 记录，指示客户端应该去查询 <code>externalName</code> 指定的那个域名。这相当于在集群 DNS 内部做了一个“符号链接”或“别名”，将内部服务名映射到集群外部的某个实际服务上，而无需关心外部服务的 IP 是否变化。</li>
</ul>
<h3 id="Pod-的-DNS-配置-dnsPolicy-etc-resolv-conf-dnsConfig-1"><a href="#Pod-的-DNS-配置-dnsPolicy-etc-resolv-conf-dnsConfig-1" class="headerlink" title="Pod 的 DNS 配置 (dnsPolicy, /etc/resolv.conf, dnsConfig)"></a>Pod 的 DNS 配置 (<code>dnsPolicy</code>, <code>/etc/resolv.conf</code>, <code>dnsConfig</code>)</h3><p>Kubernetes 通过 <code>kubelet</code> 组件管理每个 Pod 的 DNS 配置，确保 Pod 能够正确地使用 CoreDNS 进行服务发现。</p>
<ul>
<li><p><strong><code>dnsPolicy</code>:</strong> Pod Spec 中的字段，决定 DNS 配置策略。</p>
<ul>
<li><strong><code>ClusterFirst</code> (默认):</strong><ul>
<li>Pod 内 <code>/etc/resolv.conf</code> 由 Kubelet 生成。</li>
<li><code>nameserver</code> 指向 CoreDNS Service IP。</li>
<li><code>search</code> 包含 Pod 命名空间、svc 域、集群域以及 Node 的搜索域。</li>
<li>查询短名称 (点数 &lt; <code>ndots</code>) 时，会优先尝试附加 <code>search</code> 域进行解析。</li>
<li>无法解析的查询会通过 CoreDNS 转发给上游 DNS。</li>
</ul>
</li>
<li><strong><code>Default</code>:</strong> Pod 直接继承 Node 节点的 <code>/etc/resolv.conf</code> 配置。Pod 将无法解析集群内部 Service 名称，除非 Node 的 DNS 配置了指向 CoreDNS。</li>
<li><strong><code>None</code>:</strong> Kubelet <strong>不管理</strong> Pod 的 <code>/etc/resolv.conf</code>。Pod 使用镜像自带的 <code>/etc/resolv.conf</code>。通常需要配合 <code>dnsConfig</code> 字段使用。</li>
<li><strong><code>ClusterFirstWithHostNet</code>:</strong> 用于 <code>hostNetwork: true</code> 的 Pod，行为类似 <code>ClusterFirst</code>，但会考虑宿主机网络环境。</li>
</ul>
</li>
<li><p><strong><code>/etc/resolv.conf</code> (由 Kubelet 生成示例 - <code>ClusterFirst</code>):</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># search &lt;namespace&gt;.svc.&lt;cluster.local&gt; svc.&lt;cluster.local&gt; &lt;cluster.local&gt; [node&#x27;s search domains]</span><br>search default.svc.cluster.local svc.cluster.local cluster.local mycorp.com<br><span class="hljs-comment"># nameserver 指向 CoreDNS Service IP</span><br>nameserver 10.96.0.10<br><span class="hljs-comment"># options ndots:5 (关键选项)</span><br>options ndots:5 <span class="hljs-built_in">timeout</span>:1 attempts:2<br></code></pre></td></tr></table></figure>
<ul>
<li><code>search</code>: 域名搜索路径。查询 <code>my-svc</code> 时会尝试 <code>my-svc.default.svc.cluster.local</code>, <code>my-svc.svc.cluster.local</code>, …</li>
<li><code>nameserver</code>: DNS 服务器地址。</li>
<li><code>options ndots:5</code>: 如果查询的域名包含的点 <code>&lt; 5</code>，优先尝试附加 <code>search</code> 域；如果 <code>&gt;= 5</code>，则视为 FQDN 直接查询。</li>
</ul>
</li>
<li><p><strong><code>dnsConfig</code>:</strong> Pod Spec 中的字段，<strong>仅在 <code>dnsPolicy: &quot;None&quot;</code> 时或需要覆盖默认策略时使用</strong>。允许用户显式指定 <code>nameservers</code>, <code>searches</code>, <code>options</code>。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">dnsPolicy:</span> <span class="hljs-string">&quot;None&quot;</span> <span class="hljs-comment"># Kubelet 不自动配置</span><br>  <span class="hljs-attr">dnsConfig:</span>        <span class="hljs-comment"># 用户提供完整配置</span><br>    <span class="hljs-attr">nameservers:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;1.1.1.1&quot;</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;8.8.8.8&quot;</span><br>    <span class="hljs-attr">searches:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-string">my.custom.domain</span><br>    <span class="hljs-attr">options:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ndots</span><br>        <span class="hljs-attr">value:</span> <span class="hljs-string">&quot;1&quot;</span><br></code></pre></td></tr></table></figure>
<p><strong>注意:</strong> 如果使用 <code>dnsPolicy: None</code> 但仍需解析集群服务，必须在 <code>dnsConfig.nameservers</code> 中手动加入 CoreDNS 的 IP，并在 <code>dnsConfig.searches</code> 中加入集群搜索域。</p>
</li>
<li><p><strong>环境变量方式 (已不推荐):</strong> 早期 K8s 会将 Service 信息注入环境变量 (<code>&lt;SERVICE_NAME&gt;_SERVICE_HOST</code>, <code>&lt;SERVICE_NAME&gt;_SERVICE_PORT</code>)。这种方式是静态的，无法感知 Service 或 IP 变化，且污染环境，<strong>强烈建议使用 DNS 进行服务发现</strong>。</p>
</li>
</ul>
<h3 id="CoreDNS-配置示例-Corefile-1"><a href="#CoreDNS-配置示例-Corefile-1" class="headerlink" title="CoreDNS 配置示例 (Corefile)"></a>CoreDNS 配置示例 (<code>Corefile</code>)</h3><p>CoreDNS 的行为由 <code>Corefile</code> 控制，通常存储在 <code>coredns</code> ConfigMap 中。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs coredns"># Corefile 示例 (典型配置)<br>.:53 &#123;                                                  # 监听所有域 (.) 在 UDP/TCP 的 53 端口<br>    errors                                              # 启用错误日志<br>    health &#123;                                            # 健康检查 /health<br>       lameduck 5s<br>    &#125;<br>    ready                                               # 就绪检查 /ready<br>    kubernetes cluster.local in-addr.arpa ip6.arpa &#123;    # Kubernetes 插件<br>       pods insecure                                    # 解析 Pod A 记录<br>       upstream                                         # 解析 ExternalName Service 时使用上游<br>       fallthrough in-addr.arpa ip6.arpa                # 非 K8s 域查询传递给下一插件<br>    &#125;<br>    prometheus :9153                                    # 暴露 Prometheus 指标 /metrics<br>    forward . /etc/resolv.conf &#123;                        # 转发插件，使用 Node 的 DNS 作为上游<br>       max_concurrent 1000<br>    &#125;<br>    cache 30                                            # 缓存插件，TTL 30 秒<br>    loop                                                # 循环检测<br>    reload 5s                                           # 5 秒检查配置更新并热加载<br>    loadbalance                                         # 对 A/AAAA/MX 记录轮询负载均衡<br>&#125;<br></code></pre></td></tr></table></figure>
<p><strong>配置解读:</strong></p>
<ul>
<li><code>.:53 &#123; ... &#125;</code>: 定义了一个处理所有 DNS 查询（<code>.</code> 代表根区域），监听标准 DNS 端口 53。</li>
<li><code>errors</code>: 捕获并记录处理过程中的错误。</li>
<li><code>health &#123; lameduck 5s &#125;</code>: 在 <code>:8080/health</code> (默认端口) 提供健康检查接口。<code>lameduck</code> 确保在 CoreDNS 准备关闭时，会先保持 5 秒的运行状态以完成正在处理的请求，然后才报告不健康。</li>
<li><code>ready</code>: 在 <code>:8181/ready</code> (默认端口) 提供就绪检查接口，只有当 CoreDNS 完全准备好处理请求时才返回成功。</li>
<li><code>kubernetes cluster.local in-addr.arpa ip6.arpa &#123; ... &#125;</code>: 这是核心。<ul>
<li><code>cluster.local</code>: 指定 CoreDNS 负责解析的 Kubernetes 集群域。所有 <code>*.cluster.local</code> 的查询将由这个插件处理。</li>
<li><code>in-addr.arpa</code> 和 <code>ip6.arpa</code>: 使插件能够处理 PTR 反向查询，将 IP 地址解析回对应的 Kubernetes 服务或 Pod 名称。</li>
<li><code>pods insecure</code>: 允许解析 Pod 的 IP 地址到 DNS 名称的 A 记录。<code>insecure</code> 表示即使 Pod 未处于 Ready 状态也可能被解析（通常还支持 <code>verified</code> 和 <code>disabled</code> 选项）。</li>
<li><code>upstream</code>: 对于指向外部名称 (ExternalName) 的 Service，如果需要解析，会使用上游解析器。</li>
<li><code>fallthrough in-addr.arpa ip6.arpa</code>: 如果查询的名称不匹配 <code>cluster.local</code> 域，或者不匹配反向查询域，则将请求传递给插件链中的下一个插件（在这里是 <code>forward</code>）。</li>
</ul>
</li>
<li><code>prometheus :9153</code>: 在 9153 端口暴露 <code>/metrics</code> 端点，供 Prometheus 抓取监控数据。</li>
<li><code>forward . /etc/resolv.conf</code>: 对于不由 <code>kubernetes</code> 插件处理的查询（由于 <code>fallthrough</code>），使用此插件转发。<code>.</code> 表示转发所有域。<code>/etc/resolv.conf</code> 表示使用 CoreDNS Pod 所在 Node 的 DNS 配置作为上游解析器。也可以直接指定 IP 地址，如 <code>forward . 8.8.8.8 1.1.1.1</code>。</li>
<li><code>cache 30</code>: 对 DNS 记录进行缓存，默认缓存时间 30 秒。这能显著提高性能并减少对上游 DNS 的依赖。</li>
<li><code>loop</code>: 防止 DNS 查询在转发过程中形成循环。</li>
<li><code>reload 5s</code>: CoreDNS 会定期检查 <code>Corefile</code> 的修改时间。如果文件发生变化，它会在不中断服务的情况下平滑地重新加载配置。</li>
<li><code>loadbalance</code>: 对 DNS 响应中的多个 A&#x2F;AAAA&#x2F;MX 记录进行轮询分发，提供简单的客户端负载均衡。</li>
</ul>
<p>通过这套机制，CoreDNS 为 Kubernetes 集群提供了一个健壮、高效且可配置的内部 DNS 解析服务，完美解决了因 Service IP 动态变化而导致的服务发现难题，是构建稳定可靠微服务架构的关键基础设施之一。理解 CoreDNS 的工作原理和配置方式，对于深入掌握 Kubernetes 网络和服务发现至关重要。</p>
<h3 id="CoreDNS-的核心架构：内存态-DNS-与控制器模式-1"><a href="#CoreDNS-的核心架构：内存态-DNS-与控制器模式-1" class="headerlink" title="CoreDNS 的核心架构：内存态 DNS 与控制器模式"></a>CoreDNS 的核心架构：内存态 DNS 与控制器模式</h3><p>你提到的 “内存态 DNS” 和 “控制器” 是理解 CoreDNS 工作方式的关键。</p>
<p><strong>内存态 DNS</strong>：CoreDNS 本身可以配置为权威 DNS 服务器或转发器&#x2F;缓存。在 Kubernetes 环境中，通过其 <code>kubernetes</code> 插件，CoreDNS 能够动态地维护集群内部服务的 DNS 记录。这些记录主要存储在<strong>内存</strong>中，以便快速响应查询请求。这意味着 CoreDNS 不需要依赖传统的、基于文件的区域（Zone）文件来存储 K8s 服务的记录。这种内存态存储对于应对 K8s 中 Service 和 Pod 的频繁变化至关重要，可以提供低延迟的查询响应。当然，CoreDNS 也包含 <code>cache</code> 插件，用于缓存内部和外部查询结果，进一步提高性能并减少对上游 DNS 服务器的负载。</p>
<p><strong>控制器模式</strong>：这是 CoreDNS 与 Kubernetes 集成运作的核心机制。CoreDNS (通过其 <code>kubernetes</code> 插件) 扮演着一个标准的 <strong>Kubernetes 控制器</strong>角色。它通过 Kubernetes API <strong>监听（Watch）</strong> <code>Service</code> 和 <code>EndpointSlice</code>（或者旧版本的 <code>Endpoints</code>）资源的变化。</p>
<ul>
<li>当一个新的 <code>Service</code> 被创建时，控制器会收到通知，并根据 Service 的类型和配置，在内存中生成相应的 DNS 记录（A、AAAA、SRV、PTR 等）。</li>
<li>当一个 <code>Service</code> 关联的 Pod 实例发生变化（例如，Pod 启动就绪、Pod 被删除、Pod IP 改变）时，对应的 <code>EndpointSlice</code> 会更新。控制器监听到 <code>EndpointSlice</code> 的变化，并相应地更新内存中的 DNS 记录（特别是对于 Headless Service 的 A&#x2F;AAAA 记录）。</li>
<li>当 <code>Service</code> 被删除时，控制器也会移除相关的 DNS 记录。</li>
</ul>
<p>这个过程是<strong>持续、自动</strong>的。CoreDNS 不需要手动配置每个服务的 DNS 条目，它通过与 Kubernetes API 的实时交互，动态地维护了一份准确反映集群当前状态的 DNS 视图。这种机制与其他的 Kubernetes 控制器（如 Deployment Controller、ReplicaSet Controller）的工作原理类似，都是通过 <strong>Watch API -&gt; 对比期望状态与实际状态 -&gt; 执行调整动作</strong> 的循环来确保系统达到期望的状态。</p>
<p>从 Linux 内核或网络角度看，当一个 Pod 内的进程发起 DNS 查询时（通常是调用 libc 的 <code>gethostbyname</code> 或 <code>getaddrinfo</code> 等函数），最终会根据 <code>/etc/resolv.conf</code> 的配置，将 UDP 或 TCP 请求发送到 CoreDNS Pod 的 IP 地址和 53 端口。CoreDNS Pod 接收到请求后，其内部的插件链开始处理。<code>kubernetes</code> 插件会检查请求的域名是否匹配集群内部的域（如 <code>cluster.local</code>），如果匹配，则在内存数据中查找相应的记录并返回；如果不匹配，则可能由 <code>forward</code> 或 <code>proxy</code> 插件将请求转发给上游 DNS 服务器（例如节点宿主机的 DNS 或公共 DNS）。</p>
<h3 id="不同类型服务的-DNS-记录详解-1"><a href="#不同类型服务的-DNS-记录详解-1" class="headerlink" title="不同类型服务的 DNS 记录详解"></a>不同类型服务的 DNS 记录详解</h3><p>CoreDNS 如何为不同类型的 Kubernetes Service 生成 DNS 记录是服务发现的核心细节。</p>
<h4 id="普通-Service-ClusterIP-NodePort-LoadBalancer-1"><a href="#普通-Service-ClusterIP-NodePort-LoadBalancer-1" class="headerlink" title="普通 Service (ClusterIP, NodePort, LoadBalancer)"></a>普通 Service (ClusterIP, NodePort, LoadBalancer)</h4><p>这类 Service 都有一个由 Kubernetes API Server 分配的、稳定的虚拟 IP，即 <strong>ClusterIP</strong>。这个 IP 并不是绑定在某个具体的网络设备上，而是由 kube-proxy（或等效的网络组件如 Cilium eBPF）在数据平面上实现负载均衡和转发。</p>
<ul>
<li><strong>A&#x2F;AAAA 记录</strong>：CoreDNS 会为这类 Service 创建一个 <strong>FQDN（完全限定域名）</strong>，格式通常是 <strong><code>&lt;service-name&gt;.&lt;namespace-name&gt;.svc.&lt;cluster-domain&gt;</code></strong> (例如 <code>my-service.default.svc.cluster.local</code>)。这个 FQDN 解析为一个 A 记录（IPv4）或 AAAA 记录（IPv6），其值就是该 Service 的 <strong>ClusterIP</strong>。客户端 Pod 查询这个域名时，会得到 ClusterIP，然后发往这个 IP 的请求会被 kube-proxy 拦截并转发到该 Service 背后某个健康的 Pod IP 上。</li>
<li><strong>SRV 记录</strong>：如果 Service 定义了端口（Ports），CoreDNS 还会为每个命名端口（Named Port）创建 SRV 记录。格式通常是 <strong><code>_&lt;port-name&gt;._&lt;protocol&gt;.&lt;service-name&gt;.&lt;namespace-name&gt;.svc.&lt;cluster-domain&gt;</code></strong> (例如 <code>_http._tcp.my-service.default.svc.cluster.local</code>)。SRV 记录包含了端口号、优先级和权重，允许客户端发现服务提供的具体端口信息，而不仅仅是 IP 地址。这对于需要知道特定服务端口的应用（如 gRPC、LDAP）非常有用。</li>
<li><strong>PTR 记录</strong>：用于反向 DNS 查询，将 IP 地址解析回对应的 Kubernetes 服务或 Pod 名称。</li>
</ul>
<h4 id="Headless-Service-1"><a href="#Headless-Service-1" class="headerlink" title="Headless Service"></a>Headless Service</h4><p><strong>直接指向所有 Ready 的 Pod IP</strong>。</p>
<p>当 Service 的 <code>.spec.clusterIP</code> 字段被显式设置为 <code>None</code> 时，就创建了一个 <strong>Headless Service</strong>。顾名思义，它<strong>没有 ClusterIP</strong>。API Server 不会为其分配虚拟 IP，kube-proxy 也不会处理到这个 Service 的流量。</p>
<ul>
<li><strong>A&#x2F;AAAA 记录 (多条)</strong>：对于 Headless Service，当查询其 FQDN <strong><code>&lt;service-name&gt;.&lt;namespace-name&gt;.svc.&lt;cluster-domain&gt;</code></strong> 时，CoreDNS <strong>不会返回 ClusterIP</strong>（因为它不存在）。相反，它会返回多个 A&#x2F;AAAA 记录，每个记录对应一个**当前就绪（Ready）**状态的、属于该 Service 的 <strong>Pod 的 IP 地址</strong>。**这意味着客户端直接解析到后端 Pod 的真实 IP 列表。**这对于需要直接与 Pod 通信的场景（如 StatefulSet 的 Pod 间发现）或者希望自己实现客户端负载均衡策略的场景非常有用。</li>
<li><strong>Pod FQDN 记录</strong>：对于与 Headless Service 关联的每个 Pod（特别是那些由 StatefulSet 创建的、具有稳定网络标识符的 Pod），CoreDNS 还会创建一个特定的 FQDN，格式通常是 <strong><code>&lt;pod-hostname&gt;.&lt;service-name&gt;.&lt;namespace-name&gt;.svc.&lt;cluster-domain&gt;</code></strong> (如果 Service 指定了 <code>subdomain</code> 字段，格式会是 <code>&lt;pod-hostname&gt;.&lt;subdomain&gt;.&lt;namespace-name&gt;.svc.&lt;cluster-domain&gt;</code>) 或者对于普通 Pod 可能简化为类似 <strong><code>&lt;pod-ip-dashed&gt;.&lt;namespace-name&gt;.pod.&lt;cluster-domain&gt;</code></strong>。这个记录直接解析为该 <strong>Pod 的 IP 地址</strong>。这允许直接寻址到某个具体的 Pod 实例。</li>
</ul>
<h4 id="ExternalName-Service-1"><a href="#ExternalName-Service-1" class="headerlink" title="ExternalName Service"></a>ExternalName Service</h4><p>这种类型的 Service 比较特殊，它<strong>不涉及 Pod 选择器或 IP 地址分配</strong>。它的目的是在集群内部为<strong>外部的一个 DNS 域名</strong>创建一个别名。</p>
<ul>
<li><strong>CNAME 记录</strong>：CoreDNS 会为 ExternalName Service 的 FQDN <strong><code>&lt;service-name&gt;.&lt;namespace-name&gt;.svc.&lt;cluster-domain&gt;</code></strong> 创建一个 <strong>CNAME 记录</strong>。这个 CNAME 记录的值是 Service 定义中 <code>.spec.externalName</code> 字段指定的外部域名。当集群内部的 Pod 查询这个 Service FQDN 时，CoreDNS 会返回 CNAME 记录，指示客户端应该去查询 <code>externalName</code> 指定的那个域名。这相当于在集群 DNS 内部做了一个“符号链接”或“别名”，将内部服务名映射到集群外部的某个实际服务上，而无需关心外部服务的 IP 是否变化。</li>
</ul>
<h3 id="Pod-的-DNS-配置-dnsPolicy-etc-resolv-conf-dnsConfig-2"><a href="#Pod-的-DNS-配置-dnsPolicy-etc-resolv-conf-dnsConfig-2" class="headerlink" title="Pod 的 DNS 配置 (dnsPolicy, /etc/resolv.conf, dnsConfig)"></a>Pod 的 DNS 配置 (<code>dnsPolicy</code>, <code>/etc/resolv.conf</code>, <code>dnsConfig</code>)</h3><p>Kubernetes 通过 <code>kubelet</code> 组件管理每个 Pod 的 DNS 配置，确保 Pod 能够正确地使用 CoreDNS 进行服务发现。</p>
<ul>
<li><p><strong><code>dnsPolicy</code>:</strong> Pod Spec 中的字段，决定 DNS 配置策略。</p>
<ul>
<li><strong><code>ClusterFirst</code> (默认):</strong><ul>
<li>Pod 内 <code>/etc/resolv.conf</code> 由 Kubelet 生成。</li>
<li><code>nameserver</code> 指向 CoreDNS Service IP。</li>
<li><code>search</code> 包含 Pod 命名空间、svc 域、集群域以及 Node 的搜索域。</li>
<li>查询短名称 (点数 &lt; <code>ndots</code>) 时，会优先尝试附加 <code>search</code> 域进行解析。</li>
<li>无法解析的查询会通过 CoreDNS 转发给上游 DNS。</li>
</ul>
</li>
<li><strong><code>Default</code>:</strong> Pod 直接继承 Node 节点的 <code>/etc/resolv.conf</code> 配置。Pod 将无法解析集群内部 Service 名称，除非 Node 的 DNS 配置了指向 CoreDNS。</li>
<li><strong><code>None</code>:</strong> Kubelet <strong>不管理</strong> Pod 的 <code>/etc/resolv.conf</code>。Pod 使用镜像自带的 <code>/etc/resolv.conf</code>。通常需要配合 <code>dnsConfig</code> 字段使用。</li>
<li><strong><code>ClusterFirstWithHostNet</code>:</strong> 用于 <code>hostNetwork: true</code> 的 Pod，行为类似 <code>ClusterFirst</code>，但会考虑宿主机网络环境。</li>
</ul>
</li>
<li><p><strong><code>/etc/resolv.conf</code> (由 Kubelet 生成示例 - <code>ClusterFirst</code>):</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># search &lt;namespace&gt;.svc.&lt;cluster.local&gt; svc.&lt;cluster.local&gt; &lt;cluster.local&gt; [node&#x27;s search domains]</span><br>search default.svc.cluster.local svc.cluster.local cluster.local mycorp.com<br><span class="hljs-comment"># nameserver 指向 CoreDNS Service IP</span><br>nameserver 10.96.0.10<br><span class="hljs-comment"># options ndots:5 (关键选项)</span><br>options ndots:5 <span class="hljs-built_in">timeout</span>:1 attempts:2<br></code></pre></td></tr></table></figure>
<ul>
<li><code>search</code>: 域名搜索路径。查询 <code>my-svc</code> 时会尝试 <code>my-svc.default.svc.cluster.local</code>, <code>my-svc.svc.cluster.local</code>, …</li>
<li><code>nameserver</code>: DNS 服务器地址。</li>
<li><code>options ndots:5</code>: 如果查询的域名包含的点 <code>&lt; 5</code>，优先尝试附加 <code>search</code> 域；如果 <code>&gt;= 5</code>，则视为 FQDN 直接查询。</li>
</ul>
</li>
<li><p><strong><code>dnsConfig</code>:</strong> Pod Spec 中的字段，<strong>仅在 <code>dnsPolicy: &quot;None&quot;</code> 时或需要覆盖默认策略时使用</strong>。允许用户显式指定 <code>nameservers</code>, <code>searches</code>, <code>options</code>。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">dnsPolicy:</span> <span class="hljs-string">&quot;None&quot;</span> <span class="hljs-comment"># Kubelet 不自动配置</span><br>  <span class="hljs-attr">dnsConfig:</span>        <span class="hljs-comment"># 用户提供完整配置</span><br>    <span class="hljs-attr">nameservers:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;1.1.1.1&quot;</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;8.8.8.8&quot;</span><br>    <span class="hljs-attr">searches:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-string">my.custom.domain</span><br>    <span class="hljs-attr">options:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ndots</span><br>        <span class="hljs-attr">value:</span> <span class="hljs-string">&quot;1&quot;</span><br></code></pre></td></tr></table></figure>
<p><strong>注意:</strong> 如果使用 <code>dnsPolicy: None</code> 但仍需解析集群服务，必须在 <code>dnsConfig.nameservers</code> 中手动加入 CoreDNS 的 IP，并在 <code>dnsConfig.searches</code> 中加入集群搜索域。</p>
</li>
<li><p><strong>环境变量方式 (已不推荐):</strong> 早期 K8s 会将 Service 信息注入环境变量 (<code>&lt;SERVICE_NAME&gt;_SERVICE_HOST</code>, <code>&lt;SERVICE_NAME&gt;_SERVICE_PORT</code>)。这种方式是静态的，无法感知 Service 或 IP 变化，且污染环境，<strong>强烈建议使用 DNS 进行服务发现</strong>。</p>
</li>
</ul>
<h3 id="CoreDNS-配置示例-Corefile-2"><a href="#CoreDNS-配置示例-Corefile-2" class="headerlink" title="CoreDNS 配置示例 (Corefile)"></a>CoreDNS 配置示例 (<code>Corefile</code>)</h3><p>CoreDNS 的行为由 <code>Corefile</code> 控制，通常存储在 <code>coredns</code> ConfigMap 中。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs coredns"># Corefile 示例 (典型配置)<br>.:53 &#123;                                                  # 监听所有域 (.) 在 UDP/TCP 的 53 端口<br>    errors                                              # 启用错误日志<br>    health &#123;                                            # 健康检查 /health<br>       lameduck 5s<br>    &#125;<br>    ready                                               # 就绪检查 /ready<br>    kubernetes cluster.local in-addr.arpa ip6.arpa &#123;    # Kubernetes 插件<br>       pods insecure                                    # 解析 Pod A 记录<br>       upstream                                         # 解析 ExternalName Service 时使用上游<br>       fallthrough in-addr.arpa ip6.arpa                # 非 K8s 域查询传递给下一插件<br>    &#125;<br>    prometheus :9153                                    # 暴露 Prometheus 指标 /metrics<br>    forward . /etc/resolv.conf &#123;                        # 转发插件，使用 Node 的 DNS 作为上游<br>       max_concurrent 1000<br>    &#125;<br>    cache 30                                            # 缓存插件，TTL 30 秒<br>    loop                                                # 循环检测<br>    reload 5s                                           # 5 秒检查配置更新并热加载<br>    loadbalance                                         # 对 A/AAAA/MX 记录轮询负载均衡<br>&#125;<br></code></pre></td></tr></table></figure>
<p><strong>配置解读:</strong></p>
<ul>
<li><code>.:53 &#123; ... &#125;</code>: 定义了一个处理所有 DNS 查询（<code>.</code> 代表根区域），监听标准 DNS 端口 53。</li>
<li><code>errors</code>: 捕获并记录处理过程中的错误。</li>
<li><code>health &#123; lameduck 5s &#125;</code>: 在 <code>:8080/health</code> (默认端口) 提供健康检查接口。<code>lameduck</code> 确保在 CoreDNS 准备关闭时，会先保持 5 秒的运行状态以完成正在处理的请求，然后才报告不健康。</li>
<li><code>ready</code>: 在 <code>:8181/ready</code> (默认端口) 提供就绪检查接口，只有当 CoreDNS 完全准备好处理请求时才返回成功。</li>
<li><code>kubernetes cluster.local in-addr.arpa ip6.arpa &#123; ... &#125;</code>: 这是核心。<ul>
<li><code>cluster.local</code>: 指定 CoreDNS 负责解析的 Kubernetes 集群域。所有 <code>*.cluster.local</code> 的查询将由这个插件处理。</li>
<li><code>in-addr.arpa</code> 和 <code>ip6.arpa</code>: 使插件能够处理 PTR 反向查询，将 IP 地址解析回对应的 Kubernetes 服务或 Pod 名称。</li>
<li><code>pods insecure</code>: 允许解析 Pod 的 IP 地址到 DNS 名称的 A 记录。<code>insecure</code> 表示即使 Pod 未处于 Ready 状态也可能被解析（通常还支持 <code>verified</code> 和 <code>disabled</code> 选项）。</li>
<li><code>upstream</code>: 对于指向外部名称 (ExternalName) 的 Service，如果需要解析，会使用上游解析器。</li>
<li><code>fallthrough in-addr.arpa ip6.arpa</code>: 如果查询的名称不匹配 <code>cluster.local</code> 域，或者不匹配反向查询域，则将请求传递给插件链中的下一个插件（在这里是 <code>forward</code>）。</li>
</ul>
</li>
<li><code>prometheus :9153</code>: 在 9153 端口暴露 <code>/metrics</code> 端点，供 Prometheus 抓取监控数据。</li>
<li><code>forward . /etc/resolv.conf</code>: 对于不由 <code>kubernetes</code> 插件处理的查询（由于 <code>fallthrough</code>），使用此插件转发。<code>.</code> 表示转发所有域。<code>/etc/resolv.conf</code> 表示使用 CoreDNS Pod 所在 Node 的 DNS 配置作为上游解析器。也可以直接指定 IP 地址，如 <code>forward . 8.8.8.8 1.1.1.1</code>。</li>
<li><code>cache 30</code>: 对 DNS 记录进行缓存，默认缓存时间 30 秒。这能显著提高性能并减少对上游 DNS 的依赖。</li>
<li><code>loop</code>: 防止 DNS 查询在转发过程中形成循环。</li>
<li><code>reload 5s</code>: CoreDNS 会定期检查 <code>Corefile</code> 的修改时间。如果文件发生变化，它会在不中断服务的情况下平滑地重新加载配置。</li>
<li><code>loadbalance</code>: 对 DNS 响应中的多个 A&#x2F;AAAA&#x2F;MX 记录进行轮询分发，提供简单的客户端负载均衡。</li>
</ul>
<p>通过这套机制，CoreDNS 为 Kubernetes 集群提供了一个健壮、高效且可配置的内部 DNS 解析服务，完美解决了因 Service IP 动态变化而导致的服务发现难题，是构建稳定可靠微服务架构的关键基础设施之一。理解 CoreDNS 的工作原理和配置方式，对于深入掌握 Kubernetes 网络和服务发现至关重要。</p>
<h3 id="DNS-的落地实践：与企业-DNS-集成-ExternalDNS"><a href="#DNS-的落地实践：与企业-DNS-集成-ExternalDNS" class="headerlink" title="DNS 的落地实践：与企业 DNS 集成 (ExternalDNS)"></a>DNS 的落地实践：与企业 DNS 集成 (ExternalDNS)</h3><p>为了让集群外部也能通过 DNS 访问 K8s 服务 (通常是 LoadBalancer 或 Ingress)，可以使用 <strong>ExternalDNS</strong> 控制器。</p>
<ul>
<li><strong>工作原理:</strong> ExternalDNS 监听 K8s <code>Service</code> 和 <code>Ingress</code> 资源，根据注解或配置，自动在<strong>外部 DNS 提供商</strong> (如 AWS Route 53, Google Cloud DNS, Azure DNS, 或企业内部 BIND) 中创建&#x2F;更新&#x2F;删除 DNS 记录，将服务域名指向 LoadBalancer IP 或 Ingress IP。</li>
<li><strong>目的:</strong> 实现服务在集群内外使用统一的域名访问。</li>
<li><strong>Headless Service 注意事项:</strong> 将 Headless Service 的所有 Pod IP 发布到外部 DNS 需谨慎，可能导致 DNS 记录频繁变动和管理复杂性。通常只按需发布特定 Pod 的记录，并确保 Pod IP 在外部可路由。</li>
</ul>
<p><strong>配置示例 (ExternalDNS 部署片段 - 示意)</strong>：<br>部署 ExternalDNS 时，需要配置它连接到哪个 K8s 集群、监听哪些资源、使用哪个 DNS 提供商以及如何认证。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># deployment.yaml (部分)</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">external-dns</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span> <span class="hljs-comment"># 或者其他专用 namespace</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">strategy:</span><br>    <span class="hljs-attr">type:</span> <span class="hljs-string">Recreate</span><br>  <span class="hljs-attr">selector:</span><br>    <span class="hljs-attr">matchLabels:</span><br>      <span class="hljs-attr">app:</span> <span class="hljs-string">external-dns</span><br>  <span class="hljs-attr">template:</span><br>    <span class="hljs-attr">metadata:</span><br>      <span class="hljs-attr">labels:</span><br>        <span class="hljs-attr">app:</span> <span class="hljs-string">external-dns</span><br>    <span class="hljs-attr">spec:</span><br>      <span class="hljs-attr">serviceAccountName:</span> <span class="hljs-string">external-dns</span> <span class="hljs-comment"># 需要配置 RBAC 权限访问 Service/Ingress</span><br>      <span class="hljs-attr">containers:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">external-dns</span><br>        <span class="hljs-attr">image:</span> <span class="hljs-string">registry.k8s.io/external-dns/external-dns:v0.13.x</span> <span class="hljs-comment"># 使用合适的版本</span><br>        <span class="hljs-attr">args:</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-string">--source=service</span> <span class="hljs-comment"># 监听 Service 资源</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-string">--source=ingress</span> <span class="hljs-comment"># 同时监听 Ingress 资源</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-string">--domain-filter=my-company.com</span> <span class="hljs-comment"># 只管理这个域下的记录</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-string">--provider=aws</span> <span class="hljs-comment"># 指定 DNS 提供商 (例如 AWS Route 53)</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-string">--policy=upsert-only</span> <span class="hljs-comment"># DNS 记录管理策略 (安全起见，防止误删)</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-string">--aws-zone-type=public</span> <span class="hljs-comment"># 指定 Route 53 Zone 类型</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-string">--registry=txt</span> <span class="hljs-comment"># 使用 TXT 记录来跟踪所有权</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-string">--txt-owner-id=my-k8s-cluster-id</span> <span class="hljs-comment"># 唯一的 Owner ID 防止冲突</span><br>        <span class="hljs-comment"># 可能还需要配置 AWS 凭证，通常通过 Service Account + IAM Role (IRSA) 或挂载 Secret</span><br>      <span class="hljs-attr">securityContext:</span><br>        <span class="hljs-attr">fsGroup:</span> <span class="hljs-number">65534</span> <span class="hljs-comment"># 非 root 用户运行</span><br></code></pre></td></tr></table></figure>

<hr>
<h2 id="实战分析：理解-Service-访问"><a href="#实战分析：理解-Service-访问" class="headerlink" title="实战分析：理解 Service 访问"></a>实战分析：理解 Service 访问</h2><p>让我们分析一下文章末尾提供的实战场景：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs bash">//centos pod<br>cadmin@master:~/101$ k get po<br>NAME                                READY   STATUS    RESTARTS   AGE<br>centos-686b8db5c7-krgbb             1/1     Running   0          51m<br>nginx-deployment-7854ff8877-nc8kr   1/1     Running   0          13m<br>cadmin@master:~/101$ k get svc<br>NAME          TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)        AGE<br>kubernetes    ClusterIP   10.96.0.1     &lt;none&gt;        443/TCP        41d<br>nginx-basic   NodePort    10.98.3.144   &lt;none&gt;        80:30457/TCP   28m<br><br><span class="hljs-comment"># 在 centos Pod 内部执行</span><br>[root@centos-686b8db5c7-krgbb /]# ping nginx-basic.default.svc.cluster.local<br>PING nginx-basic.default.svc.cluster.local (10.98.3.144) 56(84) bytes of data.<br><span class="hljs-comment"># DNS 解析成功，将服务名解析为 ClusterIP 10.98.3.144</span><br>From nginx-basic.default.svc.cluster.local (10.98.3.144) icmp_seq=1 Destination Port Unreachable<br><span class="hljs-comment"># Ping 失败，收到 Port Unreachable</span><br>--- nginx-basic.default.svc.cluster.local ping statistics ---<br>2 packets transmitted, 0 received, +2 errors, 100% packet loss, <span class="hljs-keyword">time</span> 1001ms<br></code></pre></td></tr></table></figure>

<p><strong>分析:</strong></p>
<ol>
<li><p><strong>DNS 解析成功:</strong> <code>ping nginx-basic.default.svc.cluster.local</code> 首先触发 DNS 查询。根据 <code>/etc/resolv.conf</code>，查询被发送到 <code>10.96.0.10</code> (CoreDNS)。CoreDNS 成功将 <code>nginx-basic.default.svc.cluster.local</code> 解析为其 ClusterIP <code>10.98.3.144</code>。这证明 <strong>CoreDNS 服务发现工作正常</strong>。</p>
</li>
<li><p><strong>Ping (ICMP) 失败:</strong> <code>ping</code> 命令使用的是 <strong>ICMP Echo Request</strong> 协议。然而，Kubernetes Service (ClusterIP) 是一个<strong>虚拟 IP</strong>，它本身并不监听任何端口或协议，而是依赖 <code>kube-proxy</code> 创建的网络规则来<strong>转发特定端口的 TCP&#x2F;UDP 流量</strong>。<code>nginx-basic</code> Service 定义了端口 <code>80/TCP</code>。当 ICMP Echo Request 到达 <code>10.98.3.144</code> 时，没有 <code>kube-proxy</code> 规则或进程直接处理 ICMP，因此内核网络栈最终返回 “Destination Port Unreachable” (更准确地说是 Protocol Unreachable 或类似 ICMP 错误，具体取决于内核实现)。</p>
</li>
<li><p><strong>如何正确测试:</strong> 要测试 Service 是否工作，需要使用 Service 定义的<strong>协议和端口</strong>。对于 <code>nginx-basic</code> (端口 80&#x2F;TCP)，应该使用 HTTP 客户端 (如 <code>curl</code>) 或 TCP 连接工具 (如 <code>telnet</code>):</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 在 centos Pod 内部执行</span><br><br><span class="hljs-comment"># 使用 curl 测试 HTTP 服务</span><br>[root@centos-686b8db5c7-krgbb /]# curl http://nginx-basic.default.svc.cluster.local<br><span class="hljs-comment"># 或者直接用 ClusterIP</span><br>[root@centos-686b8db5c7-krgbb /]# curl http://10.98.3.144<br><span class="hljs-comment"># 预期会看到 Nginx 的欢迎页面</span><br><br><span class="hljs-comment"># 使用 telnet 测试 TCP 端口连通性</span><br>[root@centos-686b8db5c7-krgbb /]# telnet nginx-basic.default.svc.cluster.local 80<br>Trying 10.98.3.144...<br>Connected to nginx-basic.default.svc.cluster.local.<br>Escape character is <span class="hljs-string">&#x27;^]&#x27;</span>.<br><span class="hljs-comment"># (连接成功，可以输入 HTTP 命令或按 Ctrl+] 退出)</span><br></code></pre></td></tr></table></figure>
<p>如果 <code>curl</code> 或 <code>telnet</code> 成功，说明 <code>kube-proxy</code> 的规则正确地将 TCP 端口 80 的流量转发到了后端的 Nginx Pod。</p>
</li>
</ol>
<p><strong>结论:</strong> <code>ping</code> Service ClusterIP 失败是预期行为，并不代表 Service 或 <code>kube-proxy</code> 有问题。测试 Service 需要使用其暴露的协议和端口。</p>
<hr>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Kubernetes 网络是一个复杂但设计精良的系统。<code>kube-proxy</code> 通过不断演进的模式 (iptables, IPVS)，利用 Linux 内核的网络能力 (Netfilter, IPVS)，实现了 Service 这一核心抽象的数据平面转发和负载均衡。<code>CoreDNS</code> 则作为集群的 DNS 服务，提供了基于稳定名称的服务发现机制，解耦了客户端与动态变化的 Pod IP。理解 <code>kube-proxy</code> 的工作原理、Netfilter 的基础以及 <code>CoreDNS</code> 的服务发现机制，对于诊断网络问题、优化集群性能和构建可靠的云原生应用至关重要。随着 eBPF 等新技术的应用，Kubernetes 网络未来将继续向着更高性能、更灵活和更可编程的方向发展。</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/kubernetes/" class="print-no-link">#kubernetes</a>
      
        <a href="/tags/networking/" class="print-no-link">#networking</a>
      
        <a href="/tags/kube-proxy/" class="print-no-link">#kube-proxy</a>
      
        <a href="/tags/coredns/" class="print-no-link">#coredns</a>
      
        <a href="/tags/netfilter/" class="print-no-link">#netfilter</a>
      
        <a href="/tags/ipvs/" class="print-no-link">#ipvs</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Kubernetes 网络核心：kube-proxy 与 CoreDNS 深度解析</div>
      <div>https://mfzzf.github.io/2025/04/10/kube-proxy/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Mzzf</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年4月10日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/04/17/docker-backup/" title="docker-backup">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">docker-backup</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/04/03/kubernetes-Service%E5%AF%B9%E8%B1%A1/" title="Kubernetes Service 深度解析">
                        <span class="hidden-mobile">Kubernetes Service 深度解析</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
