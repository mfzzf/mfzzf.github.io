

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Mzzf">
  <meta name="keywords" content="">
  
    <meta name="description" content="生产化 Kubernetes 集群管理1. 计算节点相关生产化集群的考量在构建和维护生产级别的 Kubernetes 集群时，计算节点和控制平面都需要仔细考虑以下问题： 计算节点 (Worker Nodes):  操作系统管理: 如何实现操作系统的大规模批量安装和升级？自动化工具（如 Kickstart, Preseed, Packer, Foreman）和镜像管理（如 ostree）是关键。 如">
<meta property="og:type" content="article">
<meta property="og:title" content="kubernetes生产化集群管理">
<meta property="og:url" content="https://mfzzf.github.io/2025/04/24/kubernetes%E7%94%9F%E4%BA%A7%E5%8C%96%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/index.html">
<meta property="og:site_name" content="Mzzf&#39;s Blog">
<meta property="og:description" content="生产化 Kubernetes 集群管理1. 计算节点相关生产化集群的考量在构建和维护生产级别的 Kubernetes 集群时，计算节点和控制平面都需要仔细考虑以下问题： 计算节点 (Worker Nodes):  操作系统管理: 如何实现操作系统的大规模批量安装和升级？自动化工具（如 Kickstart, Preseed, Packer, Foreman）和镜像管理（如 ostree）是关键。 如">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/24/kubernetes%E7%94%9F%E4%BA%A7%E5%8C%96%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/image-20250425002033364.png">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/24/kubernetes%E7%94%9F%E4%BA%A7%E5%8C%96%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/image-20250426234348146.png">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/24/kubernetes%E7%94%9F%E4%BA%A7%E5%8C%96%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/image-20250428090416959.png">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/24/kubernetes%E7%94%9F%E4%BA%A7%E5%8C%96%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/image-20250429104305577.png">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/24/kubernetes%E7%94%9F%E4%BA%A7%E5%8C%96%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/image-20250429105221429.png">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/24/kubernetes%E7%94%9F%E4%BA%A7%E5%8C%96%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/image-20250509131354765.png">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/24/kubernetes%E7%94%9F%E4%BA%A7%E5%8C%96%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/image-20250509150422859.png">
<meta property="og:image" content="https://mfzzf.github.io/2025/04/24/kubernetes%E7%94%9F%E4%BA%A7%E5%8C%96%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/image-20250509150436143.png">
<meta property="article:published_time" content="2025-04-24T15:56:37.000Z">
<meta property="article:modified_time" content="2025-05-09T07:04:36.507Z">
<meta property="article:author" content="Mzzf">
<meta property="article:tag" content="kubernetes">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://mfzzf.github.io/2025/04/24/kubernetes%E7%94%9F%E4%BA%A7%E5%8C%96%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/image-20250425002033364.png">
  
  
  
  <title>kubernetes生产化集群管理 - Mzzf&#39;s Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"mfzzf.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Mzzf&#39;s blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="kubernetes生产化集群管理"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-04-24 23:56" pubdate>
          2025年4月24日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          51k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          423 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">kubernetes生产化集群管理</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="生产化-Kubernetes-集群管理"><a href="#生产化-Kubernetes-集群管理" class="headerlink" title="生产化 Kubernetes 集群管理"></a>生产化 Kubernetes 集群管理</h1><h2 id="1-计算节点相关"><a href="#1-计算节点相关" class="headerlink" title="1. 计算节点相关"></a>1. 计算节点相关</h2><h3 id="生产化集群的考量"><a href="#生产化集群的考量" class="headerlink" title="生产化集群的考量"></a>生产化集群的考量</h3><p>在构建和维护生产级别的 Kubernetes 集群时，计算节点和控制平面都需要仔细考虑以下问题：</p>
<p><strong>计算节点 (Worker Nodes):</strong></p>
<ul>
<li><strong>操作系统管理:</strong><ul>
<li>如何实现操作系统的大规模批量安装和升级？自动化工具（如 Kickstart, Preseed, Packer, Foreman）和镜像管理（如 ostree）是关键。</li>
<li>如何统一管理和配置节点的网络信息（IP 地址、DNS、路由等）？配置管理工具（Ansible, SaltStack, Chef, Puppet）或云平台提供的服务可以提供帮助。</li>
</ul>
</li>
<li><strong>节点多样性管理:</strong><ul>
<li>如何有效管理具有不同硬件配置（CPU, 内存, GPU, 存储类型等）的节点（即不同的 SKU - Stock Keeping Unit）？利用 Kubernetes 的标签（Labels）和污点（Taints）&#x2F;容忍（Tolerations）机制进行分类和调度。</li>
</ul>
</li>
<li><strong>生命周期管理:</strong><ul>
<li>如何快速、安全地下架出现故障的计算节点？需要结合监控、自动化的故障检测和节点排空（Drain）机制。</li>
<li>如何根据负载需求快速扩容或缩容集群的计算节点规模？集群自动伸缩器（Cluster Autoscaler）是核心解决方案。</li>
</ul>
</li>
</ul>
<p><strong>控制平面 (Control Plane):</strong></p>
<ul>
<li><strong>组件部署与升级:</strong><ul>
<li>如何在主节点（Master Nodes）上高效地下载、安装和升级控制平面核心组件（kube-apiserver, etcd, kube-scheduler, kube-controller-manager）及其所需的配置文件？通常使用 kubeadm、k3s、RKE 或云厂商提供的托管服务。</li>
<li>如何确保集群依赖的其他关键插件（如 CoreDNS 用于服务发现、监控系统如 Prometheus&#x2F;Grafana、日志系统如 EFK&#x2F;Loki）已正确部署并正常运行？使用 Helm Charts 或 Operator 模式进行管理。</li>
</ul>
</li>
<li><strong>安全:</strong><ul>
<li>如何生成、分发和管理控制平面组件所需的各种安全证书（CA 证书、API Server 证书、kubelet 证书等）？需要建立一套安全的证书管理流程，可以使用 <code>kubeadm</code> 的证书管理功能或专门的工具如 <code>cert-manager</code>。</li>
</ul>
</li>
<li><strong>版本管理:</strong><ul>
<li>如何实现控制平面组件的快速、可靠升级或在出现问题时进行版本回滚？需要制定详细的升级计划，利用滚动升级、蓝绿部署等策略，并确保有可靠的备份和恢复机制（尤其是 etcd）。</li>
</ul>
</li>
</ul>
<h2 id="2-操作系统选择"><a href="#2-操作系统选择" class="headerlink" title="2. 操作系统选择"></a>2. 操作系统选择</h2><p>为 Kubernetes 计算节点选择合适的操作系统是构建稳定、高效、安全集群的基础。</p>
<h3 id="操作系统的评估与选择"><a href="#操作系统的评估与选择" class="headerlink" title="操作系统的评估与选择"></a>操作系统的评估与选择</h3><p>主要有两大类操作系统可供选择：</p>
<ol>
<li><strong>通用操作系统 (General Purpose OS):</strong><ul>
<li><strong>代表:</strong> Ubuntu, CentOS, Fedora</li>
<li><strong>优点:</strong> 生态成熟、社区支持广泛、用户熟悉度高、软件包丰富。</li>
<li><strong>缺点:</strong> 通常包含较多非必需组件，攻击面相对较大，系统升级可能涉及较多依赖和风险。</li>
</ul>
</li>
<li><strong>专为容器优化的操作系统 (Container-Optimized OS):</strong><ul>
<li><strong>代表:</strong> CoreOS (已被 Red Hat 收购并演化), Red Hat Atomic Host, Snappy Ubuntu Core, RancherOS, Flatcar Container Linux (CoreOS 分支), Bottlerocket (AWS)</li>
<li><strong>特点:</strong><ul>
<li><strong>最小化:</strong> 只包含运行容器和管理节点所必需的组件，减少资源占用和攻击面。</li>
<li><strong>原子化升级&#x2F;回滚:</strong> 通常采用基于镜像或文件系统快照的方式进行升级，保证操作的原子性，易于回滚。</li>
<li><strong>不可变性 (Immutability):</strong> 核心系统文件通常是只读的，提高了安全性，符合云原生理念。</li>
</ul>
</li>
</ul>
</li>
</ol>
<p><strong>评估和选型的标准:</strong></p>
<ul>
<li><strong>生态系统:</strong> 是否有活跃的社区、商业支持、丰富的文档和工具链？</li>
<li><strong>成熟度:</strong> 该操作系统在生产环境中的应用历史和稳定性表现如何？</li>
<li><strong>内核版本:</strong> 内核版本是否足够新，以支持 Kubernetes 所需的特性（如 Cgroups v2, eBPF 等）？内核的稳定性和安全补丁更新频率如何？</li>
<li><strong>容器运行时支持:</strong> 对 Docker, containerd, CRI-O 等主流容器运行时的兼容性和支持程度如何？</li>
<li><strong>初始化系统 (Init System):</strong> 通常是 systemd，需要考虑其管理服务的便利性和稳定性。</li>
<li><strong>包管理和系统升级:</strong> 包管理工具是否易用？系统升级机制是否可靠、安全（如原子化升级）？</li>
<li><strong>安全:</strong> 是否提供强化的安全特性？补丁更新是否及时？社区或厂商的安全响应能力如何？</li>
</ul>
<h3 id="生态系统与成熟度"><a href="#生态系统与成熟度" class="headerlink" title="生态系统与成熟度"></a>生态系统与成熟度</h3><p><strong>容器优化操作系统的优势:</strong></p>
<ul>
<li><strong>小 (Small Footprint):</strong> 系统镜像体积小，启动速度快，资源消耗低。</li>
<li><strong>原子级升级和回退 (Atomic Updates &amp; Rollbacks):</strong> 升级过程要么完全成功，要么完全回退到之前的状态，降低了升级风险。</li>
<li><strong>更高的安全性 (Enhanced Security):</strong> 最小化的组件和只读根文件系统减少了潜在的攻击面。</li>
</ul>
<p><strong>不同容器优化</strong> OS 的成熟度比较 <strong>(基于原始 PDF 内容):</strong></p>
<table>
<thead>
<tr>
<th>操作系统</th>
<th>类型</th>
<th>成熟度与特点</th>
</tr>
</thead>
<tbody><tr>
<td>CentOS &#x2F; Ubuntu</td>
<td>通用操作系统</td>
<td>成熟，生态庞大，用户基数大</td>
</tr>
<tr>
<td>CoreOS</td>
<td>容器优化</td>
<td>最早的容器优化 OS 之一，理念先进，但原公司已被收购，现在更多以 Flatcar 或 Red Hat CoreOS 形式存在。</td>
</tr>
<tr>
<td>Atomic*</td>
<td>容器优化</td>
<td>Red Hat 出品，基于 RPM 生态，品质有保证，与 RHEL&#x2F;CentOS 结合紧密。</td>
</tr>
<tr>
<td>Snappy</td>
<td>容器优化</td>
<td>Canonical (Ubuntu) 出品，最初为移动和 IoT 设备设计，采用 Snap 包格式。</td>
</tr>
<tr>
<td>RancherOS</td>
<td>容器优化</td>
<td>理念独特，系统中几乎所有服务（包括 systemd&#x2F;udev）都作为 Docker 容器运行，相对较新，现在是 SUSE 的一部分。</td>
</tr>
</tbody></table>
<p>*<em>注：Atomic Host 项目已不再积极开发，其理念和技术被整合到 RHEL CoreOS (RHCOS) 中，用于 OpenShift 集群。</em></p>
<h3 id="云原生的原则：不可变基础设施"><a href="#云原生的原则：不可变基础设施" class="headerlink" title="云原生的原则：不可变基础设施"></a>云原生的原则：不可变基础设施</h3><p><strong>可变基础设施 (Mutable Infrastructure) 的风险:</strong></p>
<ul>
<li><strong>配置漂移 (Configuration Drift):</strong> 在服务运行过程中，持续的手动修改或自动化脚本修改服务器配置，可能导致服务器状态与预期配置不一致，难以追踪和复现。</li>
<li><strong>重建困难:</strong> 当发生灾难需要重建服务时，由于缺乏精确的操作记录和自动化流程，很难精确地恢复到之前的运行状态。</li>
<li><strong>状态不一致:</strong> 持续的修改引入了中间状态，可能导致不可预知的问题，类似于程序中可变变量在并发环境下引入的风险。</li>
</ul>
<p><strong>不可变基础设施 (Immutable Infrastructure):</strong></p>
<ul>
<li><strong>核心理念:</strong> 一旦部署，基础设施（服务器、容器等）就不再被修改。如果需要更新或修复，则用新的实例替换旧的实例。</li>
<li><strong>实践:</strong><ul>
<li><strong>不可变的容器镜像 (Immutable Container Images):</strong> Dockerfile 定义了镜像的构建过程，一旦构建完成，镜像内容不再改变。更新应用需要构建新镜像并重新部署。</li>
<li><strong>不可变的主机操作系统 (Immutable Host OS):</strong> 采用容器优化 OS，操作系统本身是只读的，升级通过替换整个系统镜像或文件系统层来实现。</li>
</ul>
</li>
</ul>
<p><strong>优势:</strong></p>
<ul>
<li><strong>一致性:</strong> 保证了环境的一致性，简化了测试和部署。</li>
<li><strong>可靠性:</strong> 减少了配置漂移和手动操作引入的错误。</li>
<li><strong>可预测性:</strong> 部署和升级过程更加可预测和可重复。</li>
<li><strong>安全性:</strong> 减少了攻击面，更易于管理安全补丁。</li>
</ul>
<h3 id="Atomic-操作系统"><a href="#Atomic-操作系统" class="headerlink" title="Atomic 操作系统"></a>Atomic 操作系统</h3><ul>
<li><strong>背景:</strong> 由 Red Hat 支持的，旨在提供面向容器优化的、不可变的基础设施。有基于 Fedora, CentOS, RHEL 的不同版本。 (注意：如前所述，此项目已演进)</li>
<li><strong>核心优势:</strong><ul>
<li><strong>不可变性:</strong> 操作系统核心部分是只读的（通常只有 <code>/etc</code> 和 <code>/var</code> 可写），提高了安全性和稳定性。</li>
<li><strong>面向容器优化:</strong> 最小化安装，集成了容器运行时和相关工具。</li>
<li><strong>灵活性与安全性:</strong> 结合了传统 Linux 的灵活性和不可变基础设施的安全性。</li>
</ul>
</li>
<li><strong>关键技术: rpm-ostree:</strong><ul>
<li><strong>定义:</strong> 一个开源项目，结合了 RPM 包管理和 OSTree（类似 Git 的文件系统版本管理）技术。</li>
<li><strong>功能:</strong><ul>
<li>允许以原子方式管理系统包，构建可启动的操作系统镜像。</li>
<li>支持操作系统的原子升级和回滚。可以轻松切换到不同的系统版本（commit）。</li>
<li>使得在生产环境中构建和管理操作系统镜像更加简单和可靠。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="最小化主机操作系统"><a href="#最小化主机操作系统" class="headerlink" title="最小化主机操作系统"></a>最小化主机操作系统</h3><ul>
<li><strong>原则:</strong><ul>
<li>只安装运行 Kubernetes 节点（kubelet, 容器运行时等）和基本系统维护所<strong>绝对必需</strong>的工具。</li>
<li>任何临时的调试工具（如 <code>tcpdump</code>, <code>strace</code>, <code>perf</code> 等性能或网络排查工具）都应在需要时以容器的形式运行（例如，通过 <code>kubectl debug</code> 或部署特权调试 Pod）。</li>
</ul>
</li>
<li><strong>意义:</strong><ul>
<li><strong>性能:</strong> 更少的后台进程和服务意味着更低的资源消耗（CPU, 内存），为业务容器提供更多资源。</li>
<li><strong>稳定性:</strong> 更少的组件意味着更少的潜在故障点和软件冲突。</li>
<li><strong>安全保障:</strong> 最小化的攻击面，减少了被利用的风险。补丁管理也更简单。</li>
</ul>
</li>
</ul>
<h3 id="操作系统构建流程"><a href="#操作系统构建流程" class="headerlink" title="操作系统构建流程"></a>操作系统构建流程</h3><img src="/2025/04/24/kubernetes%E7%94%9F%E4%BA%A7%E5%8C%96%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/image-20250425002033364.png" srcset="/img/loading.gif" lazyload class="" title="image-20250425002033364">

<ul>
<li><strong>流程概述:</strong><ol>
<li><strong>基础 RPM 包:</strong> 从上游 (Upstream Patch) 或内部 Mirror 获取基础的 RPM 包。</li>
<li><strong>RPM 构建 (rpm builder):</strong> 可能用于构建自定义的 RPM 包。</li>
<li><strong>RPM 快照 (Snapshot):</strong> 将选定的 RPM 包集合形成一个快照。</li>
<li><strong>ostree 构建 (rpm-ostree):</strong> 使用 <code>rpm-ostree</code> 工具，基于 RPM 快照和可能的 Docker 镜像 (通过 <code>buildah</code> 处理)，构建成 ostree commit (版本化的文件系统树)。</li>
<li><strong>ostree</strong> 仓库 <strong>(ostree httpd):</strong> 将构建好的 ostree commit 存储在可通过 HTTP 访问的仓库中。</li>
<li><strong>镜像构建 (Packer Builder):</strong> 使用 Packer 等工具，从 ostree 仓库拉取指定的 commit，并结合 Glance (OpenStack 镜像服务) 或其他机制，构建成虚拟机镜像 (如 qcow2, vhd) 或物理机部署所需的文件。</li>
<li><strong>部署:</strong><ul>
<li><strong>虚拟机:</strong> 通过 OpenStack 或其他虚拟化平台部署镜像。</li>
<li><strong>物理机:</strong> 通过 Foreman 等 PXE 引导工具，加载 Kickstart&#x2F;Preseed 文件，该文件指示系统从 ostree 仓库部署操作系统。</li>
</ul>
</li>
<li><strong>节点运行 (K8s Nodes):</strong> 部署好的节点加入 Kubernetes 集群。</li>
<li><strong>更新 (OS Updater):</strong> 节点上的 OS Updater (可能是 <code>rpm-ostree</code> 自身或自定义脚本) 定期检查 ostree 仓库是否有新版本，并执行原子化升级。</li>
</ol>
</li>
</ul>
<h3 id="ostree-详解"><a href="#ostree-详解" class="headerlink" title="ostree 详解"></a>ostree 详解</h3><ul>
<li><strong>核心库:</strong> 提供 <code>libostree</code> 共享库和一系列命令行工具。</li>
<li><strong>类 Git 操作:</strong> 提供类似 Git 的命令行体验 (<code>ostree commit</code>, <code>ostree pull</code>, <code>ostree checkout</code> 等)，用于提交、下载和管理完整的、可启动的文件系统树的版本。每个版本是一个 commit hash。</li>
<li><strong>启动加载器集成:</strong> 提供将 ostree 管理的文件系统版本部署到 Bootloader (如 GRUB) 的机制，允许在启动时选择不同的系统版本。<ul>
<li><strong>示例 (Dracut 模块):</strong> <code>ostree-prepare-root</code> 服务在 initramfs 阶段运行，准备挂载 ostree 管理的根文件系统。</li>
</ul>
</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-comment"># 示例 Dracut 模块安装脚本片段</span><br><span class="hljs-comment"># (来自 https://github.com/ostreedev/ostree/blob/main/src/boot/dracut/module-setup.sh)</span><br><br><span class="hljs-function"><span class="hljs-title">install</span></span>() &#123;<br>    <span class="hljs-comment"># 安装 ostree 准备根文件系统的工具</span><br>    dracut_install /usr/lib/ostree/ostree-prepare-root<br>    <span class="hljs-comment"># 安装 systemd 服务单元</span><br>    inst_simple <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;systemdsystemunitdir&#125;</span>/ostree-prepare-root.service&quot;</span><br>    <span class="hljs-comment"># 创建必要的目录结构</span><br>    <span class="hljs-built_in">mkdir</span> -p <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;initdir&#125;</span><span class="hljs-variable">$&#123;systemdsystemconfdir&#125;</span>/initrd-root-fs.target.wants&quot;</span><br>    <span class="hljs-comment"># 创建服务链接，使其在 initrd 启动时运行</span><br>    ln_r <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;systemdsystemunitdir&#125;</span>/ostree-prepare-root.service&quot;</span> \<br>         <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;systemdsystemconfdir&#125;</span>/initrd-root-fs.target.wants/ostree-prepare-root.service&quot;</span><br>&#125;<br></code></pre></td></tr></table></figure>

<h3 id="构建-ostree"><a href="#构建-ostree" class="headerlink" title="构建 ostree"></a>构建 ostree</h3><ul>
<li><strong>工具:</strong> <code>rpm-ostree</code></li>
<li><strong>功能:</strong><ul>
<li>基于 <code>treefile</code> (JSON 格式的配置文件) 将指定的 RPM 包构建成 ostree commit。</li>
<li>管理 ostree 仓库和节点的 Bootloader 配置。</li>
</ul>
</li>
<li><strong>treefile (配置文件示例):</strong> 定义了构建 ostree commit 所需的元数据和包列表。</li>
</ul>
<figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli">&#123;<br>  <span class="hljs-string">//</span> 定义操作系统名称<br>  <span class="hljs-string">&quot;osname&quot;</span>: <span class="hljs-string">&quot;centos-atomic-host&quot;</span>,<br>  <span class="hljs-string">//</span> 定义 ostree 引用 <span class="hljs-params">(ref)</span>，类似 Git 分支 <span class="hljs-params">(名称/版本/架构/类型)</span><br>  <span class="hljs-string">&quot;ref&quot;</span>: <span class="hljs-string">&quot;centos-atomic-host/8/x86_64/standard&quot;</span>,<br>  <span class="hljs-string">//</span> 指定使用的 RPM 仓库<br>  <span class="hljs-string">&quot;repos&quot;</span>: [<br>    <span class="hljs-string">&quot;base&quot;</span>,<br>    <span class="hljs-string">&quot;appstream&quot;</span>,<br>    <span class="hljs-string">&quot;epel&quot;</span>,<br>    <span class="hljs-string">&quot;docker-ce&quot;</span>, <span class="hljs-string">//</span> 示例：包含 Docker CE 仓库<br>    <span class="hljs-string">&quot;ceph&quot;</span>,      <span class="hljs-string">//</span> 示例：包含 Ceph 仓库<br>    <span class="hljs-string">&quot;salt&quot;</span>,      <span class="hljs-string">//</span> 示例：包含 SaltStack 仓库<br>    <span class="hljs-string">&quot;other&quot;</span>,<br>    <span class="hljs-string">&quot;kernel-al&quot;</span>,<br>    <span class="hljs-string">&quot;ovs&quot;</span>,<br>    <span class="hljs-string">&quot;zts&quot;</span><br>  ],<br>  <span class="hljs-string">//</span> 是否启用 SELinux<br>  <span class="hljs-string">&quot;selinux&quot;</span>: <span class="hljs-literal">true</span>,<br>  <span class="hljs-string">//</span> 安装的语言包<br>  <span class="hljs-string">&quot;install-langs&quot;</span>: [<span class="hljs-string">&quot;en_US&quot;</span>],<br>  <span class="hljs-string">//</span> 是否包含文档<br>  <span class="hljs-string">&quot;documentation&quot;</span>: <span class="hljs-literal">false</span>,<br>  <span class="hljs-string">//</span> initramfs 参数<br>  <span class="hljs-string">&quot;initramfs-args&quot;</span>: [<span class="hljs-string">&quot;--no-hostonly&quot;</span>],<br>  <span class="hljs-string">//</span> 构建后处理脚本<br>  <span class="hljs-string">&quot;postprocess-script&quot;</span>: <span class="hljs-string">&quot;customize.sh&quot;</span>,<br>  <span class="hljs-string">//</span> 添加用户到指定组<br>  <span class="hljs-string">&quot;etc-group-members&quot;</span>: [<span class="hljs-string">&quot;wheel&quot;</span>, <span class="hljs-string">&quot;docker&quot;</span>],<br>  <span class="hljs-string">//</span> 忽略的用户/组 <span class="hljs-params">(用于保持 /etc 下文件的干净)</span><br>  <span class="hljs-string">&quot;ignore-removed-users&quot;</span>: [<span class="hljs-string">&quot;root&quot;</span>],<br>  <span class="hljs-string">&quot;ignore-removed-groups&quot;</span>: [<span class="hljs-string">&quot;root&quot;</span>],<br>  <span class="hljs-string">//</span> 检查 passwd/group 文件<br>  <span class="hljs-string">&quot;check-passwd&quot;</span>: &#123; <span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;file&quot;</span>, <span class="hljs-string">&quot;filename&quot;</span>: <span class="hljs-string">&quot;passwd&quot;</span> &#125;,<br>  <span class="hljs-string">&quot;check-group&quot;</span>: &#123; <span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;file&quot;</span>, <span class="hljs-string">&quot;filename&quot;</span>: <span class="hljs-string">&quot;group&quot;</span> &#125;,<br>  <span class="hljs-string">//</span> 需要安装的核心 RPM 包列表<br>  <span class="hljs-string">&quot;packages&quot;</span>: [<br>    <span class="hljs-string">&quot;authconfig&quot;</span>,<br>    <span class="hljs-string">&quot;ceph-common&quot;</span>,<br>    <span class="hljs-string">&quot;ceph-fuse&quot;</span>,<br>    <span class="hljs-string">&quot;chrony&quot;</span>,<br>    <span class="hljs-string">&quot;cronie&quot;</span>,<br>    <span class="hljs-string">&quot;cloud-init&quot;</span>, <span class="hljs-string">//</span> 用于云环境初始化<br>    <span class="hljs-string">&quot;cloud-utils-growpart&quot;</span>, <span class="hljs-string">//</span> 用于分区扩展<br>    <span class="hljs-string">&quot;coreutils&quot;</span>,<br>    <span class="hljs-string">&quot;conntrack-tools&quot;</span>,<br>    <span class="hljs-string">&quot;docker-ce&quot;</span>, <span class="hljs-string">//</span> 安装 Docker 运行时<br>    <span class="hljs-string">//</span> <span class="hljs-string">...</span> 其他必要的包<br>  ]<br>&#125;<br></code></pre></td></tr></table></figure>

<ul>
<li><strong>构建命令:</strong></li>
</ul>
<figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli"><span class="hljs-comment"># 使用 rpm-ostree 根据 treefile 构建 ostree commit</span><br>rpm-ostree compose tree <span class="hljs-params">--unified-core</span> <span class="hljs-params">--cachedir=cache</span> <span class="hljs-params">--repo=</span><span class="hljs-string">./build-repo</span> <span class="hljs-string">/path/to/treefile.json</span><br></code></pre></td></tr></table></figure>

<h3 id="加载-ostree"><a href="#加载-ostree" class="headerlink" title="加载 ostree"></a>加载 ostree</h3><ul>
<li><p><strong>节点初始化:</strong></p>
<ol>
<li><p><strong>初始化 OS:</strong> 在目标节点上，初始化 ostree 管理环境。</p>
<figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs gherkin">ostree admin os-init <span class="hljs-variable">&lt;osname&gt;</span><br><span class="hljs-comment"># 例如: ostree admin os-init centos-atomic-host</span><br></code></pre></td></tr></table></figure>
</li>
<li><p><strong>添加远程仓库:</strong> 添加存储 ostree commit 的 HTTP 仓库地址。</p>
<figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs gherkin">ostree remote add <span class="hljs-variable">&lt;remote_name&gt;</span> <span class="hljs-variable">&lt;repo_url&gt;</span><br><span class="hljs-comment"># 例如: ostree remote add atomic http://ostree.svr/ostree</span><br></code></pre></td></tr></table></figure>
</li>
<li><p><strong>拉取 Commit:</strong> 从远程仓库拉取指定的 ostree commit (版本)。</p>
<figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs gherkin">ostree pull <span class="hljs-variable">&lt;remote_name&gt;</span> <span class="hljs-variable">&lt;ref&gt;</span><br><span class="hljs-comment"># 例如: ostree pull atomic centos-atomic-host/8/x86_64/standard</span><br></code></pre></td></tr></table></figure>
</li>
<li><p><strong>部署 OS:</strong> 将拉取的 commit 部署为可启动的系统，并配置内核启动参数 (kargs)。</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs routeros">ostree admin deploy <span class="hljs-attribute">--os</span>=&lt;osname&gt; &lt;ref&gt; <span class="hljs-attribute">--karg</span>=<span class="hljs-string">&#x27;&lt;kernel_argument&gt;&#x27;</span><br><span class="hljs-comment"># 例如: ostree admin deploy --os=centos-atomic-host centos-atomic-host/8/x86_64/standard --karg=&#x27;root=/dev/mapper/atomicos-root&#x27;</span><br></code></pre></td></tr></table></figure></li>
</ol>
</li>
<li><p><strong>ostree 仓库结构 (示例):</strong> [Image 2: ostree 仓库目录结构示例]</p>
<ul>
<li><code>objects/</code>: 存储所有文件内容的去重块 (类似 Git 的 objects)。</li>
<li><code>refs/heads/</code>: 存储分支引用 (ref)，指向具体的 commit hash。</li>
<li><code>repo/config</code>: 仓库配置文件。</li>
</ul>
</li>
</ul>
<h3 id="操作系统加载方式"><a href="#操作系统加载方式" class="headerlink" title="操作系统加载方式"></a>操作系统加载方式</h3><ul>
<li><strong>物理机 (Bare Metal):</strong><ul>
<li>通常使用网络引导 (PXE Boot) 结合自动化部署工具，如 <strong>Foreman</strong>。</li>
<li>PXE 服务器提供引导程序，加载 <strong>Kickstart</strong> (Red Hat&#x2F;CentOS&#x2F;Fedora) 或 <strong>Preseed</strong> (Debian&#x2F;Ubuntu) 自动化安装脚本。</li>
<li>Kickstart&#x2F;Preseed 脚本中包含执行 <code>ostree admin deploy</code> 的命令，从 ostree 仓库完成操作系统的部署。</li>
</ul>
</li>
<li><strong>虚拟机 (Virtual Machine):</strong><ul>
<li>需要使用镜像构建工具 (如 <strong>Packer</strong>) 将 ostree commit 打包成特定虚拟化平台支持的镜像格式 (如 QCOW2 for KVM&#x2F;OpenStack, VHD for Hyper-V, VMDK for VMware, RAW)。</li>
<li>然后通过虚拟化管理平台 (如 OpenStack Glance, vCenter) 上传和部署这些镜像。</li>
</ul>
</li>
</ul>
<h3 id="生产环境陷阱与定制化参数"><a href="#生产环境陷阱与定制化参数" class="headerlink" title="生产环境陷阱与定制化参数"></a>生产环境陷阱与定制化参数</h3><p>在生产环境中使用特定操作系统或配置时，可能会遇到各种问题：</p>
<p><strong>遇到的陷阱示例:</strong></p>
<ul>
<li><strong>cloud-init Bug (0.7.7):</strong> 在某些版本中，cloud-init 可能无法正确应用静态网络配置，导致节点初始化失败或网络不通。</li>
<li><strong>Docker</strong> Bug <strong>(1.9.1):</strong> 旧版本 Docker 在处理快速输出的大量日志时可能存在内存泄漏问题。</li>
<li><strong>Kernel</strong> Panic (4.4.6): 特定内核版本在 Cgroup 创建和销毁操作频繁时可能触发内核崩溃 (Kernel Panic)。</li>
<li><strong>rootfs 分区过小:</strong><ul>
<li>根分区 (<code>/</code>) 空间不足会导致各种问题，例如 Docker 守护进程无法启动、无法写入日志、无法创建新容器等。</li>
<li><strong>导致占满的原因:</strong><ul>
<li>CI&#x2F;CD 过程中的构建工具（如 Maven）可能将大量依赖下载到临时目录 (<code>/tmp</code>)，如果 <code>/tmp</code> 在根分区，可能耗尽空间。</li>
<li>容器日志增长过快，超出了日志轮转 (log rotation) 的处理能力，导致单个日志文件或日志总量撑爆磁盘。 <strong>解决方案:</strong> 合理配置容器日志驱动（如 <code>json-file</code> 的 <code>max-size</code>, <code>max-file</code>），或使用集中的日志收集方案。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>需要定制化的操作系统参数:</strong></p>
<ul>
<li><strong>背景:</strong> 某些应用（如 Elasticsearch, Ceph）对操作系统的默认参数有特定要求。</li>
<li><strong>示例:</strong> Elasticsearch 要求 <code>vm.max_map_count</code> 内核参数至少为 <code>262144</code>，而大多数 Linux 发行版默认值可能只有 <code>65530</code>。</li>
<li><strong>解决方案:</strong> 需要在操作系统镜像构建阶段或节点初始化阶段（如通过 cloud-init, Ignition, 或配置管理工具）就将这些必要的参数配置好，确保节点启动后即满足应用需求。</li>
</ul>
<h2 id="3-节点资源管理"><a href="#3-节点资源管理" class="headerlink" title="3. 节点资源管理"></a>3. 节点资源管理</h2><p>在 Kubernetes 集群中，<strong>节点 (Node)</strong> 是运行工作负载 (Pod) 的基本单元。对节点的资源进行有效管理是确保集群稳定性、性能和资源利用率的关键。生产环境中的节点资源管理需要考虑多个维度，包括节点自身的健康状态、为系统组件预留资源、防止资源耗尽的机制以及如何精确地为容器分配和限制资源。本章将深入探讨这些方面。</p>
<h3 id="NUMA-Node-Non-Uniform-Memory-Access"><a href="#NUMA-Node-Non-Uniform-Memory-Access" class="headerlink" title="NUMA Node (Non-Uniform Memory Access)"></a>NUMA Node (Non-Uniform Memory Access)</h3><p>在现代多处理器服务器架构中，<strong>NUMA (Non-Uniform Memory Access)</strong> 是一个重要的内存设计。其核心思想是，处理器访问本地内存（直接连接到该处理器的内存）的速度要快于访问远程内存（连接到其他处理器的内存）。如下图所示，一个系统可能包含多个 NUMA Node，每个 Node 包含若干 CPU 核心和本地内存。</p>
<img src="/2025/04/24/kubernetes%E7%94%9F%E4%BA%A7%E5%8C%96%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/image-20250426234348146.png" srcset="/img/loading.gif" lazyload class="" title="image-20250426234348146">

<ul>
<li><strong>意义</strong>: 当一个 Pod 内的进程运行时，如果其使用的 CPU 核心和内存分布在不同的 NUMA Node 上，就会发生跨 NUMA Node 的内存访问，导致性能下降。对于需要高性能、低延迟的应用（如数据库、DPDK 应用、高性能计算），NUMA 亲和性至关重要。</li>
<li><strong>Kubernetes 处理</strong>: Kubernetes <strong>Kubelet</strong> 默认情况下并不保证 Pod 内的 CPU 和内存分配在同一个 NUMA Node 上。然而，通过 <strong>Topology Manager</strong>（Kubelet 的一个特性门控功能），可以实现更精细的资源拓扑感知调度。当设置为 <code>single-numa-node</code> 策略时，对于 <strong>Guaranteed QoS</strong> 的 Pod，Kubelet 会尝试将所有分配的 CPU 核心和内存都限制在同一个 NUMA Node 内，从而提升性能。这通常需要与 <strong>CPU Manager</strong> (<code>static</code> 策略) 和 <strong>Memory Manager</strong> (<code>static</code> 策略，较新版本支持) 配合使用。</li>
</ul>
<h3 id="状态上报-Status-Reporting"><a href="#状态上报-Status-Reporting" class="headerlink" title="状态上报 (Status Reporting)"></a>状态上报 (Status Reporting)</h3><p><strong>Kubelet</strong> 作为运行在每个节点上的代理，承担着监控节点状态并将信息汇报给 <strong>API Server</strong> 的核心职责。这些信息对于集群的调度决策和健康状况判断至关重要。</p>
<ul>
<li><strong>汇报内容</strong>:<ul>
<li><strong>节点基础信息</strong>: 包括节点的 IP 地址、主机名、操作系统类型与版本、Linux 内核版本、容器运行时（Docker&#x2F;Containerd）版本、Kubelet 版本、Kube-proxy 版本等。</li>
<li><strong>节点资源信息</strong>: 包括 <strong>CPU 总量</strong>、<strong>内存总量</strong>、<strong>HugePages 大页内存</strong>、<strong>临时存储 (Ephemeral Storage)</strong>、可用的 <strong>GPU</strong> 等硬件设备信息，以及这些资源中可以分配给 Pod 使用的部分 (Allocatable Resources)。</li>
<li><strong>节点状态 (Conditions)</strong>: Kubelet 会定期更新一系列反映节点当前状况的状态条件。这些 Conditions 是调度器（kube-scheduler）和控制器（如 Node Controller）判断节点是否可用、是否处于压力状态的关键依据。</li>
</ul>
</li>
<li><strong>常见的节点 Conditions 及其意义</strong>:<table>
<thead>
<tr>
<th align="left">状态 (Condition Type)</th>
<th align="left">状态的意义</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>Ready</strong></td>
<td align="left">节点是否健康并准备好接受 Pod 调度。<code>True</code> 表示健康，<code>False</code> 表示不健康，<code>Unknown</code> 表示节点控制器在一段时间内未收到 Kubelet 的心跳。</td>
</tr>
<tr>
<td align="left"><strong>MemoryPressure</strong></td>
<td align="left">节点是否存在内存资源压力。<code>True</code> 表示内存紧张。</td>
</tr>
<tr>
<td align="left"><strong>PIDPressure</strong></td>
<td align="left">节点上是否存在进程 ID (PID) 资源压力，即进程数量过多。<code>True</code> 表示 PID 紧张。</td>
</tr>
<tr>
<td align="left"><strong>DiskPressure</strong></td>
<td align="left">节点是否存在磁盘空间压力（通常指 Kubelet 使用的根分区或镜像分区）。<code>True</code> 表示磁盘空间紧张。</td>
</tr>
<tr>
<td align="left"><strong>NetworkUnavailable</strong></td>
<td align="left">节点的网络配置是否尚未正确设置或存在问题。<code>True</code> 表示网络不可用。</td>
</tr>
</tbody></table>
</li>
<li><strong>调度决策</strong>: <strong>kube-scheduler</strong> 在为 Pod 选择目标节点时，会过滤掉 <code>Ready</code> 状态不为 <code>True</code> 的节点。同时，如果节点存在 <code>MemoryPressure</code>、<code>DiskPressure</code> 或 <code>PIDPressure</code> 状态，调度器默认会给节点添加相应的 <strong>Taint (污点)</strong>，阻止新的 Pod（除非 Pod 能容忍这些 Taint）被调度到该节点上，这是一种保护机制，避免节点资源进一步耗尽。</li>
</ul>
<h3 id="Lease-对象-Lease-Objects"><a href="#Lease-对象-Lease-Objects" class="headerlink" title="Lease 对象 (Lease Objects)"></a>Lease 对象 (Lease Objects)</h3><p>在早期 Kubernetes 版本中，Kubelet 通过频繁更新其对应的 <strong>Node 对象</strong> 来向 API Server 汇报心跳，表明自身存活。然而，Node 对象通常很大，包含了大量的静态信息和状态，频繁更新会对 API Server 和 etcd 造成显著压力，尤其是在大规模集群中。<br>为了解决这个问题，Kubernetes 引入了 <strong>Lease 对象</strong> (位于 <code>coordination.k8s.io</code> API 组)。</p>
<ul>
<li><strong>作用</strong>: Lease 对象是一种轻量级的资源，专门用于节点心跳。每个节点在 <code>kube-node-lease</code> 命名空间下维护一个对应的 Lease 对象。Kubelet 会定期更新这个 Lease 对象的 <code>renewTime</code> 字段。</li>
<li><strong>机制</strong>: <strong>Node Controller</strong> 会监控 Lease 对象。如果一个 Lease 对象在 <code>nodeLeaseDurationSeconds</code>（默认 40 秒）内没有被更新，Node Controller 就会认为该节点失联，并将节点的 <code>Ready</code> Condition 更新为 <code>Unknown</code>。这大大降低了心跳机制对 API Server 和 etcd 的负载。</li>
<li><strong>配置示例 (Lease Object YAML)</strong>:<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">coordination.k8s.io/v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Lease</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">creationTimestamp:</span> <span class="hljs-string">&quot;2021-08-19T02:50:09Z&quot;</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">k8snode</span> <span class="hljs-comment"># 通常是节点名</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-node-lease</span><br>  <span class="hljs-attr">ownerReferences:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br>    <span class="hljs-attr">kind:</span> <span class="hljs-string">Node</span><br>    <span class="hljs-attr">name:</span> <span class="hljs-string">k8snode</span><br>    <span class="hljs-attr">uid:</span> <span class="hljs-number">58679942</span><span class="hljs-string">-e2dd-4ead-aada-385f099d5f56</span><br>  <span class="hljs-attr">resourceVersion:</span> <span class="hljs-string">&quot;1293702&quot;</span><br>  <span class="hljs-attr">uid:</span> <span class="hljs-string">1bf51951-b832-49da-8708-4b224b1ec3ed</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">holderIdentity:</span> <span class="hljs-string">k8snode</span> <span class="hljs-comment"># 持有者标识，通常是节点名</span><br>  <span class="hljs-attr">leaseDurationSeconds:</span> <span class="hljs-number">40</span> <span class="hljs-comment"># 租约持续时间</span><br>  <span class="hljs-attr">renewTime:</span> <span class="hljs-string">&quot;2021-09-08T01:34:16.489589Z&quot;</span> <span class="hljs-comment"># 上次续约时间</span><br></code></pre></td></tr></table></figure></li>
</ul>
<h3 id="资源预留-Resource-Reservation"><a href="#资源预留-Resource-Reservation" class="headerlink" title="资源预留 (Resource Reservation)"></a>资源预留 (Resource Reservation)</h3><p>Kubernetes 节点上除了运行用户的 Pod 外，还必须运行许多支撑系统运行的基础服务。这些服务包括 <strong>操作系统本身的守护进程</strong> (如 systemd, journald, sshd)，<strong>容器运行时</strong> (如 dockerd, containerd)，以及 <strong>Kubernetes 自身的组件</strong> (如 kubelet, kube-proxy)。</p>
<ul>
<li><strong>问题</strong>: 如果不为这些系统关键进程预留资源，当用户 Pod 消耗过多资源时，可能导致系统进程因资源不足而运行缓慢甚至崩溃，进而影响整个节点的稳定性和功能。</li>
<li><strong>解决方案</strong>: Kubelet 提供了参数来为这些非 Pod 进程预留一部分节点资源。这些资源将从节点总资源中扣除，不会被计入可供 Pod 分配的资源 (Allocatable)。<ul>
<li><code>--kube-reserved=[cpu=100m,memory=256Mi,ephemeral-storage=1Gi,pid=1000]</code>: 为 Kubernetes 系统组件 (kubelet, kube-proxy, 容器运行时等) 预留资源。</li>
<li><code>--system-reserved=[cpu=100m,memory=256Mi,ephemeral-storage=1Gi,pid=1000]</code>: 为操作系统级别的系统守护进程 (systemd, journald, sshd 等) 预留资源。</li>
<li><code>--reserved-cpus=0,1</code>: （CPU Manager static 策略下）显式保留某些物理 CPU 核心，不用于 Pod 分配，专门给系统或 Kubelet 进程使用。</li>
</ul>
</li>
<li><strong>配置</strong>: 这些参数通常在 Kubelet 的启动配置文件 (如 <code>/etc/kubernetes/kubelet.conf</code> 或 systemd service unit 文件) 中设置。合理的预留值需要根据节点的实际负载和运行的系统服务来确定，通常需要进行测试和调整。</li>
</ul>
<h3 id="Capacity-与-Allocatable"><a href="#Capacity-与-Allocatable" class="headerlink" title="Capacity 与 Allocatable"></a>Capacity 与 Allocatable</h3><p>理解节点的 <strong>Capacity (总容量)</strong> 和 <strong>Allocatable (可分配容量)</strong> 对于资源管理和调度至关重要。</p>
<ul>
<li><p><strong>Capacity</strong>: 指 Kubelet 检测到的节点硬件资源总量。</p>
<ul>
<li><strong>CPU</strong>: 通常来源于 Linux <code>/proc/cpuinfo</code> 文件，表示逻辑 CPU 核心数。</li>
<li><strong>Memory</strong>: 通常来源于 Linux <code>/proc/meminfo</code> 文件，表示节点总物理内存。</li>
<li><strong>Ephemeral Storage</strong>: 指节点根分区 (<code>/</code>) 或者 Kubelet 配置的用于存储 Pod 日志、<code>emptyDir</code> 卷等的临时存储分区的总大小。</li>
</ul>
</li>
<li><p><strong>Allocatable</strong>: 指节点上实际可供用户 Pod 申请和使用的资源量。它是通过从 <strong>Capacity</strong> 中减去为系统预留的资源以及 Kubelet 驱逐阈值所保留的资源得到的。<br><strong><code>Allocatable = Capacity - KubeReserved - SystemReserved - Eviction Thresholds</code></strong></p>
</li>
<li><p><strong>调度依据</strong>: <strong>kube-scheduler 只关心节点的 Allocatable 资源</strong>。当调度 Pod 时，它会检查节点的 Allocatable 资源是否满足 Pod 的 <code>requests</code>。</p>
</li>
<li><p><strong>查看</strong>: 可以通过 <code>kubectl describe node &lt;node-name&gt;</code> 命令查看节点的 Capacity 和 Allocatable 信息。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 示例输出片段</span><br>Capacity:<br>  cpu:                24<br>  ephemeral-storage:  205838Mi<br>  hugepages-1Gi:      0<br>  hugepages-2Mi:      0<br>  memory:             179504088Ki <span class="hljs-comment"># 约 171 GiB</span><br>  pods:               110<br>Allocatable:<br>  cpu:                24<br>  ephemeral-storage:  205838Mi <span class="hljs-comment"># 示例中未配置预留或驱逐阈值</span><br>  hugepages-1Gi:      0<br>  hugepages-2Mi:      0<br>  memory:             177304536Ki <span class="hljs-comment"># 约 169 GiB (Capacity - 预留/驱逐)</span><br>  pods:               110<br></code></pre></td></tr></table></figure></li>
</ul>
<h3 id="节点磁盘管理-Node-Disk-Management"><a href="#节点磁盘管理-Node-Disk-Management" class="headerlink" title="节点磁盘管理 (Node Disk Management)"></a>节点磁盘管理 (Node Disk Management)</h3><p>Kubelet 会监控节点上与 Pod 运行相关的两个主要文件系统（如果存在的话），以防止磁盘耗尽导致的问题。</p>
<ul>
<li><strong><code>nodefs</code></strong>: 通常指包含 Kubelet 工作目录（默认为 <code>/var/lib/kubelet</code>）的文件系统，也可能是节点的根文件系统 (<code>/</code>)。这个分区用于存储 Pod 的 <strong><code>emptyDir</code> 卷</strong>、<strong>容器日志</strong>、<strong>镜像层（如果 <code>imagefs</code> 不独立）</strong>、以及 Kubelet 自身的一些数据。</li>
<li><strong><code>imagefs</code></strong>: （可选）指容器运行时（如 Docker, Containerd）专门用于存储<strong>容器镜像</strong>和<strong>容器可写层 (writable layers)</strong> 的文件系统。如果配置了独立的 <code>imagefs</code>，它可以减轻 <code>nodefs</code> 的压力。如果未配置，镜像和可写层通常会存储在 <code>nodefs</code> 上（例如 Docker 默认的 <code>/var/lib/docker</code> 可能与 <code>/var/lib/kubelet</code> 在同一分区）。<br>Kubelet 会监控这两个文件系统的可用空间和 inode 数量，并在资源紧张时触发 <strong>DiskPressure</strong> 状态和可能的驱逐操作。</li>
</ul>
<h3 id="驱逐管理-Eviction-Management"><a href="#驱逐管理-Eviction-Management" class="headerlink" title="驱逐管理 (Eviction Management)"></a>驱逐管理 (Eviction Management)</h3><p><strong>驱逐 (Eviction)</strong> 是 Kubelet 在节点资源不足时，为了保证节点稳定性而主动停止（驱逐）一个或多个 Pod 的过程。这是 Kubernetes <strong>自愈能力</strong> 的体现，防止节点因资源耗尽而完全崩溃。</p>
<ul>
<li><strong>触发</strong>: 当节点的可用内存、磁盘空间、可用 inode 或可用 PID 低于 Kubelet 配置的<strong>驱逐阈值 (Eviction Thresholds)</strong> 时，驱逐机制会被触发。</li>
<li><strong>行为</strong>:<ul>
<li>Kubelet <strong>不会直接删除 Pod 对象</strong>。它会终止 Pod 内的容器进程。</li>
<li>被驱逐的 Pod 的 <code>status.phase</code> 会被标记为 <strong><code>Failed</code></strong>。</li>
<li>Pod 的 <code>status.reason</code> 会被设置为 <strong><code>Evicted</code></strong>。</li>
<li>Pod 的 <code>status.message</code> 会记录被驱逐的具体原因（例如 “Node tenia disk pressure: …”）。</li>
</ul>
</li>
<li><strong>重要性</strong>: 驱逐是一种 <strong>防御机制</strong>，旨在牺牲部分 Pod 以保证节点及其上运行的其他关键 Pod 和系统服务的存活。</li>
</ul>
<h3 id="资源可用额监控-Available-Resource-Monitoring"><a href="#资源可用额监控-Available-Resource-Monitoring" class="headerlink" title="资源可用额监控 (Available Resource Monitoring)"></a>资源可用额监控 (Available Resource Monitoring)</h3><p>Kubelet 需要持续监控节点的可用资源，以便及时发现资源压力并做出响应（如设置 Condition 或触发驱逐）。</p>
<ul>
<li><strong>数据来源</strong>: Kubelet 内部集成了 <strong>cAdvisor (Container Advisor)</strong> 组件，用于收集节点和容器的资源使用情况。</li>
<li><strong>监控指标</strong>: Kubelet 主要关注<strong>不可压缩资源 (Incompressible Resources)</strong> 的可用量，因为这些资源的耗尽会直接导致系统或应用失败。CPU 是可压缩资源，CPU 繁忙通常只会导致性能下降，一般不直接触发驱逐（除非配置了特定策略）。<ul>
<li><strong><code>memory.available</code></strong>: 节点当前可用的内存量 (通常基于 <code>/proc/meminfo</code> 中的 <code>MemAvailable</code>)。</li>
<li><strong><code>nodefs.available</code></strong>: <code>nodefs</code> 文件系统的可用磁盘空间。</li>
<li><strong><code>nodefs.inodesFree</code></strong>: <code>nodefs</code> 文件系统的可用 inode 数量。</li>
<li><strong><code>imagefs.available</code></strong>: （如果存在）<code>imagefs</code> 文件系统的可用磁盘空间。</li>
<li><strong><code>imagefs.inodesFree</code></strong>: （如果存在）<code>imagefs</code> 文件系统的可用 inode 数量。</li>
</ul>
</li>
<li><strong>检查周期</strong>: Kubelet 会定期（默认 10 秒）检查这些指标。</li>
</ul>
<h3 id="驱逐策略-Eviction-Policy"><a href="#驱逐策略-Eviction-Policy" class="headerlink" title="驱逐策略 (Eviction Policy)"></a>驱逐策略 (Eviction Policy)</h3><p>Kubelet 根据预先配置的驱逐策略来决定何时以及如何驱逐 Pod。</p>
<ul>
<li><strong>驱逐阈值 (Eviction Thresholds)</strong>: 定义了触发驱逐的资源条件。阈值可以是绝对值（如 <code>memory.available&lt;100Mi</code>）或百分比（如 <code>nodefs.available&lt;10%</code>）。<ul>
<li><strong>硬驱逐 (Hard Eviction)</strong>: 当资源可用量低于硬驱逐阈值时，Kubelet <strong>立即</strong> 开始驱逐 Pod，<strong>没有宽限期 (Grace Period)</strong>。这是为了尽快回收资源，防止节点崩溃。<ul>
<li>配置示例: <code>--eviction-hard=memory.available&lt;100Mi,nodefs.available&lt;5%,nodefs.inodesFree&lt;5%</code></li>
</ul>
</li>
<li><strong>软驱逐 (Soft Eviction)</strong>: 当资源可用量低于软驱逐阈值，并且持续时间超过了指定的<strong>宽限期 (Grace Period)</strong> 后，Kubelet 才开始驱逐 Pod。这提供了一个缓冲期，让系统或 Pod 有机会自行恢复或被优雅地移除。<ul>
<li>配置示例: <code>--eviction-soft=memory.available&lt;200Mi,nodefs.available&lt;10%</code></li>
<li>宽限期配置: <code>--eviction-soft-grace-period=memory.available=1m30s,nodefs.available=5m</code> (指定特定信号的宽限期)</li>
<li>Pod 终止宽限期: 软驱逐还会考虑 Pod 自身的 <code>terminationGracePeriodSeconds</code>，取两者中的较小值。</li>
</ul>
</li>
</ul>
</li>
<li><strong>最小回收量 (<code>evictionMinimumReclaim</code>)</strong>: 可以配置 Kubelet 在每次驱逐操作后尝试回收的最小资源量，以避免因阈值波动导致过于频繁的小规模驱逐。<ul>
<li>配置示例: <code>--eviction-minimum-reclaim=memory.available=0Mi,nodefs.available=500Mi,imagefs.available=2Gi</code></li>
</ul>
</li>
<li><strong>驱逐策略表</strong>:<table>
<thead>
<tr>
<th align="left">Kubelet 参数</th>
<th align="left">分类</th>
<th align="left">驱逐方式</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>evictionSoft</code></td>
<td align="left">软驱逐</td>
<td align="left">当检测到当前资源达到软驱逐的阈值时，并不会立即启动驱逐操作，而是要等待一个 <strong>宽限期 (Grace Period)</strong>。这个宽限期选取 <code>evictionSoftGracePeriod</code> 和 Pod 指定的 <code>terminationGracePeriodSeconds</code> 中较小的值。</td>
</tr>
<tr>
<td align="left"><code>evictionHard</code></td>
<td align="left">硬驱逐</td>
<td align="left"><strong>没有宽限期</strong>，一旦检测到满足硬驱逐的条件，就直接中止 Pod 来释放资源。</td>
</tr>
</tbody></table>
</li>
</ul>
<h3 id="基于内存压力的驱逐-Eviction-based-on-Memory-Pressure"><a href="#基于内存压力的驱逐-Eviction-based-on-Memory-Pressure" class="headerlink" title="基于内存压力的驱逐 (Eviction based on Memory Pressure)"></a>基于内存压力的驱逐 (Eviction based on Memory Pressure)</h3><p>当 <code>memory.available</code> 低于设定的驱逐阈值（默认硬驱逐是 <code>memory.available&lt;100Mi</code>）时：</p>
<ol>
<li><strong>设置 Condition</strong>: Kubelet 会将节点的 <code>MemoryPressure</code> Condition 设置为 <code>True</code>。</li>
<li><strong>调度器反应</strong>: 调度器会避免向该节点调度新的 BestEffort Pod（通过 Taint）。</li>
<li><strong>启动驱逐</strong>: Kubelet 开始驱逐 Pod 以回收内存。</li>
<li><strong>驱逐顺序</strong>:<ul>
<li><strong>(候选)</strong>: 首先判断 Pod 的<strong>内存使用量是否超出了其 <code>requests</code></strong>。超出请求的 Pod 成为主要的驱逐候选目标。如果所有 Pod 都在其请求范围内，那么所有 Pod 都可能被驱逐。</li>
<li><strong>(优先级)</strong>: 按照 Pod 的 <strong>服务质量 (QoS) 等级</strong> 和 <strong>运行时优先级 (Priority)</strong> 进行排序。优先级最低的先被驱逐：<ol>
<li><strong>BestEffort</strong> QoS 的 Pod (没有设置 requests 和 limits)。</li>
<li><strong>Burstable</strong> QoS 的 Pod，且内存使用量超出了其 <code>requests</code>。</li>
<li><strong>Guaranteed</strong> QoS 的 Pod，以及内存使用量未超出 <code>requests</code> 的 Burstable Pod（理论上不应发生，除非系统预留不足或计算错误，但在极端情况下可能被驱逐）。</li>
</ol>
</li>
<li><strong>(内存使用量)</strong>: 在相同优先级&#x2F;QoS 等级内，<strong>当前内存使用量超出其 <code>requests</code> 值. 最多. 的 Pod</strong> 会被优先驱逐。</li>
</ul>
</li>
</ol>
<h3 id="基于磁盘压力的驱逐-Eviction-based-on-Disk-Pressure"><a href="#基于磁盘压力的驱逐-Eviction-based-on-Disk-Pressure" class="headerlink" title="基于磁盘压力的驱逐 (Eviction based on Disk Pressure)"></a>基于磁盘压力的驱逐 (Eviction based on Disk Pressure)</h3><p>当 <code>nodefs</code> 或 <code>imagefs</code> 的可用空间或 inode 低于设定的驱逐阈值时：</p>
<ol>
<li><strong>设置 Condition</strong>: Kubelet 会将节点的 <code>DiskPressure</code> Condition 设置为 <code>True</code>。</li>
<li><strong>调度器反应</strong>: 调度器会避免向该节点调度新的 Pod（通过 Taint）。</li>
<li><strong>启动驱逐&#x2F;清理</strong>: Kubelet 开始回收磁盘空间。</li>
<li><strong>回收行为</strong>:<ul>
<li><strong>第一阶段：清理容器和镜像 (如果适用)</strong><ul>
<li><strong>有独立 <code>imagefs</code> 分区</strong>:<ul>
<li>Kubelet 首先删除<strong>已退出 (dead) 的容器</strong> (清理 <code>nodefs</code> 上的日志和可写层残留)。</li>
<li>然后 Kubelet 删除<strong>未被使用的镜像</strong> (清理 <code>imagefs</code>)。</li>
</ul>
</li>
<li><strong>无独立 <code>imagefs</code> 分区 (共享 <code>nodefs</code>)</strong>:<ul>
<li>Kubelet 同时删除<strong>已退出的容器</strong>和<strong>未被使用的镜像</strong>。</li>
</ul>
</li>
</ul>
</li>
<li><strong>第二阶段：驱逐运行中的 Pod (如果第一阶段后仍满足驱逐条件)</strong><ul>
<li><strong>(候选)</strong>: 判断 Pod 的<strong>本地临时存储 (ephemeral-storage) 使用量是否超出了其 <code>requests</code></strong>。超出请求的 Pod 成为主要驱逐候选目标。</li>
<li><strong>(优先级)</strong>: 同样按照 Pod 的 QoS 等级和运行时优先级排序（BestEffort -&gt; Burstable -&gt; Guaranteed）。</li>
<li><strong>(磁盘使用量)</strong>: 在相同优先级&#x2F;QoS 等级内，<strong>当前本地临时存储使用量超出其 <code>requests</code> 值最多的 Pod</strong> 会被优先驱逐。</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="容器和资源配置-Cgroup"><a href="#容器和资源配置-Cgroup" class="headerlink" title="容器和资源配置 (Cgroup)"></a>容器和资源配置 (Cgroup)</h3><p>Kubernetes 使用 <strong>Linux Cgroups (Control Groups)</strong> 来限制和隔离 Pod 及容器的资源使用。Kubelet 会根据 Pod 的 <strong>QoS (Quality of Service) Class</strong> 将 Pod 组织在不同的 Cgroup 层级下。</p>
<ul>
<li><strong>QoS Classes</strong>:<ul>
<li><strong>Guaranteed</strong>: Pod 中所有容器都必须同时设置了 CPU 和 Memory 的 <code>requests</code> 和 <code>limits</code>，并且 <code>requests</code> 值必须等于 <code>limits</code> 值。</li>
<li><strong>Burstable</strong>: Pod 中至少有一个容器设置了 CPU 或 Memory 的 <code>requests</code>，但不满足 Guaranteed 的条件（例如 <code>requests</code> &lt; <code>limits</code>，或只有部分容器设置了资源）。</li>
<li><strong>BestEffort</strong>: Pod 中所有容器都没有设置 CPU 或 Memory 的 <code>requests</code> 和 <code>limits</code>。</li>
</ul>
</li>
<li><strong>Cgroup Hierarchy (示例 for CPU)</strong>: Kubelet 通常会在 <code>/sys/fs/cgroup/cpu/kubepods.slice/</code> (或其他配置的 Cgroup Root 下) 创建层级结构：<ul>
<li><code>kubepods-besteffort.slice</code>: 用于 BestEffort Pod。</li>
<li><code>kubepods-burstable.slice</code>: 用于 Burstable Pod。</li>
<li><code>kubepods-pod&lt;PodUID&gt;.slice</code>: 每个 Guaranteed 和 Burstable Pod 拥有自己的 Cgroup Slice。</li>
<li><code>docker-&lt;ContainerID&gt;.scope</code> (或 <code>crio-...</code>): 每个容器在 Pod 的 Slice 下有自己的 Cgroup Scope。</li>
</ul>
<figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli"><span class="hljs-string">/sys/fs/cgroup/cpu</span><br>└── kubepods.slice<br>    ├── kubepods-besteffort.slice<br>    │   └── kubepods-pod&lt;PodUID_A&gt;<span class="hljs-string">.slice</span><br>    │       ├── docker-&lt;ContainerID_A1&gt;<span class="hljs-string">.scope</span><br>    │       └── docker-&lt;ContainerID_A2&gt;<span class="hljs-string">.scope</span><br>    ├── kubepods-burstable.slice<br>    │   └── kubepods-pod&lt;PodUID_B&gt;<span class="hljs-string">.slice</span><br>    │       ├── docker-&lt;ContainerID_B1&gt;<span class="hljs-string">.scope</span><br>    │       └── docker-&lt;ContainerID_B2&gt;<span class="hljs-string">.scope</span><br>    └── kubepods-pod&lt;PodUID_C&gt;<span class="hljs-string">.slice</span>  <span class="hljs-comment"># Guaranteed Pod directly under kubepods</span><br>        ├── docker-&lt;ContainerID_C1&gt;<span class="hljs-string">.scope</span><br>        └── docker-&lt;ContainerID_C2&gt;<span class="hljs-string">.scope</span><br></code></pre></td></tr></table></figure>
<p><em>(注意: 实际层级可能因 Kubelet Cgroup driver (systemd&#x2F;cgroupfs) 和版本略有不同，上图为简化示意)</em></p>
</li>
<li><strong>Cgroup Hierarchy (示例 for Memory)</strong>: 内存 Cgroup 结构类似。<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs stylus">/sys/fs/cgroup/memory<br>└── kubepods<span class="hljs-selector-class">.slice</span><br>    ├── kubepods-besteffort<span class="hljs-selector-class">.slice</span><br>    │   └── kubepods-pod&lt;PodUID_A&gt;<span class="hljs-selector-class">.slice</span><br>    │       ├── docker-&lt;ContainerID_A1&gt;<span class="hljs-selector-class">.scope</span><br>    │       └── docker-&lt;ContainerID_A2&gt;<span class="hljs-selector-class">.scope</span><br>    ├── kubepods-burstable<span class="hljs-selector-class">.slice</span><br>    │   └── kubepods-pod&lt;PodUID_B&gt;<span class="hljs-selector-class">.slice</span><br>    │       ├── docker-&lt;ContainerID_B1&gt;<span class="hljs-selector-class">.scope</span><br>    │       └── docker-&lt;ContainerID_B2&gt;<span class="hljs-selector-class">.scope</span><br>    └── kubepods-pod&lt;PodUID_C&gt;<span class="hljs-selector-class">.slice</span><br>        ├── docker-&lt;ContainerID_C1&gt;<span class="hljs-selector-class">.scope</span><br>        └── docker-&lt;ContainerID_C2&gt;.scope<br></code></pre></td></tr></table></figure></li>
</ul>
<h3 id="CPU-CGroup-配置"><a href="#CPU-CGroup-配置" class="headerlink" title="CPU CGroup 配置"></a>CPU CGroup 配置</h3><p>Kubelet 会将 Pod Spec 中定义的 CPU <code>requests</code> 和 <code>limits</code> 转换为对应的 Cgroup 参数。</p>
<table>
<thead>
<tr>
<th align="left">CGroup 类型</th>
<th align="left">参数</th>
<th align="left">QoS 类型</th>
<th align="left">值 (示例)</th>
<th align="left">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>容器的 CGroup</strong></td>
<td align="left"><strong><code>cpu.shares</code></strong></td>
<td align="left">BestEffort</td>
<td align="left"><code>2</code></td>
<td align="left">权重最低，在 CPU 资源竞争时获得最少时间片。</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"></td>
<td align="left">Burstable</td>
<td align="left"><code>requests.cpu * 1024</code> (转换为 millicores * 1.024，近似等比)</td>
<td align="left">权重根据 <code>requests</code> 按比例分配。<code>requests</code> 越高，权重越大。</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"></td>
<td align="left">Guaranteed</td>
<td align="left"><code>requests.cpu * 1024</code></td>
<td align="left">同 Burstable，权重根据 <code>requests</code> (等于 <code>limits</code>) 分配。</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><strong><code>cpu.cfs_quota_us</code></strong></td>
<td align="left">BestEffort</td>
<td align="left"><code>-1</code></td>
<td align="left"><code>-1</code> 表示无限制，容器可以使用节点空闲的 CPU 资源，但权重最低。</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"></td>
<td align="left">Burstable</td>
<td align="left"><code>limits.cpu * 100000</code> (假定 period&#x3D;100ms)</td>
<td align="left">在每个调度周期 (cfs_period_us, 通常 100ms) 内，最多使用 <code>limits.cpu</code> 对应的 CPU 时间。如果未设置 limit，则为 <code>-1</code> (无限制)。</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"></td>
<td align="left">Guaranteed</td>
<td align="left"><code>limits.cpu * 100000</code></td>
<td align="left">同 Burstable，严格限制 CPU 使用不超过 <code>limits.cpu</code>。</td>
</tr>
<tr>
<td align="left"><strong>Pod 的 CGroup</strong></td>
<td align="left"><strong><code>cpu.shares</code></strong></td>
<td align="left">BestEffort</td>
<td align="left"><code>2</code></td>
<td align="left">Pod 整体权重最低。</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"></td>
<td align="left">Burstable</td>
<td align="left">Pod 所有容器 <code>(requests.cpu * 1024)</code> 之和</td>
<td align="left">Pod 整体权重是其下所有容器 <code>requests</code> 对应权重之和。</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"></td>
<td align="left">Guaranteed</td>
<td align="left">Pod 所有容器 <code>(requests.cpu * 1024)</code> 之和</td>
<td align="left">同 Burstable。</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><strong><code>cpu.cfs_quota_us</code></strong></td>
<td align="left">BestEffort</td>
<td align="left"><code>-1</code></td>
<td align="left">Pod 整体无 CPU 硬限制。</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"></td>
<td align="left">Burstable</td>
<td align="left">Pod 所有容器 <code>(limits.cpu * 100000)</code> 之和 (如果容器设置了 limit)</td>
<td align="left">Pod 整体的 CPU 硬限制是其下所有设置了 <code>limits</code> 的容器的 CPU limit 之和。</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"></td>
<td align="left">Guaranteed</td>
<td align="left">Pod 所有容器 <code>(limits.cpu * 100000)</code> 之和</td>
<td align="left">Pod 整体严格限制 CPU 使用，等于其下所有容器的 limit 之和。</td>
</tr>
</tbody></table>
<h3 id="内存-CGroup-配置"><a href="#内存-CGroup-配置" class="headerlink" title="内存 CGroup 配置"></a>内存 CGroup 配置</h3><p>Kubelet 会将 Pod Spec 中定义的 Memory <code>limits</code> 转换为内存 Cgroup 的 <code>memory.limit_in_bytes</code> 参数。Memory <code>requests</code> 主要用于调度和计算 <code>oom_score_adj</code>，不直接设置 Cgroup 参数（但 Memory Manager 静态策略下会利用 request）。</p>
<table>
<thead>
<tr>
<th align="left">CGroup 类型</th>
<th align="left">参数</th>
<th align="left">QoS 类型</th>
<th align="left">值 (示例)</th>
<th align="left">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>容器的 CGroup</strong></td>
<td align="left"><strong><code>memory.limit_in_bytes</code></strong></td>
<td align="left">BestEffort</td>
<td align="left"><code>9223372036854771712</code> (非常大的值, 约等于无限制)</td>
<td align="left">BestEffort 容器没有内存硬限制，但内存压力大时最先被 OOM Kill。</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"></td>
<td align="left">Burstable</td>
<td align="left"><code>limits.memory</code> (转换为 bytes)</td>
<td align="left">如果设置了 <code>limits.memory</code>，则容器内存使用不能超过此值。如果未设置，则同 BestEffort。</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"></td>
<td align="left">Guaranteed</td>
<td align="left"><code>limits.memory</code> (转换为 bytes)</td>
<td align="left">严格限制内存使用不超过 <code>limits.memory</code> (等于 <code>requests.memory</code>)。</td>
</tr>
<tr>
<td align="left"><strong>Pod 的 CGroup</strong></td>
<td align="left"><strong><code>memory.limit_in_bytes</code></strong></td>
<td align="left">BestEffort</td>
<td align="left"><code>9223372036854771712</code></td>
<td align="left">Pod 整体没有内存硬限制。</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"></td>
<td align="left">Burstable</td>
<td align="left">所有设置了 <code>limits.memory</code> 的容器的 <code>limits.memory</code> (转换为 bytes) 之和</td>
<td align="left">Pod 整体的内存硬限制是其下所有设置了 <code>limits</code> 的容器的 limit 之和。</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"></td>
<td align="left">Guaranteed</td>
<td align="left">所有容器的 <code>limits.memory</code> (转换为 bytes) 之和</td>
<td align="left">Pod 整体严格限制内存使用，等于其下所有容器的 limit 之和。</td>
</tr>
</tbody></table>
<h3 id="OOM-Killer-行为-Out-Of-Memory-Killer"><a href="#OOM-Killer-行为-Out-Of-Memory-Killer" class="headerlink" title="OOM Killer 行为 (Out Of Memory Killer)"></a>OOM Killer 行为 (Out Of Memory Killer)</h3><p>当节点内存严重不足时，<strong>Linux 内核的 OOM Killer</strong> 会介入，选择一个或多个进程杀死以释放内存。Kubernetes 通过设置 Cgroup 和调整进程的 <code>oom_score_adj</code> 值来影响 OOM Killer 的决策，以保护更重要的 Pod。</p>
<ul>
<li><p><strong><code>oom_score</code></strong>: 内核为每个进程计算一个 <code>oom_score</code> (0-1000)。分数越高，越容易被 OOM Killer 选中。分数主要基于进程使用的物理内存占系统总内存的百分比。</p>
</li>
<li><p><strong><code>oom_score_adj</code></strong>: Kubelet 可以为容器内的进程设置 <code>oom_score_adj</code> 值 (-1000 到 1000)。这个值会调整内核计算出的 <code>oom_score</code>。</p>
<ul>
<li><code>-1000</code>: 完全禁止 OOM Killer 杀死该进程。</li>
<li><code>1000</code>: 使进程非常容易被 OOM Killer 杀死。</li>
</ul>
</li>
<li><p><strong>计算逻辑</strong>: 进程的最终 OOM 评分大致为 <code>(内存使用百分比 * 10) + oom_score_adj</code> (简化)。</p>
</li>
<li><p><strong>Kubernetes QoS 与 <code>oom_score_adj</code></strong>: Kubelet 根据 Pod 的 QoS 类型和内存 <code>requests</code> 来设置容器进程的 <code>oom_score_adj</code>：</p>
<table>
<thead>
<tr>
<th align="left">Pod QoS 类型</th>
<th align="left"><code>oom_score_adj</code></th>
<th align="left">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>Guaranteed</strong></td>
<td align="left"><code>-998</code></td>
<td align="left">非常不容易被 OOM Kill，受到最高保护。Kubelet 自身和其他关键系统进程通常有更低的 <code>oom_score_adj</code> (如 -999)。</td>
</tr>
<tr>
<td align="left"><strong>BestEffort</strong></td>
<td align="left"><code>1000</code></td>
<td align="left">最容易被 OOM Kill，在内存不足时最先被牺牲。</td>
</tr>
<tr>
<td align="left"><strong>Burstable</strong></td>
<td align="left"><code>min(max(2, 1000 - (1000 * memoryRequestBytes) / machineMemoryCapacityBytes), 999)</code></td>
<td align="left">动态计算，介于 2 和 999 之间。内存 <code>requests</code> 越接近节点总内存，<code>oom_score_adj</code> 越低，保护程度越高；<code>requests</code> 越小，越容易被 OOM Kill。</td>
</tr>
</tbody></table>
</li>
</ul>
<h3 id="日志管理-Log-Management"><a href="#日志管理-Log-Management" class="headerlink" title="日志管理 (Log Management)"></a>日志管理 (Log Management)</h3><p>节点上系统服务和容器产生的大量日志如果不加管理，会迅速耗尽磁盘空间，触发 DiskPressure 甚至导致节点不可用。</p>
<ul>
<li><strong>系统日志</strong>: 节点操作系统层面的日志（如 systemd journal, &#x2F;var&#x2F;log&#x2F;<em>），需要配置 <strong><code>logrotate</code></strong> 或类似的工具进行*<em>定期轮转 (rotate)</em></em> 和 <strong>清理 (clean)</strong>。<ul>
<li><strong>配置</strong>: 编辑 <code>/etc/logrotate.conf</code> 或 <code>/etc/logrotate.d/</code> 下的配置文件，设置轮转周期（daily, weekly）、保留文件数 (<code>rotate N</code>)、大小限制 (<code>size M</code>)、压缩 (<code>compress</code>) 等。</li>
<li><strong>注意</strong>: <code>logrotate</code> 的执行周期（通常是 cron 任务）不能过长，否则可能在周期到来前磁盘就被写满；也不能过短，以免过于频繁地操作影响性能。需要合理配置触发条件（如大小和时间结合）。</li>
</ul>
</li>
<li><strong>容器日志</strong>:<ul>
<li><strong>Docker&#x2F;Containerd</strong>: 容器运行时本身通常提供日志驱动 (logging driver)，如 <code>json-file</code>，可以配置每个容器日志文件的<strong>最大大小 (<code>max-size</code>)</strong> 和 <strong>最大文件数 (<code>max-file</code>)</strong>。容器运行时会在写入前检查大小并执行轮转。这是推荐的方式。<ul>
<li><strong>Docker 配置示例</strong> (<code>/etc/docker/daemon.json</code>):<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><br>  <span class="hljs-attr">&quot;log-driver&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;json-file&quot;</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;log-opts&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">&#123;</span><br>    <span class="hljs-attr">&quot;max-size&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;100m&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;max-file&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;3&quot;</span><br>  <span class="hljs-punctuation">&#125;</span><br><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure></li>
</ul>
</li>
<li><strong>Kubelet</strong>: Kubelet 也可以管理容器日志（主要是通过定期运行 <code>du</code> 检查大小）。可以通过 Kubelet 参数 <code>--container-log-max-size</code> 和 <code>--container-log-max-files</code> 来设置全局默认值（但优先级低于运行时自身的配置）。</li>
</ul>
</li>
</ul>
<h3 id="Docker-卷管理-Docker-Volume-Management-Legacy-Context"><a href="#Docker-卷管理-Docker-Volume-Management-Legacy-Context" class="headerlink" title="Docker 卷管理 (Docker Volume Management - Legacy Context)"></a>Docker 卷管理 (Docker Volume Management - Legacy Context)</h3><ul>
<li><strong>Dockerfile <code>VOLUME</code> 指令</strong>: 在 Dockerfile 中使用 <code>VOLUME</code> 指令会创建一个由 Docker 管理的卷。在 Kubernetes 环境中，这种卷的管理和生命周期与 Kubernetes 的 Volume 机制不兼容，<strong>强烈建议不要在用于 Kubernetes 的镜像的 Dockerfile 中使用 <code>VOLUME</code> 指令</strong>。应使用 Kubernetes 的 Volume 类型（如 <code>emptyDir</code>, <code>hostPath</code>, <code>persistentVolumeClaim</code>）来管理数据。</li>
<li><strong>容器可写层与 <code>emptyDir</code></strong>: 如果容器向其<strong>可写层 (writable layer)</strong> 或<strong>挂载的 <code>emptyDir</code> 卷</strong>大量写入数据，会消耗节点上的临时存储 (<code>nodefs</code>)。这可能导致磁盘 I&#x2F;O 过高，影响节点上其他 Pod 和系统进程的性能，并可能触发 <code>DiskPressure</code> 和驱逐。</li>
<li><strong>I&#x2F;O 限制</strong>: Docker 和 Containerd 运行时都基于 <strong>Cgroup v1</strong> (在许多环境中仍然是主流) 对块设备 I&#x2F;O 的限制支持有限，特别是对<strong>缓冲 I&#x2F;O (Buffered I&#x2F;O)</strong> 的支持不佳。虽然可以限制<strong>直接 I&#x2F;O (Direct I&#x2F;O)</strong>，但大多数应用使用缓冲 I&#x2F;O。这意味着很难精确地限制 Pod 的磁盘 I&#x2F;O 速率。<strong>Cgroup v2</strong> 提供了更好的 I&#x2F;O 控制能力，但需要操作系统和 Kubelet 配置 Cgroup v2 支持。对于有特殊 I&#x2F;O 性能或隔离需求的场景，建议使用<strong>独立的物理卷或网络存储卷 (PersistentVolume)</strong>，而不是依赖共享的节点临时存储。</li>
</ul>
<h3 id="网络资源-Network-Resources"><a href="#网络资源-Network-Resources" class="headerlink" title="网络资源 (Network Resources)"></a>网络资源 (Network Resources)</h3><p>默认情况下，Kubernetes 不对 Pod 的网络带宽进行限制。但是，可以通过 <strong>CNI (Container Network Interface) 插件</strong> 来实现带宽控制。</p>
<ul>
<li><strong>机制</strong>: 支持带宽限制的 CNI 插件（如 Calico, Cilium, 或社区的 <code>bandwidth</code> 插件）通常利用 <strong>Linux Traffic Control (TC)</strong> 子系统来实现。TC 允许在网络接口上配置<strong>队列规则 (qdisc)</strong> 和 <strong>过滤器 (filter)</strong> 来整形 (shape) 或管制 (police) 流量。</li>
<li><strong>配置</strong>: 可以通过在 Pod 的 <strong>annotations (注解)</strong> 中添加特定字段来声明期望的带宽限制。社区 <code>bandwidth</code> 插件使用的注解是：<ul>
<li><code>kubernetes.io/ingress-bandwidth</code>: Pod 的入向带宽限制。</li>
<li><code>kubernetes.io/egress-bandwidth</code>: Pod 的出向带宽限制。</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-bw-limited</span><br>  <span class="hljs-attr">annotations:</span><br>    <span class="hljs-attr">kubernetes.io/ingress-bandwidth:</span> <span class="hljs-string">10M</span> <span class="hljs-comment"># 限制入向带宽为 10 Mbps</span><br>    <span class="hljs-attr">kubernetes.io/egress-bandwidth:</span> <span class="hljs-string">20M</span> <span class="hljs-comment"># 限制出向带宽为 20 Mbps</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">containers:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">nginx</span><br>    <span class="hljs-attr">image:</span> <span class="hljs-string">nginx</span><br></code></pre></td></tr></table></figure></li>
<li><strong>注意</strong>: 具体的注解和支持情况取决于所使用的 CNI 插件。需要查阅 CNI 插件的文档。</li>
</ul>
<h3 id="进程数-Process-Count-PIDs"><a href="#进程数-Process-Count-PIDs" class="headerlink" title="进程数 (Process Count &#x2F; PIDs)"></a>进程数 (Process Count &#x2F; PIDs)</h3><p>节点上的可用 <strong>PID (Process ID)</strong> 数量是有限的（由内核参数 <code>kernel.pid_max</code> 决定，通常在 <code>/proc/sys/kernel/pid_max</code> 查看）。如果节点上的总进程数（包括系统进程和所有 Pod 内的进程）过多，耗尽了 PID 资源，将无法创建新的进程，导致节点功能异常。</p>
<ul>
<li><strong>限制</strong>:<ul>
<li><strong>Kubelet 默认不限制</strong>单个 Pod 可以创建的子进程数量。</li>
<li>可以通过 Kubelet 启动参数 <code>--pod-max-pids</code> 来<strong>限制每个 Pod 内可以运行的总 PID 数量</strong>。这是一个重要的稳定性保障措施，防止 “PID 泄漏” 或 “fork bomb” 类型的应用耗尽节点 PID 资源。</li>
<li>可以通过 Kubelet 参数 <code>--system-reserved-pids</code> 和 <code>--kube-reserved-pids</code> （如果支持）为系统和 Kubernetes 组件<strong>预留一部分 PID</strong>，确保它们在 PID 紧张时仍能正常工作。</li>
</ul>
</li>
<li><strong>监控与压力</strong>:<ul>
<li>Kubelet 会<strong>周期性地检查</strong>节点当前的 PID 使用量。</li>
<li>如果<strong>可用 PID 数量少于设定的阈值</strong>（通过 <code>--eviction-hard</code> 或 <code>--eviction-soft</code> 配置，例如 <code>pid.available&lt;1k</code>），Kubelet 会将节点的 <strong><code>PIDPressure</code> Condition</strong> 标记为 <code>True</code>。</li>
<li><strong>kube-scheduler</strong> 会避免将新的 Pod 调度到处于 <code>PIDPressure</code> 状态的节点上（通过 Taint）。</li>
<li>如果达到硬驱逐阈值，Kubelet 会根据驱逐策略开始<strong>驱逐 Pod</strong> 以回收 PID 资源（通常优先驱逐 BestEffort 和 Burstable Pod）。<br>通过对 NUMA、状态上报、资源预留、Capacity&#x2F;Allocatable、磁盘、内存、CPU、网络和 PID 等多方面的精细管理，可以显著提升 Kubernetes 生产集群的稳定性、性能和资源利用效率。</li>
</ul>
</li>
</ul>
<h2 id="4-节点异常检测"><a href="#4-节点异常检测" class="headerlink" title="4. 节点异常检测"></a>4. 节点异常检测</h2><p>在 Kubernetes 集群的日常运维中，<strong>节点（Node）的健康状态是保障整个集群稳定运行和应用高可用的基石</strong>。一个节点可能因为硬件故障、内核问题、资源耗尽（内存、磁盘、PID）、网络配置错误或 Kubelet 自身异常等原因而变得不可用或状态不佳。Kubernetes 需要一套机制来及时发现这些异常节点，并根据情况采取相应的处理措施（如驱逐 Pod），这就是节点异常检测的核心目标。</p>
<h3 id="Kubelet：节点状态的守卫者"><a href="#Kubelet：节点状态的守卫者" class="headerlink" title="Kubelet：节点状态的守卫者"></a>Kubelet：节点状态的守卫者</h3><p>每个 Kubernetes 工作节点上都运行着一个关键的代理进程：<strong>Kubelet</strong>。Kubelet 不仅负责管理本机上的 Pod 生命周期（如启动、停止容器），还承担着<strong>监控节点自身健康状况并将状态汇报给 Kubernetes API Server 的重要职责</strong>。</p>
<p>Kubelet 会定期收集节点的各种指标，例如 CPU 使用率、内存使用量、磁盘空间、网络连通性等。它通过与底层的 Linux 内核交互来获取这些信息，例如：</p>
<ul>
<li><strong>内存使用和压力</strong>：Kubelet 会检查 <code>/proc/meminfo</code> 文件以及 cgroups 提供的内存统计信息，来判断节点当前的内存使用情况以及是否存在内存压力。内核的 OOM Killer (Out Of Memory Killer) 日志也是判断内存问题的重要依据。</li>
<li><strong>磁盘使用和压力</strong>：Kubelet 会监控其管理的关键文件系统（通常是根分区 <code>/</code> 和 Docker&#x2F;containerd 使用的数据存储分区，如 <code>/var/lib/docker</code> 或 <code>/var/lib/containerd</code>）的磁盘使用率和 inode 使用率。通过 <code>statfs</code> 系统调用获取这些信息。当可用空间低于预设的阈值时，会标记磁盘压力。</li>
<li><strong>PID 压力</strong>：Linux 内核对进程 ID（PID）的数量是有限制的（可通过 <code>/proc/sys/kernel/pid_max</code> 查看）。如果节点上运行了过多的进程，耗尽了可用的 PID，将导致无法创建新的进程。Kubelet 会监控当前 PID 的使用情况，并与内核限制进行比较，判断是否存在 PID 压力。cgroups v1 和 v2 也提供了 PIDs 控制器来限制一个 cgroup 内可以创建的进程数。</li>
<li><strong>网络可用性</strong>：Kubelet 会检查节点网络是否按预期配置完成。这通常依赖于 CNI (Container Network Interface) 插件是否成功初始化并配置了节点的网络，确保 Pod 可以获得 IP 地址并进行通信。如果 CNI 插件报告失败或网络路由、设备出现问题，Kubelet 会标记网络不可用。</li>
</ul>
<p>Kubelet 将收集到的状态信息，特别是下面将要详述的 <strong>Node Conditions</strong>，定期（由 Kubelet 的启动参数 <code>--node-status-update-frequency</code> 控制，默认为 10 秒）通过 <strong>NodeStatus</strong> 对象上报给 API Server。</p>
<h3 id="Node-Conditions：节点健康状态的快照"><a href="#Node-Conditions：节点健康状态的快照" class="headerlink" title="Node Conditions：节点健康状态的快照"></a>Node Conditions：节点健康状态的快照</h3><p><code>NodeStatus</code> 中最重要的部分是 <strong>Conditions</strong> 字段，它是一个列表，包含了描述节点当前状态的关键布尔型标志。这些 Conditions 是 Kubernetes 判断节点健康与否的主要依据。核心的 Node Conditions 包括：</p>
<ul>
<li><p><strong><code>Ready</code></strong>：这是最核心的条件。</p>
<ul>
<li><strong>状态意义</strong>：表示节点是否健康并且<strong>准备好接收、运行新的 Pod</strong>。如果 <code>Ready</code> 为 <code>True</code>，表示 Kubelet 正常运行，网络插件正常工作，节点可以承担工作负载。如果为 <code>False</code> 或 <code>Unknown</code>，则表示节点存在问题，Scheduler 不会将新的 Pod 调度到该节点，并且控制平面（Node Controller）可能会在宽限期后驱逐该节点上的 Pod。</li>
<li><strong>检测原理</strong>：Kubelet 综合评估自身健康、容器运行时状态以及网络插件状态来决定 <code>Ready</code> 条件。任何导致 Kubelet 无法正常工作或无法与容器运行时&#x2F;CNI 插件通信的问题都可能导致 <code>Ready</code> 变为 <code>False</code>。</li>
</ul>
</li>
<li><p><strong><code>MemoryPressure</code></strong>：</p>
<ul>
<li><strong>状态意义</strong>：表示节点是否存在<strong>内存资源压力</strong>。如果为 <code>True</code>，意味着节点可用内存不足，可能会影响现有 Pod 的运行，甚至触发内核的 OOM Killer。</li>
<li><strong>检测原理</strong>：Kubelet 根据配置的**驱逐阈值（Eviction Thresholds）**来判断。例如，通过 Kubelet 启动参数 <code>--eviction-hard=memory.available&lt;1Gi</code> 设置硬驱逐阈值，当可用内存低于 1Gi 时，Kubelet 会将 <code>MemoryPressure</code> 标记为 <code>True</code>，并开始尝试驱逐 Pod 以回收内存。还有软驱逐阈值 (<code>--eviction-soft</code> 和 <code>--eviction-soft-grace-period</code>) 提供更灵活的驱逐策略。Kubelet 通过读取 cgroups V1 的 <code>memory.usage_in_bytes</code> 和 <code>memory.limit_in_bytes</code>，或者 cgroups V2 的 <code>memory.current</code> 和 <code>memory.max</code>，结合 <code>/proc/meminfo</code> 来计算可用内存。</li>
</ul>
</li>
<li><p><strong><code>PIDPressure</code></strong>：</p>
<ul>
<li><strong>状态意义</strong>：表示节点是否存在 <strong>PID 资源压力</strong>。如果为 <code>True</code>，意味着节点上可用 PID 数量紧张，可能无法创建新的进程，影响 Pod 甚至节点自身的稳定性。</li>
<li><strong>检测原理</strong>：同样基于 Kubelet 配置的驱逐阈值，如 <code>--eviction-hard=pid.available&lt;1k</code>。Kubelet 会检查当前已分配的 PID 数量与系统或 cgroup 限制的差值。内核参数 <code>kernel.pid_max</code> 定义了系统全局的 PID 上限，而 cgroups PIDs 控制器可以限制特定 cgroup（如 Kubelet 或 Pod）的 PID 数量。</li>
</ul>
</li>
<li><p><strong><code>DiskPressure</code></strong>：</p>
<ul>
<li><strong>状态意义</strong>：表示节点是否存在<strong>磁盘空间压力</strong>。如果为 <code>True</code>，意味着 Kubelet 管理的关键磁盘分区（通常是 <code>nodefs</code> - 节点根文件系统，和 <code>imagefs</code> - 容器镜像和可写层存储文件系统）的可用空间不足。这可能导致无法写入日志、无法创建新的容器镜像层、甚至无法调度新的 Pod（如果需要写数据到这些分区）。</li>
<li><strong>检测原理</strong>：基于 Kubelet 配置的驱逐阈值，如 <code>--eviction-hard=nodefs.available&lt;10%</code>, <code>--eviction-hard=imagefs.available&lt;15%</code>。Kubelet 使用 <code>statfs</code> 系统调用检查对应文件系统的可用块和总块数，以及可用 inode 和总 inode 数，来判断是否达到压力阈值。</li>
</ul>
</li>
<li><p><strong><code>NetworkUnavailable</code></strong>：</p>
<ul>
<li><strong>状态意义</strong>：表示节点的<strong>网络配置是否不正确或未就绪</strong>。如果为 <code>True</code>，意味着该节点的网络尚未被正确配置（通常由 CNI 插件负责），Pod 可能无法获得 IP 地址或与其他 Pod&#x2F;服务通信。</li>
<li><strong>检测原理</strong>：这个条件的设置通常由<strong>网络插件</strong>自己决定。Kubelet 会提供一个接口（例如，通过特定的文件或配置状态）让网络插件告知其配置状态。如果 Kubelet 未收到网络插件已就绪的信号，或者插件明确报告了错误，Kubelet 会将此条件设置为 <code>True</code>。这<strong>通常只在特定的云提供商环境或网络配置场景下使用</strong>，并且需要 Kubelet 配置了相应的 cloud provider。对于大多数标准的 CNI 部署，<code>Ready</code> 条件已经隐含了网络的基本可用性。</li>
</ul>
</li>
</ul>
<p>你可以通过以下命令查看一个节点的详细状态，包括它的 Conditions：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">kubectl describe node &lt;node-name&gt;<br></code></pre></td></tr></table></figure>

<p>输出中会有一个 <code>Conditions</code> 的部分，清晰地展示了每个条件的状态 (<code>True</code>, <code>False</code>, <code>Unknown</code>)、最后一次探测到状态的时间 (<code>lastProbeTime</code>) 以及最后一次状态转变的时间 (<code>lastTransitionTime</code>) 和原因 (<code>reason</code>)、消息 (<code>message</code>)。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># kubectl describe node my-node 的部分输出示例</span><br><span class="hljs-string">...</span><br><span class="hljs-attr">Conditions:</span><br>  <span class="hljs-string">Type</span>             <span class="hljs-string">Status</span>  <span class="hljs-string">LastHeartbeatTime</span>                 <span class="hljs-string">LastTransitionTime</span>                <span class="hljs-string">Reason</span>                       <span class="hljs-string">Message</span><br>  <span class="hljs-string">----</span>             <span class="hljs-string">------</span>  <span class="hljs-string">-----------------</span>                 <span class="hljs-string">------------------</span>                <span class="hljs-string">------</span>                       <span class="hljs-string">-------</span><br>  <span class="hljs-string">MemoryPressure</span>   <span class="hljs-literal">False</span>   <span class="hljs-string">Wed,</span> <span class="hljs-number">20</span> <span class="hljs-string">Nov</span> <span class="hljs-number">2024 10:30:00</span> <span class="hljs-string">+0800</span>   <span class="hljs-string">Mon,</span> <span class="hljs-number">18</span> <span class="hljs-string">Nov</span> <span class="hljs-number">2024 09:00:00</span> <span class="hljs-string">+0800</span>   <span class="hljs-string">KubeletHasSufficientMemory</span>   <span class="hljs-string">kubelet</span> <span class="hljs-string">has</span> <span class="hljs-string">sufficient</span> <span class="hljs-string">memory</span> <span class="hljs-string">available</span><br>  <span class="hljs-string">DiskPressure</span>     <span class="hljs-literal">False</span>   <span class="hljs-string">Wed,</span> <span class="hljs-number">20</span> <span class="hljs-string">Nov</span> <span class="hljs-number">2024 10:30:00</span> <span class="hljs-string">+0800</span>   <span class="hljs-string">Mon,</span> <span class="hljs-number">18</span> <span class="hljs-string">Nov</span> <span class="hljs-number">2024 09:00:00</span> <span class="hljs-string">+0800</span>   <span class="hljs-string">KubeletHasNoDiskPressure</span>     <span class="hljs-string">kubelet</span> <span class="hljs-string">has</span> <span class="hljs-literal">no</span> <span class="hljs-string">disk</span> <span class="hljs-string">pressure</span><br>  <span class="hljs-string">PIDPressure</span>      <span class="hljs-literal">False</span>   <span class="hljs-string">Wed,</span> <span class="hljs-number">20</span> <span class="hljs-string">Nov</span> <span class="hljs-number">2024 10:30:00</span> <span class="hljs-string">+0800</span>   <span class="hljs-string">Mon,</span> <span class="hljs-number">18</span> <span class="hljs-string">Nov</span> <span class="hljs-number">2024 09:00:00</span> <span class="hljs-string">+0800</span>   <span class="hljs-string">KubeletHasSufficientPID</span>      <span class="hljs-string">kubelet</span> <span class="hljs-string">has</span> <span class="hljs-string">sufficient</span> <span class="hljs-string">PID</span> <span class="hljs-string">available</span><br>  <span class="hljs-string">Ready</span>            <span class="hljs-literal">True</span>    <span class="hljs-string">Wed,</span> <span class="hljs-number">20</span> <span class="hljs-string">Nov</span> <span class="hljs-number">2024 10:30:00</span> <span class="hljs-string">+0800</span>   <span class="hljs-string">Mon,</span> <span class="hljs-number">18</span> <span class="hljs-string">Nov</span> <span class="hljs-number">2024 09:05:00</span> <span class="hljs-string">+0800</span>   <span class="hljs-string">KubeletReady</span>                 <span class="hljs-string">kubelet</span> <span class="hljs-string">is</span> <span class="hljs-string">posting</span> <span class="hljs-string">ready</span> <span class="hljs-string">status</span><br><span class="hljs-string">...</span><br></code></pre></td></tr></table></figure>

<h3 id="Node-Controller：异常节点的处理者"><a href="#Node-Controller：异常节点的处理者" class="headerlink" title="Node Controller：异常节点的处理者"></a>Node Controller：异常节点的处理者</h3><p>Kubernetes 控制平面中有一个重要的组件叫做 <strong>Node Controller</strong>（作为 <code>kube-controller-manager</code> 的一部分运行）。它的职责之一就是监控所有节点的状态，特别是它们的 <code>Ready</code> Condition 和心跳。</p>
<ul>
<li><p><strong>心跳检测</strong>：Kubelet 会定期向 API Server 发送“心跳”来表明自己还活着并且能够通信。在早期版本中，这主要是通过更新 NodeStatus 实现。为了减少 API Server 的负载，Kubernetes 引入了 <strong>Lease 对象</strong>（位于 <code>kube-node-lease</code> 命名空间）。每个节点都有一个对应的 Lease 对象，Kubelet 会以更高的频率（由 Kubelet 参数 <code>--node-lease-duration-seconds</code> 控制，默认为 40 秒，更新频率约为其 1&#x2F;4）更新这个 Lease 对象。Node Controller 会同时监视 NodeStatus 和 Lease 对象。</p>
</li>
<li><p><strong>标记节点状态</strong>：Node Controller 会根据心跳情况更新节点的 <code>Ready</code> Condition 状态为 <code>Unknown</code>。它使用 <code>--node-monitor-period</code>（默认为 5s）检查节点状态，如果在一个时间窗口（由 <code>--node-monitor-grace-period</code> 控制，默认为 40s）内没有收到 Kubelet 的心跳（无论是 NodeStatus 更新还是 Lease 更新），Node Controller 会将该节点的 <code>Ready</code> Condition 标记为 <code>Unknown</code>。</p>
</li>
<li><p><strong>污点与驱逐</strong>：当 Node Controller 检测到节点状态变为 <code>NotReady</code>（<code>Ready</code> 条件为 <code>False</code>）或 <code>Unreachable</code>（<code>Ready</code> 条件为 <code>Unknown</code> 超过一定时间）时，它会自动给这个节点添加相应的<strong>污点（Taints）</strong>：</p>
<ul>
<li><code>node.kubernetes.io/not-ready</code>: Node condition <code>Ready</code> is <code>False</code>.</li>
<li><code>node.kubernetes.io/unreachable</code>: Node condition <code>Ready</code> is <code>Unknown</code>.<br>这些污点会阻止新的 Pod（除非 Pod 有相应的容忍度 Toleration）被调度到该节点。更重要的是，Node Controller 在添加这些污点后，会等待一个<strong>驱逐宽限期</strong>（由 <code>kube-controller-manager</code> 的参数 <code>--pod-eviction-timeout</code> 控制，默认为 5 分钟）。如果节点在此期间恢复正常（<code>Ready</code> 变为 <code>True</code>），污点会被移除。如果超时后节点状态仍未恢复，Node Controller 将会<strong>触发节点上所有 Pod 的驱逐（Eviction）流程</strong>，将它们从异常节点上删除，以便 Deployment、StatefulSet 等控制器可以在健康的节点上重新创建这些 Pod，实现应用的自愈。</li>
</ul>
</li>
</ul>
<h3 id="Node-Problem-Detector-NPD-：更主动、更细粒度的检测"><a href="#Node-Problem-Detector-NPD-：更主动、更细粒度的检测" class="headerlink" title="Node Problem Detector (NPD)：更主动、更细粒度的检测"></a>Node Problem Detector (NPD)：更主动、更细粒度的检测</h3><p>虽然 Kubelet 能够检测到一些基本的节点问题（如资源压力），但对于更深层次或特定于环境的问题（如内核死锁、硬件故障、文件系统只读、网络黑洞等），Kubelet 可能无法直接感知。为了弥补这一不足，社区开发了 <strong>Node Problem Detector (NPD)</strong>。</p>
<ul>
<li><p><strong>解决了什么问题</strong>：NPD 旨在<strong>主动发现并报告 Kubelet 可能忽略的节点级别问题</strong>。它将这些问题转化为标准的 Kubernetes API 对象（Node Conditions 或 Events），使得集群管理员和自动化系统能够更容易地监控和响应这些问题。</p>
</li>
<li><p><strong>是什么</strong>：NPD 通常以 <strong>DaemonSet</strong> 的形式部署在集群的每个（或部分）节点上。它是一个独立于 Kubelet 的程序。</p>
</li>
<li><p><strong>工作原理</strong>：NPD 通过监控各种系统信号源来发现问题：</p>
<ul>
<li><strong>系统日志</strong>：监控 <code>journald</code>、<code>kern.log</code>、<code>dmesg</code> 等系统日志，通过预定义的正则表达式匹配错误或异常模式（例如，EXT4 文件系统错误、NMI watchdog 超时、硬件错误信息 MCE）。</li>
<li><strong>系统状态文件</strong>：检查特定的系统文件或 <code>/proc</code>, <code>/sys</code> 下的状态信息。</li>
<li><strong>自定义插件&#x2F;脚本</strong>：可以扩展 NPD，运行自定义的健康检查脚本来检测特定应用或硬件相关的问题。<br>当 NPD 检测到问题时，它会与 API Server 通信，执行以下操作之一：</li>
<li><strong>更新节点的 Condition</strong>：它可以添加<strong>自定义的 Node Condition</strong>（例如 <code>KernelDeadlock=True</code>, <code>FilesystemReadOnly=True</code>）到节点的 <code>status.conditions</code> 字段。这些自定义 Condition 可以被监控系统捕获，或者被自定义的控制器用来触发特定操作。</li>
<li><strong>创建 Event</strong>：为节点生成一个 Kubernetes Event，详细描述发现的问题。这对于事后分析和告警非常有用。</li>
</ul>
</li>
<li><p><strong>配置示例</strong>：NPD 的行为通过 ConfigMap 进行配置，定义了要监控的日志源、匹配规则以及发现问题时要报告的 Condition 或 Event 模板。例如，一个规则可能配置为：当在内核日志中检测到 “kernel BUG at” 字符串时，将节点的 <code>KernelOops</code> Condition 设置为 <code>True</code>。</p>
</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># NPD ConfigMap (简化示例)</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">ConfigMap</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">node-problem-detector-config</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><br><span class="hljs-attr">data:</span><br>  <span class="hljs-attr">kernel-monitor.json:</span> <span class="hljs-string">|</span><br><span class="hljs-string">    &#123;</span><br><span class="hljs-string">      &quot;plugin&quot;: &quot;journald&quot;,</span><br><span class="hljs-string">      &quot;logPath&quot;: &quot;/run/log/journal&quot;, // 或 /var/log/journal</span><br><span class="hljs-string">      &quot;lookback&quot;: &quot;5m&quot;,</span><br><span class="hljs-string">      &quot;rules&quot;: [</span><br><span class="hljs-string">        &#123;</span><br><span class="hljs-string">          &quot;type&quot;: &quot;temporary&quot;,</span><br><span class="hljs-string">          &quot;reason&quot;: &quot;KernelOops&quot;,</span><br><span class="hljs-string">          &quot;pattern&quot;: &quot;kernel BUG at&quot;</span><br><span class="hljs-string">        &#125;,</span><br><span class="hljs-string">        &#123;</span><br><span class="hljs-string">          &quot;type&quot;: &quot;temporary&quot;,</span><br><span class="hljs-string">          &quot;condition&quot;: &quot;ReadonlyFilesystem&quot;,</span><br><span class="hljs-string">          &quot;reason&quot;: &quot;FilesystemIsReadOnly&quot;,</span><br><span class="hljs-string">          &quot;pattern&quot;: &quot;Remounting filesystem read-only&quot;</span><br><span class="hljs-string">        &#125;</span><br><span class="hljs-string">      ]</span><br><span class="hljs-string">    &#125;</span><br><span class="hljs-string"></span>  <span class="hljs-comment"># 可以有其他配置文件，如 systemd-monitor.json, custom-plugin-monitor.json 等</span><br></code></pre></td></tr></table></figure>

<p>通过部署和配置 NPD，运维团队可以获得比 Kubelet 默认检查更广泛、更深入的节点健康视图，从而能够更早地发现并处理潜在的节点故障，进一步提升集群的稳定性和可靠性。</p>
<h3 id="NPD-的核心职责：检测与报告"><a href="#NPD-的核心职责：检测与报告" class="headerlink" title="NPD 的核心职责：检测与报告"></a>NPD 的核心职责：检测与报告</h3><p>根据图片内容，NPD 的核心职责非常明确：<strong>它只负责检测节点上发生的异常事件，并将这些事件作为 <code>Node Condition</code> 更新到对应节点的 <code>Node</code> 对象状态中</strong>。这一点至关重要，意味着 NPD 本身是一个<strong>信息收集器和报告器</strong>，它<strong>不会直接干预节点的调度状态或驱逐 Pod</strong>。</p>
<p> NPD 上报的 <code>Node Condition</code> 示例：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-bullet">-</span> <span class="hljs-attr">type:</span> <span class="hljs-string">KernelDeadlock</span>          <span class="hljs-comment"># Condition 类型，由 NPD 定义，表示内核死锁</span><br>  <span class="hljs-attr">status:</span> <span class="hljs-string">&quot;True&quot;</span>               <span class="hljs-comment"># 当前状态，True 表示问题存在</span><br>  <span class="hljs-attr">lastHeartbeatTime:</span> <span class="hljs-string">&quot;2021-11-06T15:44:46Z&quot;</span> <span class="hljs-comment"># NPD 最近一次汇报该 Condition 的时间</span><br>  <span class="hljs-attr">lastTransitionTime:</span> <span class="hljs-string">&quot;2021-11-06T15:29:43Z&quot;</span> <span class="hljs-comment"># 该 Condition 状态上次发生变化的时间</span><br>  <span class="hljs-attr">reason:</span> <span class="hljs-string">DockerHung</span>             <span class="hljs-comment"># 机器可读的原因标识</span><br>  <span class="hljs-attr">message:</span> <span class="hljs-string">&#x27;kernel: INFO: task docker:20744 blocked for more than 120 seconds.&#x27;</span> <span class="hljs-comment"># 人类可读的详细信息</span><br></code></pre></td></tr></table></figure>

<p>这个示例清晰地展示了 NPD 如何工作。它监测到了一个内核级别的事件（通过分析内核日志，例如 <code>dmesg</code> 或 <code>journald</code>），具体表现为 Docker 相关的任务（PID 20744）被阻塞超过 120 秒。NPD 将此问题归类为 <code>KernelDeadlock</code> 类型，并将状态设置为 <code>True</code>，附带了详细的内核日志信息。这些信息被更新到该节点对象的 <code>.status.conditions</code> 字段。</p>
<p><strong>从 Linux 内核角度看</strong>，<code>task blocked for more than X seconds</code> 这类消息通常由内核的 RCU (Read-Copy Update) 子系统或者 <code>watchdog</code> 机制检测到。当一个任务长时间处于不可中断的睡眠状态（<code>TASK_UNINTERRUPTIBLE</code>，通常是等待 I&#x2F;O 或某些锁）时，内核会打印这类警告，这往往预示着潜在的驱动程序错误、硬件问题或内核死锁。NPD 通过配置好的日志监控规则（例如，使用正则表达式匹配 <code>/var/log/dmesg</code> 或 <code>journalctl -k</code> 的输出）捕获这类关键信息。</p>
<h3 id="处理-NPD-报告：需要额外的控制器"><a href="#处理-NPD-报告：需要额外的控制器" class="headerlink" title="处理 NPD 报告：需要额外的控制器"></a>处理 NPD 报告：需要额外的控制器</h3><p>由于 NPD 自身不影响调度，为了让 Kubernetes 集群能够对 NPD 检测到的问题做出反应（例如，阻止新的 Pod 调度到故障节点），<strong>通常需要一个额外的自定义控制器（Custom Controller）</strong>。这个控制器会 <strong>监听 <code>Node</code> 对象的变化</strong>，特别是关注由 NPD 添加或更新的特定 <code>Node Condition</code> 类型（如 <code>KernelDeadlock</code>, <code>FilesystemReadOnly</code> 等）。</p>
<p>当控制器检测到一个节点出现了 NPD 报告的严重问题（例如 <code>status: &quot;True&quot;</code>），它会采取行动。最常见的行动是 <strong>给该节点添加 Taint（污点）</strong>。Taint 是 Kubernetes 的一种机制，用于阻止 Pod 被调度到具有不匹配 Taint 的节点上。</p>
<p>例如，控制器可以执行类似以下的命令来给问题节点添加一个 Taint：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># kubectl taint node &lt;node-name&gt; npd.kubernetes.io/problem=KernelDeadlock:NoSchedule</span><br>kubectl taint node my-faulty-node npd.kubernetes.io/problem=KernelDeadlock:NoSchedule<br></code></pre></td></tr></table></figure>

<p>这个命令给 <code>my-faulty-node</code> 节点添加了一个 Taint，其 <code>key</code> 是 <code>npd.kubernetes.io/problem</code>，<code>value</code> 是 <code>KernelDeadlock</code>，<code>effect</code> 是 <code>NoSchedule</code>。这意味着，除非 Pod 明确声明了对这个 Taint 的 Toleration（容忍），否则 Kubernetes 调度器不会将新的 Pod 调度到这个节点上。这样就有效地将故障节点隔离，防止问题扩大。</p>
<p><strong>从 Golang 和 Kubernetes 控制器实现的角度看</strong>，这个自定义控制器通常使用 <code>client-go</code> 库来 Watch <code>Node</code> 资源。当检测到 <code>Node</code> 对象的 <code>.status.conditions</code> 中出现需要处理的 NPD Condition 时，控制器会构造一个 <code>Node</code> 对象的 Patch 请求，将相应的 Taint 添加到 <code>.spec.taints</code> 字段中，然后通过 <code>client-go</code> 将更新发送给 Kubernetes API Server。</p>
<h3 id="问题修复与状态清理"><a href="#问题修复与状态清理" class="headerlink" title="问题修复与状态清理"></a>问题修复与状态清理</h3><p>当节点上的底层问题（例如内核死锁解除、文件系统恢复读写）被管理员修复后，需要清理 NPD 报告的错误状态以及控制器添加的 Taint。</p>
<p><strong>问题修复后，可以通过重启 NPD Pod 来清理错误事件</strong>。重启 NPD Pod 会使其重新执行初始化检查。如果此时节点问题已不存在，NPD 将不会再上报对应的 <code>Node Condition</code>，或者会将其状态更新为 <code>False</code>。随后，之前添加 Taint 的自定义控制器也应该检测到 <code>Node Condition</code> 的变化（问题 Condition 消失或状态变为 <code>False</code>），并自动移除之前添加的 Taint，从而使节点恢复正常调度。</p>
<p><strong>需要注意的是</strong>，重启 NPD Pod 是一种相对直接但可能不是最优的清理方式。更理想的情况是，NPD 能够周期性地重新检查问题状态，并在问题消失时自动更新 <code>Node Condition</code> 的状态为 <code>False</code>。同样，自定义控制器也应该具备移除 Taint 的逻辑。重启 NPD 是一种确保状态刷新的有效手段，尤其是在 NPD 或控制器逻辑不够完善的情况下。</p>
<p>总结来说，NPD 扮演着节点健康状况的“侦察兵”角色，负责深入挖掘并报告 Kubelet 无法覆盖的底层问题。但它需要与自定义控制器（通常负责添加 Taint）协同工作，才能将这些报告转化为实际的调度决策，最终实现对故障节点的有效隔离和管理。整个流程体现了 Kubernetes 通过组合不同组件、利用声明式 API 和控制器模式来解决复杂问题的设计哲学。</p>
<h2 id="5-常⽤节点问题排查⼿段"><a href="#5-常⽤节点问题排查⼿段" class="headerlink" title="5. 常⽤节点问题排查⼿段"></a>5. 常⽤节点问题排查⼿段</h2><p>在管理和维护 Kubernetes 集群时，快速定位并解决问题至关重要。无论是应用程序故障、调度异常还是节点本身的问题，有效的排查手段都离不开对集群内部状态的深入了解。图片中提到的两种核心方法——访问内部节点和查看日志，是日常排查工作的基础。</p>
<h3 id="访问集群内部节点-Accessing-Internal-Cluster-Nodes"><a href="#访问集群内部节点-Accessing-Internal-Cluster-Nodes" class="headerlink" title="访问集群内部节点 (Accessing Internal Cluster Nodes)"></a>访问集群内部节点 (Accessing Internal Cluster Nodes)</h3><ol>
<li><strong>创建一个支持 SSH 的 Pod</strong>：这通常意味着部署一个带有 SSH 服务端和必要工具（如 <code>net-tools</code>, <code>tcpdump</code> 等）的特殊 Pod，这个 Pod 可以被调度到集群中的任意节点（或通过 <code>nodeSelector</code>&#x2F;<code>affinity</code> 指定到特定节点）。然后，你可以通过 <code>kubectl exec</code> 进入这个 Pod，或者如果 Pod 的 SSH 端口通过 Service (如 NodePort 或 LoadBalancer) 暴露出来，你可以直接 SSH 到这个 Pod。这种方式提供了一个受控的、临时的节点环境访问入口，避免直接暴露节点 SSH。</li>
<li><strong>通过负载均衡器转发 SSH 请求</strong>：如果需要从集群外部访问某个特定节点（或前面提到的 SSH Pod），可以通过创建一个 <code>LoadBalancer</code> 类型的 Service，将外部流量引导至节点的 SSH 端口（通常是 22）或 SSH Pod 的服务端口。这在云环境或需要固定入口点时比较有用，但需要注意安全风险，务必配置严格的网络策略和认证。</li>
</ol>
<p><strong>实践考量与安全</strong></p>
<p>虽然直接 SSH 到生产环境的 K8s Worker Node 是最直接的方式，但这通常不被推荐，因为它破坏了基础设施即代码和不可变基础设施的原则，且有安全风险。<strong>创建临时的调试 Pod</strong> 是更符合云原生理念的方法。例如，可以使用 <code>kubectl debug</code> 命令（较新版本 K8s 提供）快速在节点上启动一个具有特权和访问节点文件系统的 Pod。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 示例：使用 kubectl debug 在指定 node 上启动一个包含网络工具的临时容器</span><br><span class="hljs-comment"># (需要 K8s v1.18+ 或更高版本，并且可能需要特定权限)</span><br><span class="hljs-comment"># kubectl debug node/my-node -it --image=nicolaka/netshoot</span><br><br><span class="hljs-comment"># 手动创建调试 Pod 的简化示例 (需根据实际需求定制镜像和配置)</span><br><span class="hljs-comment"># apiVersion: v1</span><br><span class="hljs-comment"># kind: Pod</span><br><span class="hljs-comment"># metadata:</span><br><span class="hljs-comment">#   name: debug-pod</span><br><span class="hljs-comment"># spec:</span><br><span class="hljs-comment">#   containers:</span><br><span class="hljs-comment">#   - name: debugger</span><br><span class="hljs-comment">#     image: ubuntu:latest # 或包含所需工具的自定义镜像</span><br><span class="hljs-comment">#     command: [&quot;/bin/sleep&quot;, &quot;3650d&quot;] # 保持 Pod 运行</span><br><span class="hljs-comment">#     securityContext:</span><br><span class="hljs-comment">#       privileged: true # 按需开启特权，谨慎使用</span><br><span class="hljs-comment">#   nodeName: specific-node-name # 可选：指定调度到特定节点</span><br><span class="hljs-comment">#   hostNetwork: true       # 可选：使用宿主机网络命名空间</span><br><span class="hljs-comment">#   hostPID: true           # 可选：访问宿主机进程空间</span><br></code></pre></td></tr></table></figure>

<p>然后通过 <code>kubectl exec -it debug-pod -- bash</code> 进入该 Pod 进行操作。</p>
<h3 id="查看-Systemd-管理的服务日志-Viewing-Logs-for-Services-Managed-by-Systemd"><a href="#查看-Systemd-管理的服务日志-Viewing-Logs-for-Services-Managed-by-Systemd" class="headerlink" title="查看 Systemd 管理的服务日志 (Viewing Logs for Services Managed by Systemd)"></a>查看 Systemd 管理的服务日志 (Viewing Logs for Services Managed by Systemd)</h3><p>在许多 Linux 发行版中，K8s 的核心节点组件（如 <strong><code>kubelet</code></strong>、容器运行时 <code>containerd</code> 或 <code>dockerd</code>）通常由 <strong><code>systemd</code></strong> 管理。它们的日志会被 <code>systemd-journald</code> 服务捕获并存储在 <strong>systemd journal</strong> 中。</p>
<p><strong>原理</strong>：<code>systemd-journald</code> 是一个系统服务，它从内核、系统服务（包括 <code>systemd</code> 启动的服务）、<code>syslog</code> 等多个来源收集日志，并以结构化的二进制格式存储。<code>journalctl</code> 是查询这些日志的命令行工具。</p>
<p><strong>命令示例与解析</strong>：</p>
<ul>
<li><code>journalctl -u kubelet</code>: 查看 <code>kubelet</code> 服务的所有日志。<code>-u</code> (unit) 参数指定要查询的 <code>systemd</code> 单元。<strong><code>kubelet</code> 是运行在每个 Node 上的关键代理</strong>，负责管理 Pod 的生命周期，因此其日志对于排查 Pod 相关问题至关重要。</li>
<li><code>journalctl -f -u kubelet</code>: 实时跟踪（tail）<code>kubelet</code> 的最新日志。<code>-f</code> (follow) 类似于 <code>tail -f</code>。</li>
<li><code>journalctl -u kubelet -S &quot;2019-08-26 15:00:00&quot;</code>: 查看从指定时间点 (<code>--since</code>, <code>-S</code>) 开始的 <code>kubelet</code> 日志。对于分析特定时间段内发生的问题非常有用。</li>
<li><code>journalctl -afu kubelet</code>: 结合 <code>-a</code> (show all, 显示所有字段，即使包含不可打印字符或过长)，<code>-f</code> (follow) 和 <code>-u kubelet</code>，提供详细的实时日志流。</li>
</ul>
<p><strong>配置示例</strong>：通常无需特殊配置，<code>journalctl</code> 直接可用。若要调整 <code>journald</code> 的存储策略（如持久化存储、大小限制），可编辑 <code>/etc/systemd/journald.conf</code>。</p>
<h3 id="查看容器日志-Viewing-Container-Logs"><a href="#查看容器日志-Viewing-Container-Logs" class="headerlink" title="查看容器日志 (Viewing Container Logs)"></a>查看容器日志 (Viewing Container Logs)</h3><p>容器化应用的最佳实践是<strong>将日志输出到标准输出 (stdout) 和标准错误 (stderr)</strong>。Kubernetes 会自动捕获这些输出流，并通过 <code>kubectl logs</code> 命令提供访问。</p>
<p><strong>原理</strong>：容器运行时（如 Docker 或 containerd）负责捕获容器的 <code>stdout</code> 和 <code>stderr</code> 输出，并将它们存储在节点上的特定位置（通常是 <code>/var/log/pods/...</code> 或由运行时管理的日志驱动程序处理）。当执行 <code>kubectl logs</code> 时，请求会通过 <strong>API Server</strong> 转发给目标 Pod 所在节点的 <strong>Kubelet</strong>，Kubelet 再从容器运行时获取相应的日志数据并返回给用户。</p>
<p><strong>命令示例与解析</strong>：</p>
<ul>
<li><code>kubectl logs &lt;podname&gt;</code>: 获取指定 Pod 中<strong>第一个容器</strong>的日志。如果 Pod 只有一个容器，这是最常用的命令。</li>
<li><code>kubectl logs -c &lt;containername&gt; &lt;podname&gt;</code>: 当一个 Pod 包含多个容器时（例如，应用容器 + sidecar 容器），<strong>必须使用 <code>-c</code> 参数指定要查看哪个容器的日志</strong>。这是排查多容器 Pod 问题时的关键。</li>
<li><code>kubectl logs -f &lt;podname&gt;</code> 或 <code>kubectl logs -f -c &lt;containername&gt; &lt;podname&gt;</code>: 实时跟踪指定容器的日志流。<code>-f</code> (follow) 非常适合观察正在运行的应用的行为或调试实时问题。</li>
<li><code>kubectl logs --all-containers &lt;podname&gt;</code>: 获取 Pod 内<strong>所有容器</strong>的日志。注意，这会将所有容器的日志混合在一起输出，可能较难阅读，但有时可用于快速概览。</li>
<li><code>kubectl logs &lt;podname&gt; --previous</code>: 获取 Pod 中<strong>上一个已终止的容器实例</strong>的日志。这对于<strong>诊断反复崩溃重启 (CrashLoopBackOff) 的 Pod</strong> 至关重要，因为当前运行的容器可能刚启动，日志很少，而错误信息在上次崩溃的容器日志中。</li>
</ul>
<p><strong>Golang 与日志</strong>：在 Golang 后端开发中，推荐使用标准库 <code>log</code> 或更强大的第三方库（如 <code>logrus</code>, <code>zap</code>）将日志<strong>直接输出到 <code>os.Stdout</code> 或 <code>os.Stderr</code></strong>，这样就能无缝对接 K8s 的日志收集机制。避免将日志写入容器内的文件，除非有特殊理由。</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-keyword">package</span> main<br><br><span class="hljs-keyword">import</span> (<br>	<span class="hljs-string">&quot;log&quot;</span><br>	<span class="hljs-string">&quot;net/http&quot;</span><br>	<span class="hljs-string">&quot;os&quot;</span><br>)<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> &#123;<br>	<span class="hljs-comment">// 配置标准库 log 输出到 stdout</span><br>	log.SetOutput(os.Stdout)<br>	log.Println(<span class="hljs-string">&quot;Starting server...&quot;</span>)<br><br>	http.HandleFunc(<span class="hljs-string">&quot;/&quot;</span>, <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(w http.ResponseWriter, r *http.Request)</span></span> &#123;<br>		log.Printf(<span class="hljs-string">&quot;Received request for %s from %s\n&quot;</span>, r.URL.Path, r.RemoteAddr)<br>		<span class="hljs-comment">// ... handle request ...</span><br>		w.WriteHeader(http.StatusOK)<br>		w.Write([]<span class="hljs-type">byte</span>(<span class="hljs-string">&quot;OK&quot;</span>))<br>	&#125;)<br><br>	err := http.ListenAndServe(<span class="hljs-string">&quot;:8080&quot;</span>, <span class="hljs-literal">nil</span>)<br>	<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>		<span class="hljs-comment">// 错误信息输出到 stderr</span><br>		log.SetOutput(os.Stderr)<br>		log.Fatalf(<span class="hljs-string">&quot;Server failed to start: %v&quot;</span>, err)<br>	&#125;<br>&#125;<br></code></pre></td></tr></table></figure>

<h3 id="查看重定向到文件的容器日志-Viewing-Container-Logs-Redirected-to-Files"><a href="#查看重定向到文件的容器日志-Viewing-Container-Logs-Redirected-to-Files" class="headerlink" title="查看重定向到文件的容器日志 (Viewing Container Logs Redirected to Files)"></a>查看重定向到文件的容器日志 (Viewing Container Logs Redirected to Files)</h3><p>有时，应用程序（特别是遗留系统或某些第三方软件）可能被配置为将日志写入容器内的文件，而不是 <code>stdout/stderr</code>。</p>
<p><strong>原理</strong>：<code>kubectl logs</code> 无法直接读取这些文件。此时需要使用 <code>kubectl exec</code> 命令在容器内部执行命令来查看文件内容。<code>kubectl exec</code> 通过 API Server -&gt; Kubelet -&gt; 容器运行时 的路径，在目标容器的命名空间内启动一个指定的进程（如 <code>sh</code>, <code>bash</code>, <code>cat</code>, <code>tail</code>）。</p>
<p><strong>命令示例与解析</strong>：</p>
<ul>
<li><code>kubectl exec -it &lt;podname&gt; -- tail -f /path/to/log/file</code>: 在指定 Pod 的默认容器内（或用 <code>-c &lt;containername&gt;</code> 指定容器）执行 <code>tail -f</code> 命令，实时查看位于 <code>/path/to/log/file</code> 的日志文件。<ul>
<li><code>-i</code> (stdin): 保持标准输入打开。</li>
<li><code>-t</code> (tty): 分配一个伪终端。对于交互式 shell (<code>bash</code>, <code>sh</code>) 是必需的，对于 <code>tail -f</code> 也是常用的。</li>
<li><code>--</code>: 分隔符，用于区分 <code>kubectl</code> 的参数和要在容器内执行的命令及其参数。</li>
</ul>
</li>
</ul>
<p><strong>配置示例</strong>：无需特殊配置 <code>kubectl</code>，但需要知道日志文件在容器内的确切路径。</p>
<p><strong>总结</strong></p>
<p>掌握节点访问技术和熟练运用日志查看命令是 K8s 问题排查的基础技能。理解这些命令背后的原理（如 <code>systemd journal</code>, <code>stdout/stderr</code> 捕获, <code>kubectl</code> 与 K8s 各组件的交互流程）能帮助你更有效地定位问题。在实践中，通常会结合 <code>kubectl describe pod &lt;podname&gt;</code>, <code>kubectl get events --sort-by=&#39;.lastTimestamp&#39;</code>, 以及监控系统（如 Prometheus + Grafana）的信息，形成一个完整的排查视图。</p>
<hr>
<h2 id="6-基于-extendedresource-扩展节点资源"><a href="#6-基于-extendedresource-扩展节点资源" class="headerlink" title="6. 基于 extendedresource 扩展节点资源"></a>6. 基于 extendedresource 扩展节点资源</h2><p>在 Kubernetes 集群中，计算资源的管理是核心功能之一。除了内建的 <strong>CPU</strong> 和 <strong>Memory</strong> 资源外，Kubernetes 提供了一种 <strong>扩展资源（Extended Resources）</strong> 的机制，允许集群管理员和开发者定义、通告和使用节点级别的、非内建的特殊资源。这极大地增强了 Kubernetes 在异构硬件和特定场景下的调度与管理能力。</p>
<h3 id="解决的问题与应用场景"><a href="#解决的问题与应用场景" class="headerlink" title="解决的问题与应用场景"></a>解决的问题与应用场景</h3><p>标准的 CPU 和 Memory 资源无法满足所有调度需求。例如：</p>
<ol>
<li><strong>特殊硬件调度</strong>：需要将 Pod 调度到配备了特定硬件（如 <strong>NVIDIA GPU</strong>、<strong>FPGA</strong>、<strong>高性能网卡（SmartNICs）</strong>、<strong>加密卡</strong>等）的节点上。</li>
<li><strong>逻辑资源或配额</strong>：管理一些并非物理硬件但需要按节点或集群进行计数的资源，例如软件许可证、特定服务的并发连接数、或者是像图中示例的 <code>reclaimed-cpu</code> 这样的自定义逻辑资源。</li>
<li><strong>精细化资源隔离</strong>：为特定类型的负载确保独占或定量的特殊资源访问。</li>
</ol>
<p><strong>Extended Resources</strong> 解决了 Kubernetes 原生资源模型无法描述这些特殊节点能力的问题，使得调度器能够基于这些自定义资源约束来放置 Pod。</p>
<h3 id="扩展资源的定义与通告"><a href="#扩展资源的定义与通告" class="headerlink" title="扩展资源的定义与通告"></a>扩展资源的定义与通告</h3><p>扩展资源必须遵循 <code>domain/resource-name</code> 的命名格式（<code>kubernetes.io</code> 域为 Kubernetes 核心组件保留）。有两种主要方式在节点上通告扩展资源：</p>
<ol>
<li><p><strong>设备插件（Device Plugins）</strong>：</p>
<ul>
<li>这是管理<strong>特定硬件设备</strong>（如 GPU、FPGA、SR-IOV VFs 等）的标准方式。</li>
<li><strong>Device Plugin</strong> 是一个运行在节点上的独立进程（通常是 DaemonSet），负责：<ul>
<li><strong>发现</strong>节点上的特定硬件设备。</li>
<li>向 <strong>Kubelet</strong> 注册，并<strong>汇报</strong>可用设备资源及其健康状况。Device Plugin 通过 gRPC 与 Kubelet 的 <strong>Device Manager</strong> 进行通信（监听在 <code>/var/lib/kubelet/device-plugins/kubelet.sock</code>）。</li>
<li><strong>分配</strong>设备给请求资源的容器。</li>
</ul>
</li>
<li>当 Device Plugin 向 Kubelet 注册并汇报资源后，<strong>Kubelet</strong> 会自动更新该 <strong>Node</strong> 对象的 <code>.status.capacity</code> 和 <code>.status.allocatable</code> 字段，将这些设备资源（例如 <code>nvidia.com/gpu: 2</code>）通告给 <strong>API Server</strong>。<strong>kube-scheduler</strong> 后续会读取这些信息进行调度决策。</li>
</ul>
</li>
<li><p><strong>节点级扩展资源（Operator Managed）</strong>：</p>
<ul>
<li><p>对于非硬件或者不由 Device Plugin 管理的资源（例如图中 <code>cncamp.com/reclaimed-cpu</code>），集群管理员或运维人员可以通过直接<strong>修改 Node 对象</strong>的方式来通告。</p>
</li>
<li><p>具体操作是向 <strong>API Server</strong> 发送 <strong>HTTP PATCH</strong> 请求，更新目标 Node 的 <code>.status.capacity</code> 字段。</p>
</li>
<li><p><strong>示例命令</strong> (如图片所示，用于向节点 <code>cadmin</code> 添加 <code>cncamp.com/reclaimed-cpu=2</code> 的容量)：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">curl --key admin.key --cert admin.crt --cacert ca.crt \<br>     -H <span class="hljs-string">&quot;Content-Type: application/json-patch+json&quot;</span> \<br>     -X PATCH \<br>     --data <span class="hljs-string">&#x27;[&#123;&quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/status/capacity/cncamp.com~1reclaimed-cpu&quot;, &quot;value&quot;: &quot;2&quot;&#125;]&#x27;</span> \<br>     https://&lt;api-server-ip&gt;:&lt;port&gt;/api/v1/nodes/cadmin/status<br></code></pre></td></tr></table></figure>

<ul>
<li><strong>注意</strong>: JSON Patch 路径中的 <code>/</code> 需要转义为 <code>~1</code>，所以 <code>cncamp.com/reclaimed-cpu</code> 变成了 <code>cncamp.com~1reclaimed-cpu</code>。</li>
</ul>
</li>
<li><p>在 <code>.status.capacity</code> 被更新后，节点上的 <strong>Kubelet</strong> 会<strong>异步地</strong>检测到这个变化，并相应地更新 <code>.status.allocatable</code> 字段。<strong><code>kube-scheduler</code> 在进行 Pod 调度时，主要依据的是 <code>.status.allocatable</code> 中的资源量</strong>。因此，从 <code>capacity</code> 更新到资源实际可被调度（<code>allocatable</code> 更新完成）之间可能存在短暂的延迟。</p>
</li>
</ul>
</li>
</ol>
<h3 id="Pod-使用扩展资源"><a href="#Pod-使用扩展资源" class="headerlink" title="Pod 使用扩展资源"></a>Pod 使用扩展资源</h3><p>Pod 在其规约（Spec）中可以像请求 CPU 和 Memory 一样请求扩展资源。</p>
<ul>
<li><p><strong>声明方式</strong>：在容器的 <code>resources</code> 字段下的 <code>limits</code> 和 <code>requests</code> 中声明扩展资源。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">extended-resource-pod</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">containers:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">nginx</span><br>    <span class="hljs-attr">image:</span> <span class="hljs-string">nginx</span><br>    <span class="hljs-attr">resources:</span><br>      <span class="hljs-attr">limits:</span><br>        <span class="hljs-attr">cncamp.com/reclaimed-cpu:</span> <span class="hljs-number">3</span> <span class="hljs-comment"># 请求 3 个单位的扩展资源</span><br>        <span class="hljs-attr">memory:</span> <span class="hljs-string">&quot;200Mi&quot;</span><br>        <span class="hljs-attr">cpu:</span> <span class="hljs-string">&quot;0.5&quot;</span><br>      <span class="hljs-attr">requests:</span><br>        <span class="hljs-attr">cncamp.com/reclaimed-cpu:</span> <span class="hljs-number">3</span> <span class="hljs-comment"># 请求必须等于 Limits</span><br>        <span class="hljs-attr">memory:</span> <span class="hljs-string">&quot;200Mi&quot;</span><br>        <span class="hljs-attr">cpu:</span> <span class="hljs-string">&quot;0.5&quot;</span><br></code></pre></td></tr></table></figure>
</li>
<li><p><strong>重要约束</strong>：对于扩展资源，<strong><code>requests</code> 必须等于 <code>limits</code></strong>。这是因为 Kubernetes <strong>无法对扩展资源进行超售（Overcommit）</strong>。扩展资源通常代表离散的、不可压缩的单元（如一个 GPU、一个许可证），不像 CPU 时间可以被多个进程分时共享。因此，请求多少就必须预留多少，不允许 Limit &gt; Request 的情况。</p>
</li>
</ul>
<h3 id="调度器与扩展资源"><a href="#调度器与扩展资源" class="headerlink" title="调度器与扩展资源"></a>调度器与扩展资源</h3><p><strong>kube-scheduler</strong> 在调度 Pod 时，会执行以下步骤相关的扩展资源：</p>
<ol>
<li><strong>过滤（Filtering）</strong>：检查节点列表，只保留那些 <code>.status.allocatable</code> 中声明的扩展资源<strong>满足</strong> Pod <code>requests</code> 需求的节点。如果 Pod 请求了 <code>nvidia.com/gpu: 1</code>，则只有 <code>allocatable</code> 中 <code>nvidia.com/gpu</code> 大于等于 1 的节点才会通过过滤。</li>
<li><strong>打分（Scoring）</strong>：虽然默认的打分策略可能不直接基于扩展资源的数量（除非配置了特定优先级函数），但满足扩展资源需求是调度的硬性条件。</li>
</ol>
<h3 id="集群层面的扩展资源与调度器扩展（Scheduler-Extenders）"><a href="#集群层面的扩展资源与调度器扩展（Scheduler-Extenders）" class="headerlink" title="集群层面的扩展资源与调度器扩展（Scheduler Extenders）"></a>集群层面的扩展资源与调度器扩展（Scheduler Extenders）</h3><p>有时，扩展资源可能代表的不是物理设备，而是更抽象的概念，或者其调度逻辑非常复杂，超出了默认调度器的能力。</p>
<ul>
<li><p><strong>Scheduler Extenders</strong> 是一种允许你插入自定义调度逻辑的机制。你可以部署一个外部服务（Extender），<code>kube-scheduler</code> 在调度决策的特定点（如 Filter 或 Prioritize）会调用这个外部服务。</p>
</li>
<li><p><strong>管理特定扩展资源</strong>：Extender 可以用来管理某些特定的扩展资源。例如，某个扩展资源可能代表软件许可证，Extender 需要检查全局许可证的可用性，而不仅仅是节点上的容量。</p>
</li>
<li><p><strong>忽略默认调度器处理</strong>：可以配置 <code>kube-scheduler</code> 的 <strong>Policy</strong>，使其忽略某些扩展资源。这样，这些资源的调度决策就完全委托给了 Extender。</p>
<ul>
<li><p><strong>示例配置</strong> :</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><br>  <span class="hljs-attr">&quot;kind&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;Policy&quot;</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;apiVersion&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;v1&quot;</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;extenders&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br>    <span class="hljs-punctuation">&#123;</span><br>      <span class="hljs-attr">&quot;urlPrefix&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;&lt;extender-endpoint&gt;&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-comment">// Extender 服务的 URL</span><br>      <span class="hljs-attr">&quot;bindVerb&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;bind&quot;</span><span class="hljs-punctuation">,</span>                 <span class="hljs-comment">// 指定 Extender 负责绑定操作</span><br>      <span class="hljs-attr">&quot;managedResources&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br>        <span class="hljs-punctuation">&#123;</span><br>          <span class="hljs-attr">&quot;name&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;example.com/foo&quot;</span><span class="hljs-punctuation">,</span>       <span class="hljs-comment">// Extender 管理的扩展资源</span><br>          <span class="hljs-attr">&quot;ignoredByScheduler&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-literal"><span class="hljs-keyword">true</span></span>     <span class="hljs-comment">// 指示默认调度器忽略此资源</span><br>        <span class="hljs-punctuation">&#125;</span><br>      <span class="hljs-punctuation">]</span><br>    <span class="hljs-punctuation">&#125;</span><br>  <span class="hljs-punctuation">]</span><br><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure>
</li>
<li><p>在这个配置中，<code>example.com/foo</code> 这个扩展资源的调度（过滤、打分、绑定）将不会由默认调度器处理，而是完全依赖于配置的 Extender。这对于实现复杂的资源管理（如跨节点配额、拓扑感知调度等）非常有用。</p>
</li>
</ul>
</li>
</ul>
<h3 id="GPU-作为扩展资源：核心机制-设备插件"><a href="#GPU-作为扩展资源：核心机制-设备插件" class="headerlink" title="GPU 作为扩展资源：核心机制 - 设备插件"></a>GPU 作为扩展资源：核心机制 - 设备插件</h3><p>直接通过 <code>PATCH</code> Node 对象来手动管理 GPU 资源是不切实际的，原因在于：</p>
<ol>
<li><strong>动态性</strong>：节点上的 GPU 状态（是否健康、是否可用）可能变化。</li>
<li><strong>发现</strong>：需要自动检测节点上有多少个、哪些型号的 GPU。</li>
<li><strong>分配</strong>：需要精确地将特定的物理 GPU 设备映射给请求它的容器。</li>
<li><strong>健康检查</strong>：需要监控 GPU 的健康状况，并将故障设备从可调度资源中移除。</li>
</ol>
<p>因此，管理 GPU 资源的标准方法是使用 <strong>设备插件（Device Plugin）</strong>。</p>
<p><strong>什么是 GPU 设备插件？</strong></p>
<ul>
<li>它是一个遵循 Kubernetes Device Plugin API 规范的、<strong>独立于 Kubelet 运行的进程</strong>（通常部署为 <strong>DaemonSet</strong>，确保在需要 GPU 的节点上运行）。</li>
<li><strong>供应商特定</strong>：不同的 GPU 供应商（如 NVIDIA、AMD）会提供各自的设备插件实现。最常见的是 <strong>NVIDIA Device Plugin for Kubernetes</strong>。</li>
<li><strong>职责</strong>：<ul>
<li><strong>发现 (Discovery)</strong>：启动时，插件会检测节点上存在的 GPU 设备（例如，NVIDIA 插件会使用 <code>nvidia-ml</code> 库或类似工具来发现物理 GPU）。</li>
<li><strong>注册 (Registration)</strong>：通过 gRPC 与 <strong>Kubelet</strong> 内建的 <strong>Device Manager</strong> 通信（监听在 Unix 套接字 <code>/var/lib/kubelet/device-plugins/kubelet.sock</code>），注册自己能管理的资源类型。对于 NVIDIA GPU，这个资源名通常是 <strong><code>nvidia.com/gpu</code></strong>。</li>
<li><strong>汇报资源 (Reporting)</strong>：向 Kubelet 汇报节点上可用 GPU 的数量和设备 ID。Kubelet 收到这些信息后，会<strong>自动更新</strong>该 <strong>Node</strong> 对象的 <code>.status.capacity</code> 和 <code>.status.allocatable</code> 字段，例如增加 <code>nvidia.com/gpu: 2</code> 表示该节点有 2 个可用的 GPU。这一步将 GPU 资源<strong>通告</strong>给了 Kubernetes 集群。</li>
<li><strong>分配 (Allocation)</strong>：当一个 Pod 被调度到该节点并请求 GPU 资源时，Kubelet 的 Device Manager 会调用 Device Plugin 的 <code>Allocate</code> gRPC 方法。插件负责选择一个或多个具体的、当前未被分配的 GPU 设备，并将它们的<strong>设备文件路径</strong>（例如 <code>/dev/nvidia0</code>, <code>/dev/nvidiactl</code>, <code>/dev/nvidia-uvm</code> 等）和必要的<strong>环境变量</strong>（如 <code>NVIDIA_VISIBLE_DEVICES=0</code>）返回给 Kubelet。Kubelet 随后会把这些设备挂载到 Pod 的容器内，并设置相应的环境变量，使得容器内的应用程序能够访问到指定的 GPU。</li>
<li><strong>健康检查 (Health Checking)</strong>：插件可以监控 GPU 的健康状况。如果检测到 GPU 故障，它可以通知 Kubelet 将该 GPU 标记为不可用，Kubelet 会相应地更新节点的 <code>allocatable</code> 资源。</li>
</ul>
</li>
</ul>
<h3 id="操作步骤：启用和使用-GPU-资源"><a href="#操作步骤：启用和使用-GPU-资源" class="headerlink" title="操作步骤：启用和使用 GPU 资源"></a>操作步骤：启用和使用 GPU 资源</h3><p><strong>前提条件：</strong></p>
<ul>
<li><strong>节点安装 GPU 驱动</strong>：在所有需要运行 GPU 任务的 Kubernetes Worker Node 上，必须预先正确安装相应供应商的 GPU 驱动程序（例如，NVIDIA 驱动）。这是运行 Device Plugin 和 GPU 应用的基础。</li>
</ul>
<p><strong>1. 部署 GPU 设备插件：</strong></p>
<ul>
<li><p>通常使用 <strong>DaemonSet</strong> 来部署设备插件，确保它在所有（或标记了特定标签的）具备 GPU 硬件的节点上运行。</p>
</li>
<li><p>以 <strong>NVIDIA Device Plugin</strong> 为例，你需要从 NVIDIA 的官方仓库获取其部署 YAML 文件。</p>
</li>
<li><p><strong>示例部署命令</strong>（假设你已获取 <code>nvidia-device-plugin.yml</code>）：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">kubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.14.1/nvidia-device-plugin.yml<br><span class="hljs-comment"># 注意：请使用适合你 Kubernetes 版本和 GPU 驱动版本的官方推荐 YAML</span><br></code></pre></td></tr></table></figure>
</li>
<li><p>这个 YAML 文件会创建一个 <code>DaemonSet</code> 对象，可能还包括必要的 <code>ServiceAccount</code>、<code>ClusterRole</code>、<code>ClusterRoleBinding</code> 等 RBAC 配置，赋予 Device Plugin Pod 与 Kubelet 通信所需的权限。</p>
</li>
</ul>
<p><strong>2. 验证资源通告：</strong></p>
<ul>
<li><p>等待 Device Plugin Pod 在 GPU 节点上成功运行后，检查节点的资源状态。</p>
</li>
<li><p><strong>示例命令</strong>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 查看某个安装了 GPU 且运行了插件的节点</span><br>kubectl describe node &lt;your-gpu-node-name&gt;<br><br><span class="hljs-comment"># 在输出中查找 Capacity 和 Allocatable 部分，应该能看到类似信息：</span><br><span class="hljs-comment"># Capacity:</span><br><span class="hljs-comment">#  cpu:                ...</span><br><span class="hljs-comment">#  memory:             ...</span><br><span class="hljs-comment">#  nvidia.com/gpu:     2  # 假设该节点有 2 块 GPU</span><br><span class="hljs-comment"># Allocatable:</span><br><span class="hljs-comment">#  cpu:                ...</span><br><span class="hljs-comment">#  memory:             ...</span><br><span class="hljs-comment">#  nvidia.com/gpu:     2</span><br></code></pre></td></tr></table></figure>
</li>
<li><p><code>nvidia.com/gpu: 2</code> 表明 Kubelet 已成功接收到 Device Plugin 的汇报，并将 2 个 GPU 资源通告给了 API Server。</p>
</li>
</ul>
<p><strong>3. 在 Pod 中请求 GPU 资源：</strong></p>
<ul>
<li><p>在 Pod 的容器规格（<code>spec.containers[]</code>）中，通过 <code>resources.limits</code> 字段来请求 GPU。</p>
</li>
<li><p><strong>关键点</strong>：对于 GPU 这类扩展资源，<code>requests</code> <strong>必须省略</strong> 或者 <strong>等于</strong> <code>limits</code>。Kubernetes 不支持 GPU 资源的超售。</p>
</li>
<li><p><strong>示例 Pod YAML</strong>：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">cuda-vector-add</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">restartPolicy:</span> <span class="hljs-string">OnFailure</span><br>  <span class="hljs-attr">containers:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">cuda-vector-add</span><br>      <span class="hljs-comment"># 使用包含 CUDA 工具包和驱动兼容的镜像</span><br>      <span class="hljs-attr">image:</span> <span class="hljs-string">&quot;nvidia/cuda:11.4.0-base-ubuntu20.04&quot;</span><br>      <span class="hljs-attr">resources:</span><br>        <span class="hljs-attr">limits:</span><br>          <span class="hljs-comment"># 请求 1 个 NVIDIA GPU</span><br>          <span class="hljs-attr">nvidia.com/gpu:</span> <span class="hljs-number">1</span><br>      <span class="hljs-attr">command:</span> [<span class="hljs-string">&quot;/bin/sh&quot;</span>, <span class="hljs-string">&quot;-c&quot;</span>]<br>      <span class="hljs-attr">args:</span> [<span class="hljs-string">&quot;nvidia-smi &amp;&amp; sleep 3600&quot;</span>] <span class="hljs-comment"># 简单运行 nvidia-smi 验证 GPU 可见性</span><br></code></pre></td></tr></table></figure>
</li>
<li><p><strong>工作流程</strong>：</p>
<ol>
<li>用户创建这个 Pod。</li>
<li><strong>kube-scheduler</strong> 查找 <code>allocatable</code> 中 <code>nvidia.com/gpu</code> 至少为 1 的节点。</li>
<li>Pod 被调度到满足条件的 GPU 节点。</li>
<li>该节点上的 <strong>Kubelet</strong> 看到 Pod 请求 GPU，调用本地运行的 <strong>NVIDIA Device Plugin</strong> 的 <code>Allocate</code> 接口。</li>
<li>Device Plugin 返回一个可用 GPU 的设备信息（如设备 ID <code>0</code>）。</li>
<li>Kubelet 配置容器的 cgroup、挂载 <code>/dev/nvidia0</code> 等设备文件，并设置环境变量 <code>NVIDIA_VISIBLE_DEVICES=0</code>。</li>
<li>容器启动，其内部运行的 <code>nvidia-smi</code> 命令应该能看到并访问到被分配的那个 GPU。</li>
</ol>
</li>
</ul>
<h3 id="高级特性（NVIDIA-相关）"><a href="#高级特性（NVIDIA-相关）" class="headerlink" title="高级特性（NVIDIA 相关）"></a>高级特性（NVIDIA 相关）</h3><p>NVIDIA Device Plugin 还支持一些高级特性，例如：</p>
<ul>
<li><strong>时间片共享 (Time-Slicing)</strong>：允许非关键任务以时间片方式共享同一个 GPU，通过 ConfigMap 配置。资源请求仍然是 <code>nvidia.com/gpu: 1</code>，但实际是共享使用。</li>
<li><strong>多实例 GPU (Multi-Instance GPU, MIG)</strong>：对于支持 MIG 的 GPU（如 A100），可以将一个物理 GPU 分割成多个独立的、硬件隔离的 <strong>GPU 实例</strong>。Device Plugin 可以配置为将这些 MIG 设备作为不同的资源类型（例如 <code>nvidia.com/mig-1g.5gb: 1</code>）进行通告和调度，实现更细粒度的 GPU 资源分配和隔离。</li>
</ul>
<p>总结来说，要在 Kubernetes 中使用 GPU，标准且推荐的做法是部署对应 GPU 供应商（如 NVIDIA）提供的 <strong>Device Plugin</strong>。这个插件负责 GPU 的发现、注册、资源汇报、分配和健康检查，通过与 <strong>Kubelet</strong> 的 <strong>Device Manager</strong> 交互，将 GPU 作为名为 <code>vendor.com/gpu</code>（如 <code>nvidia.com/gpu</code>）的 <strong>Extended Resource</strong> 暴露给 Kubernetes 系统。用户只需在 Pod 的 <code>resources.limits</code> 中声明对该资源的需求即可，调度器和 Kubelet 会协同 Device Plugin 完成后续的调度和设备分配。</p>
<h3 id="资源浪费与成本优化"><a href="#资源浪费与成本优化" class="headerlink" title="资源浪费与成本优化"></a>资源浪费与成本优化</h3><p>在典型的 Kubernetes 集群中，为了保证核心在线服务的 <strong>SLA（服务等级协议）</strong>，我们通常会根据其<strong>峰值需求</strong>来设置 Pod 的 <code>requests</code> 和 <code>limits</code>，并配置足够的节点容量。然而，在<strong>非高峰时段</strong>（波谷），这些核心服务的实际资源使用率可能远低于其请求值，导致节点上存在大量<strong>已分配但未使用的 CPU 和内存资源</strong>。这些闲置资源，尤其是在公有云环境中，意味着持续的成本支出却没有产生价值。</p>
<p>直接在这些节点上运行低优先级任务，如果不加控制，可能会因为资源争抢而干扰到高优先级服务（<strong>“ noisy neighbor” 问题</strong>）。使用标准的 CPU <code>requests</code> 为低优先级任务预留资源又违背了利用“空闲”资源初衷，且可能导致节点资源不足。</p>
<p><strong>Extended Resources</strong> 提供了一种机制，可以将这种动态变化的、机会性的“空闲资源”显式化，并纳入 Kubernetes 的调度体系。</p>
<h3 id="解决方案：基于-Agent-的空闲资源扩展"><a href="#解决方案：基于-Agent-的空闲资源扩展" class="headerlink" title="解决方案：基于 Agent 的空闲资源扩展"></a>解决方案：基于 Agent 的空闲资源扩展</h3><p><strong>1. 定义扩展资源:</strong></p>
<p>我们首先定义一种自定义的扩展资源，用于代表节点上可被低优先级任务使用的、回收来的 CPU 资源。例如，我们称之为 <code>example.com/reclaimed-cpu</code>。这里的 <code>example.com</code> 是你的组织或项目的域名，<code>reclaimed-cpu</code> 清晰地表达了资源的来源和性质。单位通常沿用 Kubernetes 的 CPU 单位，如 <code>m</code> (milliCPU)。</p>
<p><strong>2. 开发并部署 Idle Resource Agent:</strong></p>
<p>我们需要在希望回收资源的节点上部署一个 <strong>Agent</strong>（通常以 <strong>DaemonSet</strong> 形式运行，确保覆盖所有目标节点）。这个 Agent 的核心职责是：</p>
<ul>
<li><p><strong>监控节点资源使用情况</strong>：Agent 需要持续监控当前节点的<strong>实际 CPU 使用率</strong>。更精确的做法是，监控节点总 CPU 容量，并减去<strong>关键 Pod（高优先级应用）的实际 CPU 使用量</strong>（可以通过 Kubelet 的 <code>/metrics/cadvisor</code> 端点获取容器级别的指标，或集成 Prometheus 等监控系统）。</p>
</li>
<li><p><strong>计算可回收资源量</strong>：基于监控数据，Agent 计算出当前有多少 CPU 资源是“空闲”的，可以被安全地“回收”给低优先级任务使用。</p>
<figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs abnf"><span class="hljs-attribute">ReclaimedCPU</span> <span class="hljs-operator">=</span> NodeTotalCPU - Sum(CriticalPodsActualCPUUsage) - SafetyBuffer<br></code></pre></td></tr></table></figure>

<ul>
<li><code>NodeTotalCPU</code>: 节点的总 CPU 容量。</li>
<li><code>CriticalPodsActualCPUUsage</code>: 通过标签选择器等方式识别出的关键 Pod 的实时 CPU 使用总和。</li>
<li><code>SafetyBuffer</code>: <strong>非常重要</strong>！设置一个安全缓冲 CPU 量，防止低优先级任务在关键应用 CPU 使用量突然增加时造成严重干扰。这个 Buffer 可以是固定值，也可以是节点容量的一个百分比。</li>
</ul>
</li>
<li><p><strong>动态更新 Node Status</strong>：Agent 定期（例如每 30-60 秒）通过 Kubernetes API <strong>PATCH</strong> 请求，更新其所在 <strong>Node</strong> 对象的 <code>.status.capacity</code> 字段，将计算出的 <code>example.com/reclaimed-cpu</code> 的值写入。</p>
<p><strong>Agent 内部 Go 代码片段（使用 client-go 示意）：</strong></p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-keyword">import</span> (<br>    <span class="hljs-string">&quot;context&quot;</span><br>    <span class="hljs-string">&quot;fmt&quot;</span><br>    <span class="hljs-string">&quot;os&quot;</span><br>    <span class="hljs-string">&quot;time&quot;</span><br><br>    <span class="hljs-string">&quot;k8s.io/apimachinery/pkg/types&quot;</span><br>    <span class="hljs-string">&quot;k8s.io/apimachinery/pkg/util/json&quot;</span> <span class="hljs-comment">// For JSON Patch</span><br>    <span class="hljs-string">&quot;k8s.io/client-go/kubernetes&quot;</span><br>    <span class="hljs-string">&quot;k8s.io/client-go/rest&quot;</span><br>    metav1 <span class="hljs-string">&quot;k8s.io/apimachinery/pkg/apis/meta/v1&quot;</span><br>    corev1 <span class="hljs-string">&quot;k8s.io/api/core/v1&quot;</span> <span class="hljs-comment">// For Node type, though patch might not need it directly</span><br>)<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">updateNodeReclaimedCPU</span><span class="hljs-params">(clientset *kubernetes.Clientset, nodeName <span class="hljs-type">string</span>, reclaimedCPUValue <span class="hljs-type">string</span>)</span></span> <span class="hljs-type">error</span> &#123;<br>    patchPayload := []<span class="hljs-keyword">map</span>[<span class="hljs-type">string</span>]<span class="hljs-keyword">interface</span>&#123;&#125;&#123;<br>        &#123;<br>            <span class="hljs-string">&quot;op&quot;</span>:    <span class="hljs-string">&quot;add&quot;</span>, <span class="hljs-comment">// Use &quot;replace&quot; if the path might already exist, or check first</span><br>            <span class="hljs-string">&quot;path&quot;</span>:  <span class="hljs-string">&quot;/status/capacity/example.com~1reclaimed-cpu&quot;</span>, <span class="hljs-comment">// ~1 is JSON Pointer escape for /</span><br>            <span class="hljs-string">&quot;value&quot;</span>: reclaimedCPUValue, <span class="hljs-comment">// e.g., &quot;1500m&quot; or &quot;2&quot;</span><br>        &#125;,<br>    &#125;<br>    patchBytes, _ := json.Marshal(patchPayload)<br><br>    _, err := clientset.CoreV1().Nodes().Patch(context.TODO(), nodeName, types.JSONPatchType, patchBytes, metav1.PatchOptions&#123;&#125;, <span class="hljs-string">&quot;status&quot;</span>)<br>    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>        <span class="hljs-comment">// Handle error (e.g., log, retry logic)</span><br>        <span class="hljs-keyword">return</span> fmt.Errorf(<span class="hljs-string">&quot;failed to patch node %s status: %v&quot;</span>, nodeName, err)<br>    &#125;<br>    fmt.Printf(<span class="hljs-string">&quot;Successfully patched node %s with reclaimed-cpu: %s\n&quot;</span>, nodeName, reclaimedCPUValue)<br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>&#125;<br><br><span class="hljs-comment">// In the main loop of the Agent:</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">agentLoop</span><span class="hljs-params">(clientset *kubernetes.Clientset, nodeName <span class="hljs-type">string</span>)</span></span> &#123;<br>    ticker := time.NewTicker(<span class="hljs-number">30</span> * time.Second)<br>    <span class="hljs-keyword">defer</span> ticker.Stop()<br><br>    <span class="hljs-keyword">for</span> <span class="hljs-keyword">range</span> ticker.C &#123;<br>        <span class="hljs-comment">// 1. Monitor CPU usage (implementation depends on metrics source)</span><br>        actualUsageCPU := getCriticalPodsUsage() <span class="hljs-comment">// Placeholder function</span><br>        nodeCapacityCPU := getNodeCapacity()     <span class="hljs-comment">// Placeholder function</span><br>        safetyBuffer := getSafetyBuffer()      <span class="hljs-comment">// Placeholder function</span><br><br>        <span class="hljs-comment">// 2. Calculate reclaimed CPU (ensure non-negative)</span><br>        reclaimedMilliCPU := nodeCapacityCPU*<span class="hljs-number">1000</span> - actualUsageCPU*<span class="hljs-number">1000</span> - safetyBuffer*<span class="hljs-number">1000</span><br>        <span class="hljs-keyword">if</span> reclaimedMilliCPU &lt; <span class="hljs-number">0</span> &#123;<br>            reclaimedMilliCPU = <span class="hljs-number">0</span><br>        &#125;<br>        reclaimedCPUValue := fmt.Sprintf(<span class="hljs-string">&quot;%dm&quot;</span>, reclaimedMilliCPU)<br><br>        <span class="hljs-comment">// 3. Get current value to decide if update is needed (optional optimization)</span><br>        <span class="hljs-comment">// currentNode, _ := clientset.CoreV1().Nodes().Get(context.TODO(), nodeName, metav1.GetOptions&#123;&#125;)</span><br>        <span class="hljs-comment">// currentReclaimed, _ := currentNode.Status.Capacity[corev1.ResourceName(&quot;example.com/reclaimed-cpu&quot;)]</span><br>        <span class="hljs-comment">// if currentReclaimed.String() == reclaimedCPUValue &#123; continue &#125;</span><br>        <span class="hljs-comment">// 4. Update Node Status via PATCH</span><br>        err := updateNodeReclaimedCPU(clientset, nodeName, reclaimedCPUValue)<br>        <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>            fmt.Fprintf(os.Stderr, <span class="hljs-string">&quot;Error updating node status: %v\n&quot;</span>, err)<br>        &#125;<br>    &#125;<br>&#125;<br><br><span class="hljs-comment">// Note: The Agent needs RBAC permissions to Get and Patch Node Status</span><br><span class="hljs-comment">// Ensure its ServiceAccount is bound to a Role/ClusterRole with these permissions.</span><br></code></pre></td></tr></table></figure></li>
</ul>
<p><strong>Agent 所需 RBAC (示例 ClusterRole):</strong></p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">idle-resource-agent-role</span><br><span class="hljs-attr">rules:</span><br><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> [<span class="hljs-string">&quot;&quot;</span>]<br>  <span class="hljs-attr">resources:</span> [<span class="hljs-string">&quot;nodes/status&quot;</span>]<br>  <span class="hljs-attr">verbs:</span> [<span class="hljs-string">&quot;get&quot;</span>, <span class="hljs-string">&quot;patch&quot;</span>, <span class="hljs-string">&quot;update&quot;</span>] <span class="hljs-comment"># Patch is generally preferred for status updates</span><br><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> [<span class="hljs-string">&quot;&quot;</span>]<br>  <span class="hljs-attr">resources:</span> [<span class="hljs-string">&quot;nodes&quot;</span>]<br>  <span class="hljs-attr">verbs:</span> [<span class="hljs-string">&quot;get&quot;</span>, <span class="hljs-string">&quot;list&quot;</span>, <span class="hljs-string">&quot;watch&quot;</span>] <span class="hljs-comment"># Needed to get node capacity, etc.</span><br><span class="hljs-comment"># Might need more permissions depending on how it monitors pod usage (e.g., access to metrics endpoints)</span><br></code></pre></td></tr></table></figure>

<ul>
<li><strong>处理零资源</strong>：当计算出的可回收 CPU 小于等于零时，Agent 应将 Node Status 中的 <code>example.com/reclaimed-cpu</code> 更新为 <code>0</code> 或完全移除该条目，防止新的低优先级 Pod 被调度到该节点。</li>
</ul>
<p><strong>3. 配置低优先级 Pod 请求扩展资源:</strong></p>
<p>现在，对于那些可以容忍资源波动、优先级较低的“波谷”工作负载（例如，一个批处理 Job），在定义它们的 Pod Spec 时：</p>
<ul>
<li><p><strong>不请求</strong>或请求<strong>极少量</strong>的<strong>标准 <code>cpu</code> 资源</strong>。</p>
</li>
<li><p>在 <code>resources.limits</code> 中请求<strong>自定义的 <code>example.com/reclaimed-cpu</code> 资源</strong>。记住，对于扩展资源，<code>requests</code> 必须等于 <code>limits</code>（或者省略 <code>requests</code>，它会默认等于 <code>limits</code>）。</p>
<p><strong>示例 Job YAML:</strong></p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">batch/v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Job</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">batch-processing-job</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">template:</span><br>    <span class="hljs-attr">spec:</span><br>      <span class="hljs-attr">containers:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">processor</span><br>        <span class="hljs-attr">image:</span> <span class="hljs-string">my-batch-processor:latest</span><br>        <span class="hljs-attr">resources:</span><br>          <span class="hljs-attr">limits:</span><br>            <span class="hljs-comment"># Only request the reclaimed CPU, not standard CPU</span><br>            <span class="hljs-attr">example.com/reclaimed-cpu:</span> <span class="hljs-string">&quot;1000m&quot;</span> <span class="hljs-comment"># Request 1 reclaimed CPU core</span><br>            <span class="hljs-attr">memory:</span> <span class="hljs-string">&quot;2Gi&quot;</span> <span class="hljs-comment"># Still request memory as usual</span><br>          <span class="hljs-attr">requests:</span><br>            <span class="hljs-comment"># Requests must equal limits for extended resources</span><br>            <span class="hljs-attr">example.com/reclaimed-cpu:</span> <span class="hljs-string">&quot;1000m&quot;</span><br>            <span class="hljs-attr">memory:</span> <span class="hljs-string">&quot;2Gi&quot;</span><br>      <span class="hljs-attr">restartPolicy:</span> <span class="hljs-string">Never</span> <span class="hljs-comment"># Or OnFailure, typical for Jobs</span><br>      <span class="hljs-comment"># Important: Add tolerations or node selectors if the agent only runs on specific nodes</span><br>      <span class="hljs-comment"># topologySpreadConstraints might also be useful</span><br>      <span class="hljs-comment"># Consider setting a low priorityClassName</span><br>      <span class="hljs-attr">priorityClassName:</span> <span class="hljs-string">low-priority</span> <span class="hljs-comment"># Assuming you have defined this class</span><br></code></pre></td></tr></table></figure></li>
</ul>
<p><strong>4. 工作流程与调度:</strong></p>
<ul>
<li>Agent 持续运行，根据实际负载动态更新各节点上 <code>example.com/reclaimed-cpu</code> 的 <code>capacity</code>（Kubelet 会相应更新 <code>allocatable</code>）。</li>
<li>当用户提交上述 <code>batch-processing-job</code> 时，<strong>kube-scheduler</strong> 会寻找满足其资源请求的节点。</li>
<li>调度器只会考虑那些当前 <code>.status.allocatable</code> 中 <code>example.com/reclaimed-cpu</code> <strong>大于或等于 1000m</strong> 的节点。</li>
<li>Job 的 Pod 会被调度到有足够“空闲”资源的节点上运行。</li>
<li>如果某个节点上的高优先级应用负载上升，Agent 会减少或清零该节点的 <code>reclaimed-cpu</code> 容量，新的低优先级 Pod 就不会再被调度到该节点。</li>
</ul>
<h3 id="如何实现降本增效"><a href="#如何实现降本增效" class="headerlink" title="如何实现降本增效"></a>如何实现降本增效</h3><ol>
<li><strong>提高资源利用率</strong>：原本闲置的 CPU 周期被有效利用起来运行额外的（低优先级）工作负载，提高了整个集群的资源利用率。</li>
<li><strong>避免过度配置</strong>：可以更精细地控制资源分配，减少为应对峰值而预留的、大部分时间闲置的资源量。</li>
<li><strong>降低成本</strong>：<ul>
<li>在<strong>公有云</strong>上，更高的资源利用率意味着可以用<strong>更少或更小</strong>的节点实例来承载相同的工作负载总量，直接降低虚拟机费用。</li>
<li>在<strong>私有云或物理部署</strong>中，可以支持更多的业务，延缓硬件采购需求。</li>
</ul>
</li>
<li><strong>保障核心业务</strong>：通过 <code>SafetyBuffer</code> 和仅让低优先级任务使用 <code>reclaimed-cpu</code> 的方式，最大限度地减少了对高优先级在线服务性能的影响。</li>
</ol>
<h2 id="7-构建和管理高可用（HA）集群"><a href="#7-构建和管理高可用（HA）集群" class="headerlink" title="7. 构建和管理高可用（HA）集群"></a>7. 构建和管理高可用（HA）集群</h2><h3 id="Kubernetes-高可用层级"><a href="#Kubernetes-高可用层级" class="headerlink" title="Kubernetes 高可用层级"></a>Kubernetes 高可用层级</h3><img src="/2025/04/24/kubernetes%E7%94%9F%E4%BA%A7%E5%8C%96%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/image-20250428090416959.png" srcset="/img/loading.gif" lazyload class="" title="image-20250428090416959">

<h4 id="基础架构管理-Infrastructure-Management"><a href="#基础架构管理-Infrastructure-Management" class="headerlink" title="基础架构管理 (Infrastructure Management)"></a>基础架构管理 (Infrastructure Management)</h4><p>这是 Kubernetes 集群运行的物理或虚拟基础。<strong>高可用的基石在于底层基础设施的稳定性与冗余性</strong>。如果底层硬件、网络或操作系统频繁故障，上层的 Kubernetes 也难以保证高可用。</p>
<ul>
<li><strong>主机上架 (Server Provisioning):</strong> 需要有多台物理或虚拟机作为 Kubernetes 的 Master 和 Worker 节点。在关键集群中，这些主机应分布在不同的<strong>故障域 (Failure Domains)</strong>，例如不同的机架、不同的可用区（Availability Zones, AZs），以防止单点物理故障。</li>
<li><strong>OS 管理 (OS Management):</strong> 运行稳定且经过安全加固的 Linux 发行版（如 Ubuntu, CentOS, RHEL）。内核版本、系统参数（如 <code>fs.file-max</code>, <code>net.core.somaxconn</code>）需要根据集群规模和负载进行优化。操作系统的补丁管理和升级策略也需要考虑对集群服务的影响。</li>
<li><strong>安全策略 (Security Policies):</strong> 网络隔离（如使用安全组、防火墙 <code>iptables</code>&#x2F;<code>nftables</code>）、主机访问控制、镜像安全扫描等，虽然不直接等同于 HA，但安全事件可能导致服务中断，因此也是广义可用性的一部分。</li>
<li><strong>主机网络 (Host Networking):</strong> <strong>网络冗余是基础设施 HA 的关键</strong>。通常会配置多块网卡进行<strong>绑定 (Bonding)</strong>，例如使用 Linux 内核的 <code>bonding</code> 模块，配置 <code>active-backup</code> 或 <code>LACP (802.3ad)</code> 模式，确保单块网卡或交换机端口故障时不影响节点网络连接。底层网络架构也应具备冗余性（如冗余交换机、路由器）。</li>
<li><strong>Container Runtime:</strong> Docker、containerd 或 CRI-O 等容器运行时本身需要稳定可靠。虽然运行时本身通常不直接做 HA 集群，但其稳定运行是 Pod 能否正常启动和运行的前提。它们的配置（如存储驱动、网络插件接口）会影响性能和稳定性。</li>
</ul>
<h4 id="集群管理-Cluster-Management"><a href="#集群管理-Cluster-Management" class="headerlink" title="集群管理 (Cluster Management)"></a>集群管理 (Cluster Management)</h4><p>这一层关注 Kubernetes 集群本身的搭建、维护和管理，确保集群作为一个整体是可用的。</p>
<ul>
<li><strong>集群安装 (Cluster Installation):</strong> 使用如 <code>kubeadm</code>、<code>k3s</code>、<code>RKE</code> 或商业发行版安装 K8s。安装过程需要规划好<strong>控制平面节点 (Control Plane Nodes) 和工作节点 (Worker Nodes)</strong> 的数量和分布。</li>
<li><strong>节点管理 (Node Management):</strong> 集群管理员需要监控节点状态（<code>kubectl get nodes</code>），处理 <code>NotReady</code> 状态的节点，执行节点维护（如内核升级、硬件更换）时的<strong>排空 (Drain)</strong> 操作 (<code>kubectl drain &lt;node-name&gt;</code>)，以优雅地迁移 Pod。</li>
<li><strong>认证授权 (Authentication &amp; Authorization):</strong> RBAC (Role-Based Access Control) 等机制虽然主要关注安全，但也关系到哪些用户或服务账号可以执行可能影响可用性的操作（如删除 Deployment）。</li>
<li><strong>网络 (Networking):</strong> <strong>CNI (Container Network Interface) 插件的选择和配置至关重要</strong>。例如 Calico、Flannel、Cilium 等。一些 CNI 插件（如 Calico BGP 模式）自身可以配置路由冗余。网络策略 (NetworkPolicy) 用于隔离 Pod 间流量，防止故障扩散。</li>
<li><strong>存储 (Storage):</strong> <strong>CSI (Container Storage Interface) 驱动的选择和配置直接关系到有状态应用的数据持久性和可用性</strong>。需要使用支持动态卷分配 (Dynamic Provisioning) 和跨节点挂载的存储解决方案（如 Ceph、NFS、云厂商块存储）。存储系统自身的 HA 是有状态应用 HA 的前提。</li>
<li><strong>配额管理 (Resource Quotas):</strong> 通过 ResourceQuota 和 LimitRange 限制命名空间或 Pod 的资源使用，防止某个应用耗尽节点资源导致其他应用甚至节点本身不可用。</li>
<li><strong>备份恢复 (Backup &amp; Restore):</strong> <strong><code>etcd</code> 是 Kubernetes 的状态存储核心，其备份和恢复机制是集群灾难恢复的关键</strong>。通常需要定期对 <code>etcd</code> 进行快照备份，并演练恢复流程。</li>
</ul>
<h4 id="控制平面-Control-Plane"><a href="#控制平面-Control-Plane" class="headerlink" title="控制平面 (Control Plane)"></a>控制平面 (Control Plane)</h4><p>这是 Kubernetes 的“大脑”，负责集群决策和状态管理。<strong>控制平面的高可用是 Kubernetes 集群 HA 的核心</strong>。</p>
<ul>
<li><p><strong>核心组件 (Core Components):</strong></p>
<ul>
<li><p><strong><code>etcd</code>:</strong> 分布式键值存储，保存集群的所有状态数据。<strong>必须部署为集群模式（通常 3 或 5 个节点）</strong>，利用 Raft 协议保证数据一致性和容错性。只要超过半数的 <code>etcd</code> 节点存活，<code>etcd</code> 集群就能正常工作。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 查看 etcd 集群成员状态 (需在 etcd Pod 或有 etcdctl 的地方执行)</span><br>ETCDCTL_API=3 etcdctl --endpoints=https://[ETCD_IP]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key member list<br></code></pre></td></tr></table></figure>
</li>
<li><p><strong><code>kube-apiserver</code>:</strong> 集群的入口，处理所有 API 请求。<strong>需要部署多个实例</strong>，并在它们前面放置一个<strong>负载均衡器 (Load Balancer)</strong>（可以是云厂商的 ELB，也可以是自建的 HAProxy&#x2F;Nginx 或 Keepalived+LVS）。所有其他组件（<code>kubectl</code>, <code>kubelet</code>, controllers）都通过这个负载均衡器的 VIP 或 DNS 地址访问 API Server。</p>
</li>
<li><p><strong><code>kube-controller-manager</code>:</strong> 运行各种控制器（如 Deployment Controller, Node Controller）。<strong>同一时间只有一个实例是 active (leader)</strong>，其他实例处于 standby 状态。它们通过<strong>租约 (Lease) 对象或 Endpoint 对象进行领导者选举 (Leader Election)</strong>。如果当前 leader 故障，其他实例会竞选成为新的 leader。这个机制通常是利用 Kubernetes API 实现的，例如 Golang 的 <code>client-go</code> 库提供了 <code>leaderelection</code> 包。</p>
</li>
<li><p><strong><code>kube-scheduler</code>:</strong> 负责将 Pod 调度到合适的 Node 上。与 Controller Manager 类似，<strong>也需要部署多个实例并进行领导者选举</strong>。</p>
</li>
</ul>
</li>
<li><p><strong>插件 (Plugins) &amp; 用户空间控制器 (Custom Controllers):</strong> 部署在集群中用于扩展功能的组件（如 metrics-server, ingress controller, cert-manager 等）也需要考虑自身的 HA，通常也是通过部署多个副本和 Leader Election（如果需要）来实现。</p>
</li>
<li><p><strong>Assertion:</strong> 这可能指的是集群健康检查和断言机制，确保集群状态符合预期。</p>
</li>
</ul>
<h4 id="数据平面-Data-Plane"><a href="#数据平面-Data-Plane" class="headerlink" title="数据平面 (Data Plane)"></a>数据平面 (Data Plane)</h4><p>这是用户应用负载实际运行的地方。数据平面的高可用关注的是应用本身在节点故障、Pod 故障等情况下的持续服务能力。</p>
<ul>
<li><p><strong>Pod:</strong> <strong>运行应用的最小单元。本身是短暂的 (Ephemeral)</strong>。高可用不依赖单个 Pod 的存活，而是依赖于<strong>多个 Pod 副本 (Replicas)</strong>。</p>
</li>
<li><p><strong>PVC (PersistentVolumeClaim):</strong> 应用对持久化存储的请求。需要由可靠的、支持跨节点访问的 PV (PersistentVolume) 来满足，如前述的网络存储。确保在 Pod 漂移到新节点后，数据卷能够被重新挂载。</p>
</li>
<li><p><strong>Service:</strong> <strong>提供了一个稳定的访问入口（ClusterIP, NodePort, LoadBalancer）来访问一组 Pod</strong>。它通过 <code>kube-proxy</code>（运行在每个 Node 上）在 Linux 内核中利用 <code>iptables</code> 或 <code>IPVS</code> 规则实现负载均衡。当一个 Pod 故障并被控制器替换后，Service 会自动将流量转发到健康的 Pod 上。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># Service 示例：为 app=my-app 的 Pod 提供服务</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">my-app-svc</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">selector:</span><br>    <span class="hljs-attr">app:</span> <span class="hljs-string">my-app</span> <span class="hljs-comment"># 选择标签为 app=my-app 的 Pod</span><br>  <span class="hljs-attr">ports:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span><br>      <span class="hljs-attr">port:</span> <span class="hljs-number">80</span>       <span class="hljs-comment"># Service 暴露的端口</span><br>      <span class="hljs-attr">targetPort:</span> <span class="hljs-number">8080</span> <span class="hljs-comment"># Pod 容器监听的端口</span><br>  <span class="hljs-attr">type:</span> <span class="hljs-string">ClusterIP</span>    <span class="hljs-comment"># 或 NodePort, LoadBalancer</span><br></code></pre></td></tr></table></figure>
</li>
<li><p><strong>Ingress:</strong> 管理对集群内部 Service 的外部访问（通常是 HTTP&#x2F;HTTPS）。<strong>Ingress Controller (如 Nginx Ingress, Traefik) 本身也需要部署多个副本</strong>，并通常通过 Service of type LoadBalancer 或 NodePort 对外暴露。Ingress Controller 的 HA 确保了外部流量入口的可用性。</p>
</li>
</ul>
<h4 id="分级部署-Staged-Deployment-Application-Focus"><a href="#分级部署-Staged-Deployment-Application-Focus" class="headerlink" title="分级部署 (Staged Deployment - Application Focus)"></a>分级部署 (Staged Deployment - Application Focus)</h4><p>这部分更侧重于应用开发者如何利用 Kubernetes 机制来保障其应用的高可用。</p>
<ul>
<li><p><strong>资源需求 (Resource Requests&#x2F;Limits):</strong> 正确设置 Pod 的 CPU 和 Memory Requests&#x2F;Limits，确保 Pod 获得必要的资源，避免因资源不足而被 OOMKilled 或影响性能，同时也帮助 Scheduler 做出更合理的调度决策。</p>
</li>
<li><p><strong>接入需求 (Ingress Needs):</strong> 应用需要被外部访问时，定义 Ingress 规则。</p>
</li>
<li><p><strong>亲和性 (Affinity&#x2F;Anti-Affinity):</strong> <strong>使用 Pod (Anti-)Affinity 规则来控制 Pod 的部署位置</strong>。例如，使用 <code>podAntiAffinity</code> 要求一个 Deployment 的多个 Pod 分散到不同的节点或可用区，提高容错性。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># Pod Anti-Affinity 示例：尽量不将同一应用的 Pod 调度到同一节点</span><br><span class="hljs-attr">affinity:</span><br>  <span class="hljs-attr">podAntiAffinity:</span><br>    <span class="hljs-attr">requiredDuringSchedulingIgnoredDuringExecution:</span> <span class="hljs-comment"># 或 preferredDuringSchedulingIgnoredDuringExecution</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-attr">labelSelector:</span><br>        <span class="hljs-attr">matchExpressions:</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">app</span><br>          <span class="hljs-attr">operator:</span> <span class="hljs-string">In</span><br>          <span class="hljs-attr">values:</span><br>          <span class="hljs-bullet">-</span> <span class="hljs-string">my-app</span> <span class="hljs-comment"># 与自身的 app 标签匹配</span><br>      <span class="hljs-attr">topologyKey:</span> <span class="hljs-string">&quot;kubernetes.io/hostname&quot;</span> <span class="hljs-comment"># 拓扑域为节点主机名</span><br></code></pre></td></tr></table></figure>
</li>
<li><p><strong>横向扩展 (Horizontal Scaling):</strong> 使用 <strong>Deployment 或 StatefulSet 管理 Pod 副本</strong>，并配置 <strong>HorizontalPodAutoscaler (HPA)</strong> 根据 CPU、内存使用率或其他自定义指标自动增减 Pod 数量，应对负载变化，保证服务能力。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># HPA 示例：根据 CPU 使用率自动伸缩 Deployment</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">autoscaling/v2</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">HorizontalPodAutoscaler</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">my-app-hpa</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">scaleTargetRef:</span><br>    <span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span><br>    <span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span><br>    <span class="hljs-attr">name:</span> <span class="hljs-string">my-app-deployment</span> <span class="hljs-comment"># 要伸缩的目标 Deployment</span><br>  <span class="hljs-attr">minReplicas:</span> <span class="hljs-number">2</span><br>  <span class="hljs-attr">maxReplicas:</span> <span class="hljs-number">10</span><br>  <span class="hljs-attr">metrics:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">type:</span> <span class="hljs-string">Resource</span><br>    <span class="hljs-attr">resource:</span><br>      <span class="hljs-attr">name:</span> <span class="hljs-string">cpu</span><br>      <span class="hljs-attr">target:</span><br>        <span class="hljs-attr">type:</span> <span class="hljs-string">Utilization</span><br>        <span class="hljs-attr">averageUtilization:</span> <span class="hljs-number">80</span> <span class="hljs-comment"># CPU 使用率达到 80% 时扩容</span><br></code></pre></td></tr></table></figure>
</li>
<li><p><strong>服务注册&#x2F;发现 (Service Registration&#x2F;Discovery):</strong> Kubernetes 内置了基于 DNS (CoreDNS&#x2F;KubeDNS) 和 Service 的服务发现机制。应用可以通过 Service 名称（如 <code>my-app-svc.my-namespace.svc.cluster.local</code>）来访问其他服务，无需关心 Pod 的具体 IP 地址。</p>
</li>
<li><p><strong>健康检查 (Health Checks):</strong> <strong>配置 Liveness Probe 和 Readiness Probe</strong>。Liveness Probe 用于检测容器是否存活，如果不存活，<code>kubelet</code> 会杀死并重启容器。Readiness Probe 用于检测容器是否准备好接收流量，如果未就绪，会被从 Service 的 Endpoints 列表中移除。<strong>这是保证 Service 流量只打到健康 Pod 上的关键</strong>。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># Probe 示例</span><br><span class="hljs-attr">livenessProbe:</span><br>  <span class="hljs-attr">httpGet:</span><br>    <span class="hljs-attr">path:</span> <span class="hljs-string">/healthz</span><br>    <span class="hljs-attr">port:</span> <span class="hljs-number">8080</span><br>  <span class="hljs-attr">initialDelaySeconds:</span> <span class="hljs-number">15</span><br>  <span class="hljs-attr">periodSeconds:</span> <span class="hljs-number">20</span><br><span class="hljs-attr">readinessProbe:</span><br>  <span class="hljs-attr">httpGet:</span><br>    <span class="hljs-attr">path:</span> <span class="hljs-string">/readyz</span><br>    <span class="hljs-attr">port:</span> <span class="hljs-number">8080</span><br>  <span class="hljs-attr">initialDelaySeconds:</span> <span class="hljs-number">5</span><br>  <span class="hljs-attr">periodSeconds:</span> <span class="hljs-number">10</span><br></code></pre></td></tr></table></figure>
</li>
<li><p><strong>滚动更新与中断预算 (Rolling Updates &amp; PodDisruptionBudget):</strong> Deployment 默认使用滚动更新策略，保证更新过程中服务不中断。<strong>配置 PodDisruptionBudget (PDB)</strong> 可以确保在自愿性中断（如节点维护）期间，至少有多少个 Pod 副本是可用的。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># PDB 示例：确保 my-app 至少有 2 个副本可用</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">policy/v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">PodDisruptionBudget</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">my-app-pdb</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">minAvailable:</span> <span class="hljs-number">2</span> <span class="hljs-comment"># 或 maxUnavailable: 1</span><br>  <span class="hljs-attr">selector:</span><br>    <span class="hljs-attr">matchLabels:</span><br>      <span class="hljs-attr">app:</span> <span class="hljs-string">my-app</span><br></code></pre></td></tr></table></figure></li>
</ul>
<h4 id="应用开发-Application-Development"><a href="#应用开发-Application-Development" class="headerlink" title="应用开发 (Application Development)"></a>应用开发 (Application Development)</h4><p>开发者自身在编码层面也需要考虑高可用。</p>
<ul>
<li><strong>应用管理 (Application Management):</strong> 设计为<strong>无状态 (Stateless)</strong> 服务是最佳实践，易于水平扩展和替换。如果是有状态服务，需要妥善处理状态的持久化和一致性。</li>
<li><strong>服务开发 (Service Development):</strong> 应用需要能够处理<strong>优雅终止 (Graceful Shutdown)</strong>。当收到 <code>SIGTERM</code> 信号时（Pod 删除前会发送），应用应完成当前请求，释放资源，然后退出。否则强制终止 (<code>SIGKILL</code>) 可能导致请求失败或数据不一致。</li>
<li><strong>持续集成 (Continuous Integration):</strong> CI&#x2F;CD 流程的稳定性和自动化程度也影响着应用部署和更新的可靠性。</li>
</ul>
<h4 id="Kubernetes-公共服务-企业公共服务"><a href="#Kubernetes-公共服务-企业公共服务" class="headerlink" title="Kubernetes 公共服务 &amp; 企业公共服务"></a>Kubernetes 公共服务 &amp; 企业公共服务</h4><p>这些是集群运行所依赖的支撑服务，它们自身也需要高可用。</p>
<ul>
<li><strong>日志&#x2F;监控&#x2F;告警:</strong> Prometheus&#x2F;Thanos、Elasticsearch&#x2F;Fluentd&#x2F;Kibana (EFK)、Loki&#x2F;Grafana&#x2F;Promtail (PLG) 等组件通常都需要部署为 HA 模式（多副本、数据冗余）。</li>
<li><strong>本地&#x2F;企业镜像仓库:</strong> Harbor 等镜像仓库需要保证高可用，否则应用无法部署和更新。</li>
<li><strong>四层&#x2F;七层代理&#x2F;Service Mesh:</strong> 除了 Ingress Controller，Service Mesh (如 Istio, Linkerd) 的控制平面和数据平面也需要高可用配置。</li>
<li><strong>KubeDNS&#x2F;CoreDNS:</strong> 作为集群内部服务发现的核心，必须部署多个副本，并利用 Kubernetes Service 对外提供服务。</li>
<li><strong>Jenkins&#x2F;CI&#x2F;CD:</strong> CI&#x2F;CD 平台的高可用保证了开发和部署流程的顺畅。</li>
<li><strong>企业 DNS, ELB, 用户管理:</strong> 这些外部依赖（通常由企业 IT 部门或云厂商提供）的可用性直接影响 Kubernetes 集群和应用的对外服务能力。</li>
</ul>
<hr>
<h3 id="高可用的数据中心-Highly-Available-Data-Center"><a href="#高可用的数据中心-Highly-Available-Data-Center" class="headerlink" title="高可用的数据中心 (Highly Available Data Center)"></a>高可用的数据中心 (Highly Available Data Center)</h3><p>这部分内容描述了实现 Kubernetes 或任何关键业务系统高可用的物理基础设施层面的要求。它是整个高可用体系的<strong>物理基石</strong>。如果数据中心本身存在单点故障，那么上层所有的 HA 机制都可能失效。</p>
<ul>
<li><p><strong>多地部署 (Multi-Site Deployment):</strong> 这是最高级别的物理容灾策略。将系统部署在地理位置分散的多个数据中心，可以抵御区域性的灾难（如地震、洪水、大规模断电）。每个数据中心都是一个独立的故障域。虽然 Kubernetes 本身可以通过 Federation v2 或其他多集群管理工具（如 Karmada）实现跨数据中心的管理和应用分发，但这通常带来了更高的复杂性、成本和潜在的网络延迟问题。对于大多数组织而言，在单个地理区域内的多个可用区部署是更常见的实践。</p>
</li>
<li><p><strong>高可用区 (Availability Zones - AZs):</strong> 这是在单个地理区域（Region）内实现高可用的核心概念。<strong>每个数据中心需要划分成多个具有独立供电、制冷、网络设备的高可用区（AZ）</strong>。这里的“独立”意味着一个 AZ 的基础设施故障（如断电、空调故障、核心交换机故障）不应影响到其他 AZ。<strong>AZ 的设计目标是隔离物理故障</strong>，使得单个基础设施组件的失败不会导致整个数据中心或区域的服务中断。在公有云环境中，如 AWS、Azure、GCP，AZ 是标准的物理隔离单元。在自建数据中心，也需要参照此原则进行规划。</p>
</li>
<li><p><strong>独立硬件资产 (Independent Hardware Assets):</strong> <strong>每个高可用区管理独立的硬件资产</strong>，这是实现 AZ 隔离的具体手段。这包括：</p>
<ul>
<li><strong>机架 (Racks):</strong> 服务器应分布在不同 AZ 的不同机架上。</li>
<li><strong>计算节点 (Compute Nodes):</strong> 即运行 Kubernetes Master和 Worker 角色的物理或虚拟机。关键的控制平面节点（<code>etcd</code>, <code>api-server</code> 等）和应用负载的工作节点都应<strong>跨 AZ 部署</strong>，以确保在一个 AZ 完全故障时，集群和其他 AZ 中的应用仍然可用。Kubernetes 通过<strong>节点标签 (Node Labels)</strong> 和<strong>拓扑感知调度 (Topology Aware Scheduling)</strong> 来支持这种跨 AZ 部署。例如，节点通常会被打上 <code>topology.kubernetes.io/zone=us-east-1a</code> 这样的标签，调度器和控制器（如 StatefulSet 的 <code>podAntiAffinity</code>）可以利用这些标签来分散 Pod。</li>
<li><strong>存储 (Storage):</strong> 持久化存储系统本身需要高可用，并且最好是 AZ 感知的。例如，使用 Ceph 并配置 CRUSH 规则将数据副本分布到不同的 AZ；或者使用云厂商提供的跨 AZ 复制的块存储或文件存储服务。这确保了当一个 AZ 的存储不可用时，数据仍然可以从其他 AZ 访问（可能需要应用或 K8s 层面的故障转移）。</li>
<li><strong>负载均衡器 (Load Balancers):</strong> 无论是硬件 F5 还是软件 Nginx&#x2F;HAProxy，或者是云厂商的 ELB&#x2F;ALB，都需要有冗余部署，并且能够跨 AZ 分发流量。入口流量的负载均衡器是外部用户访问集群服务的关键路径。</li>
<li><strong>防火墙 (Firewalls):</strong> 网络边界的安全设备也需要冗余部署，通常是主备或集群模式，分布在不同的 AZ 或至少有冗余的网络路径接入。</li>
</ul>
</li>
</ul>
<p>总而言之，高可用的数据中心设计通过<strong>物理层面的冗余和故障域隔离（主要是 AZ）</strong>，为上层的 Kubernetes 集群和应用提供了稳定可靠运行的基础环境。没有这一层，软件层面的 HA 措施效果将大打折扣。</p>
<h3 id="Node-的生命周期管理-Node-Lifecycle-Management"><a href="#Node-的生命周期管理-Node-Lifecycle-Management" class="headerlink" title="Node 的生命周期管理 (Node Lifecycle Management)"></a>Node 的生命周期管理 (Node Lifecycle Management)</h3><p>这部分描述了 Kubernetes 集群运维中的一个核心任务：管理集群中计算节点（Node）从加入到退出的整个过程。<strong>运营 Kubernetes 集群不仅仅是初始搭建，更重要的是持续地、自动化地管理节点的生命周期，以保证集群的健康、弹性和资源充足</strong>。核心是 Node 的生命周期管理，而不是简单的扩容和缩容。</p>
<ul>
<li><strong>集群搭建 (Cluster Setup):</strong> 这是生命周期的起点，但不是管理的全部。</li>
<li><strong>集群扩容&#x2F;缩容 (Cluster Scaling):</strong> 这是生命周期中最常见的操作之一，涉及节点的加入 (Onboard) 和移除 (Offboard)。</li>
<li><strong>集群销毁 (Cluster Decommissioning):</strong> 很少发生，但涉及所有节点的 Offboard。</li>
</ul>
<p>生命周期管理主要包含以下阶段：</p>
<ul>
<li><p><strong>Onboard (节点加入&#x2F;扩容):</strong> 将一个新的物理机或虚拟机添加到 Kubernetes 集群作为工作节点的过程。</p>
<ul>
<li><p><strong>物理资产上架 (Physical Asset Provisioning):</strong> 准备好服务器硬件。</p>
</li>
<li><p><strong>操作系统安装 (OS Installation):</strong> 安装符合要求的 Linux 操作系统（内核版本、依赖包等）。</p>
</li>
<li><p><strong>网络配置 (Network Configuration):</strong> 配置服务器 IP 地址、网关、DNS，确保它可以访问 Kubernetes 控制平面和其他节点，并满足 CNI 插件的网络要求（例如，可能需要特定的 MTU 设置或路由配置）。</p>
</li>
<li><p><strong>Kubernetes 组件安装 (K8s Component Installation):</strong> 安装<strong>容器运行时 (Container Runtime)</strong>（如 containerd, Docker）、<strong><code>kubelet</code></strong>（负责管理节点上的 Pod 和容器，与 API Server 通信）和 <strong><code>kube-proxy</code></strong>（负责实现 Kubernetes Service 的网络规则，通常通过 <code>iptables</code> 或 <code>IPVS</code>。<code>IPVS</code> 利用 Linux 内核的 Netfilter 和 IP Virtual Server 提供更高效的负载均衡）。</p>
</li>
<li><p><strong>创建 Node 对象 (Node Object Creation):</strong> 通常使用 <code>kubeadm join</code> 命令将节点加入集群。这个过程包括 TLS 引导（安全地获取证书与 API Server 通信）、<code>kubelet</code> 向 API Server 注册自身。成功注册后，API Server 中会创建一个 <strong>Node 对象</strong>，代表这个新加入的机器。集群管理员可以通过 <code>kubectl get nodes</code> 查看到这个新节点，并且其状态应为 <code>Ready</code>。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 示例：一个新节点加入集群（使用 kubeadm）</span><br><span class="hljs-comment"># 在 Master 节点上生成 join 命令</span><br><span class="hljs-comment"># kubeadm token create --print-join-command</span><br><span class="hljs-comment"># 在新 Worker 节点上执行 Master 节点生成的 join 命令</span><br><span class="hljs-comment"># sudo kubeadm join &lt;control-plane-host&gt;:&lt;control-plane-port&gt; --token &lt;token&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;</span><br></code></pre></td></tr></table></figure>

<p>此时，该节点变为可调度状态（除非被标记为 <code>Unschedulable</code>），<code>kube-scheduler</code> 可以将新的 Pod 分配到这个节点上。</p>
</li>
</ul>
</li>
<li><p><strong>故障处理 (Fault Handling):</strong> 节点在运行过程中可能会遇到问题。</p>
<ul>
<li><strong>检测:</strong> Kubernetes <strong>Node Controller</strong>（运行在 <code>kube-controller-manager</code> 中）负责监控节点状态。<code>kubelet</code> 会定期向 API Server 发送<strong>心跳 (Heartbeat)</strong> 更新 NodeStatus。如果 Node Controller 在一定时间（<code>--node-monitor-grace-period</code>，默认为 40s）内未收到心跳，会将节点状态标记为 <code>Unknown</code>，如果持续无法联系（总超时 <code>--node-monitor-period</code> + grace period），则标记为 <code>NotReady</code>。</li>
<li><strong>临时故障 (Temporary Fault):</strong> 例如节点重启。如果节点恢复并 <code>kubelet</code> 重新启动，它会再次开始发送心跳，节点状态会恢复为 <code>Ready</code>。运行在该节点上的 Pod（如果是被 Deployment 等控制器管理的）通常会在节点恢复后继续运行（如果 Pod 本身没问题），或者如果之前被判定为故障节点上的 Pod 已被驱逐，则控制器会在其他节点上重建它们。</li>
<li><strong>永久故障 (Permanent Fault):</strong> 例如硬件损坏导致机器无法启动。节点会长时间处于 <code>NotReady</code> 状态。Kubernetes 会在超时后（<code>--pod-eviction-timeout</code>，默认为 5 分钟）<strong>驱逐 (Evict)</strong> 该节点上的 Pod。这意味着 API Server 会删除这些 Pod 对象，对应的控制器（如 Deployment Controller）会监测到 Pod 数量少于期望值，并在其他健康节点上创建新的 Pod 来替代它们。<strong>这是 Kubernetes 实现应用自愈和高可用的关键机制之一</strong>。对于永久故障的节点，需要进行 Offboard 操作。</li>
</ul>
</li>
<li><p><strong>Offboard (节点移除&#x2F;缩容&#x2F;退役):</strong> 从集群中移除一个节点。这可能是因为缩容、硬件维护、升级或退役。</p>
<ul>
<li><p><strong>驱逐 Pod (Drain):</strong> <strong>在移除节点前，必须先执行 <code>kubectl drain &lt;node-name&gt;</code> 操作</strong>。<code>drain</code> 命令会首先将节点标记为<strong>不可调度 (Unschedulable&#x2F;Cordoned)</strong>，阻止新的 Pod 被调度上来；然后<strong>优雅地驱逐 (Evict)</strong> 节点上现有的所有 Pod（除了不受控制器管理的 Pod 和 DaemonSet 的 Pod，可以通过参数 <code>--ignore-daemonsets</code> 和 <code>--delete-local-data</code> 控制）。驱逐过程会遵循 Pod 的<strong>优雅终止期 (Graceful Termination Period)</strong> 和 <strong>PodDisruptionBudgets (PDB)</strong>，确保服务的平稳过渡。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 示例：优雅地移除节点上的 Pod 并标记为不可调度</span><br>kubectl drain &lt;node-name&gt; --ignore-daemonsets --delete-local-data --force<br><span class="hljs-comment"># 注意：--force 通常用于处理不受控制器管理的 Pod，需谨慎使用</span><br></code></pre></td></tr></table></figure>
</li>
<li><p><strong>删除 Node 对象 (Delete Node Object):</strong> 在所有 Pod 都被成功驱逐后，可以从 Kubernetes API 中删除该 Node 对象：<code>kubectl delete node &lt;node-name&gt;</code>。这会通知 Kubernetes 不再管理这台机器。</p>
</li>
<li><p><strong>物理资产下架&#x2F;送修&#x2F;报废 (Physical Asset Decommissioning):</strong> 最后，关闭服务器电源，进行物理下架、维修或报废处理。</p>
</li>
</ul>
</li>
</ul>
<p>Node 的生命周期管理是 Kubernetes 运维的核心，涉及到集群的弹性伸缩、故障恢复和日常维护。自动化这些流程（例如使用 Cluster Autoscaler 自动扩缩容，或结合监控告警自动触发 <code>drain</code> 和节点替换流程）对于大规模集群的稳定运行至关重要。</p>
<h3 id="主机管理-Host-Management"><a href="#主机管理-Host-Management" class="headerlink" title="主机管理 (Host Management)"></a><strong>主机管理 (Host Management)</strong></h3><h4 id="主机选型与规划"><a href="#主机选型与规划" class="headerlink" title="主机选型与规划"></a>主机选型与规划</h4><ul>
<li><strong>选定系统内核版本与发行版 (Select Kernel Version and Distribution):</strong> <strong>内核版本直接影响系统的性能、稳定性和对新硬件、新功能（如 eBPF, cgroups v2）的支持</strong>。选择一个经过广泛测试、与 Kubernetes 版本兼容、并且有长期支持（LTS）的 Linux 发行版（如 Ubuntu LTS, CentOS Stream, RHEL）和内核版本至关重要。例如，较新的 K8s 版本可能依赖特定内核功能才能完全发挥 CNI 或 CSI 插件的性能。不恰当的内核版本可能导致性能瓶颈、驱动不兼容甚至内核恐慌 (Kernel Panic)。</li>
<li><strong>安装工具集 (Install Toolsets):</strong> 主机上需要预装一些必要的工具，如 <code>socat</code>, <code>conntrack</code>, <code>ipset</code> 等，这些是 <code>kubelet</code>, <code>kube-proxy</code> 或某些 CNI 插件正常运行所依赖的。标准化这些基础工具集可以确保节点行为的一致性。</li>
<li><strong>主机网络规划 (Host Network Planning):</strong> 需要规划节点的 IP 地址分配、子网划分、网关设置、DNS 配置。<strong>确保主机网络有足够的带宽和低延迟，并且具备冗余性</strong>（如前述的网卡 Bonding）。网络规划还需考虑与 Kubernetes CNI 插件的兼容性，例如，某些 CNI（如 Calico BGP 模式）可能对底层网络架构有特定要求。合理的网络规划可以避免 IP 冲突、网络隔离问题，并优化 Pod 间的通信效率。</li>
</ul>
<h4 id="主机镜像升级与风险"><a href="#主机镜像升级与风险" class="headerlink" title="主机镜像升级与风险"></a>主机镜像升级与风险</h4><p>图片指出了一个关键的运维挑战：<strong>日常的主机镜像升级更新也可能是造成服务不可用的因素之一</strong>。这里的“主机镜像”通常指预先制作好的包含操作系统、固定内核、必要工具和配置的操作系统模板（VM 模板或物理机安装镜像）。</p>
<p>即使是常规的安全补丁更新或内核升级，也可能引入问题：</p>
<ol>
<li><strong>兼容性问题:</strong> 新内核或系统库可能与 <code>kubelet</code>、容器运行时或 CNI&#x2F;CSI 插件不兼容，导致节点无法加入集群或 Pod 运行异常。</li>
<li><strong>配置漂移:</strong> 更新过程可能意外修改了关键配置文件（如 <code>sysctl.conf</code>、网络配置、防火墙规则），影响节点功能。</li>
<li><strong>更新失败:</strong> 更新过程本身可能失败，导致主机处于不稳定状态。</li>
<li><strong>重启中断:</strong> 大多数内核或关键系统库的更新都需要<strong>重启主机</strong>才能生效。在 Kubernetes 环境下，这意味着节点需要被<strong>排空 (Drain)</strong>，上面的 Pod 需要被迁移，这会暂时减少集群的可用容量，并可能对有状态应用或配置了严格 PDB 的应用造成影响。如果更新或重启过程耗时过长，或者多个节点同时更新，可能会对集群整体可用性产生显著影响。</li>
</ol>
<h4 id="A-B-系统-OTA-升级方式"><a href="#A-B-系统-OTA-升级方式" class="headerlink" title="A&#x2F;B 系统 OTA 升级方式"></a>A&#x2F;B 系统 OTA 升级方式</h4><p>为了<strong>降低主机操作系统升级带来的风险和停机时间</strong>，图片介绍了一种先进的升级策略：<strong>A&#x2F;B 系统 OTA (Over-The-Air) 升级</strong>。这种方式常见于嵌入式设备、移动操作系统（如 Android），也被一些现代 Linux 发行版（如 CoreOS&#x2F;Flatcar Linux, Fedora Silverblue&#x2F;Kinoite）或专门的服务器管理方案所采用。</p>
<p>其核心思想是：</p>
<ul>
<li><strong>双分区系统 (Dual Partition System):</strong> 主机的存储设备（通常是根文件系统所在的磁盘或分区）被划分为两个独立的系统分区，我们称之为 A 分区和 B 分区。在任何时候，只有一个分区是<strong>活动 (Active)</strong> 的，系统从该分区启动并运行。另一个分区是<strong>非活动 (Inactive)</strong> 的。</li>
<li><strong>共享数据 (Shared Data):</strong> 用户数据、应用程序数据、持久化配置等通常存储在独立于 A&#x2F;B 分区的、共享的数据分区上（例如挂载到 <code>/var</code> 或 <code>/home</code>）。这样，无论系统从 A 启动还是从 B 启动，都能访问到相同的数据。</li>
<li><strong>后台升级 (Background Update):</strong> 当需要进行系统升级时，<strong>升级包会被下载并应用到当前的非活动分区 (Inactive Partition)</strong>。例如，如果当前系统运行在 A 分区，那么新的操作系统镜像会被完整地写入 B 分区。<strong>这个过程在后台进行，不影响当前正在运行的系统（即 A 分区）</strong>。</li>
<li><strong>切换启动目标 (Switch Boot Target):</strong> 升级完成后，系统会修改<strong>引导加载程序 (Bootloader，通常是 GRUB)</strong> 的配置，将下一次启动的目标指向刚刚更新完成的那个分区（在这个例子中是 B 分区）。</li>
<li><strong>重启生效 (Reboot to Apply):</strong> <strong>只需要一次重启</strong>，系统就会从新的、已更新的分区（B 分区）启动。由于新系统镜像已经提前完整写入，启动过程通常很快，并且减少了在启动过程中应用补丁可能带来的风险。</li>
<li><strong>故障回滚 (Rollback Capability):</strong> <strong>这是 A&#x2F;B 升级的关键优势</strong>。如果在从新分区（B 分区）启动后发现任何问题（例如，系统不稳定、关键服务无法启动），管理员可以<strong>简单地再次修改引导加载程序配置，让系统下次从原来的、未经修改的分区（A 分区）启动</strong>。这就实现了快速且可靠的回滚，将系统恢复到升级前的状态。</li>
</ul>
<p><strong>在 Kubernetes 环境中应用 A&#x2F;B 升级的优势：</strong></p>
<ol>
<li><strong>减少维护窗口:</strong> 升级的主要工作（下载和写入镜像）在后台完成，真正需要中断节点服务的时间仅限于一次重启。配合 <code>kubectl drain</code>，可以将对业务的影响降到最低。</li>
<li><strong>提高升级可靠性:</strong> 避免了在运行中的系统上执行复杂的原地升级（in-place upgrade）脚本可能带来的风险。新系统是作为一个整体被验证和部署的。</li>
<li><strong>快速回滚:</strong> 如果升级后的节点出现问题（例如 <code>kubelet</code> 无法启动，或者性能下降），可以快速重启回滚到之前的稳定版本，大大缩短故障恢复时间。</li>
</ol>
<p><strong>实现方式：</strong></p>
<ul>
<li>需要操作系统和引导加载程序支持 A&#x2F;B 分区方案。例如，使用支持该模式的 Linux 发行版，或者通过工具（如 <code>ostree</code>, <code>Mender.io</code>, <code>RAUC</code>）和自定义分区布局来实现。</li>
<li>需要配套的 OTA 更新服务器和客户端机制来管理镜像的分发和升级流程。</li>
</ul>
<h3 id="生产化集群管理"><a href="#生产化集群管理" class="headerlink" title="生产化集群管理"></a>生产化集群管理</h3><h4 id="如何设定单个集群规模-Setting-Single-Cluster-Size"><a href="#如何设定单个集群规模-Setting-Single-Cluster-Size" class="headerlink" title="如何设定单个集群规模 (Setting Single Cluster Size)"></a>如何设定单个集群规模 (Setting Single Cluster Size)</h4><p>这是一个基础且关键的决策，涉及到对集群管理复杂性、故障影响范围（Blast Radius）以及资源利用率的权衡。</p>
<ul>
<li><p><strong>社区规模声明与现实挑战:</strong> Kubernetes 社区通常声明单个集群理论上可支持高达 <strong>5000 个节点</strong>。然而，这更多是一个基准或上限参考，实际可达到的稳定规模<strong>强依赖于控制平面的资源配置、etcd 的性能、网络质量以及集群上运行的工作负载类型和密度</strong>。在超大规模集群（数千节点）中，管理和运维会面临诸多挑战：</p>
<ul>
<li><strong>控制平面压力:</strong> <code>kube-apiserver</code> 需要处理来自大量 <code>kubelet</code>、用户和其他组件的请求，可能成为瓶leneck。<code>etcd</code> 作为状态存储核心，其读写性能和存储容量会面临巨大压力，对延迟非常敏感，大规模集群的事件风暴可能导致其性能下降甚至不稳定。Linux 内核的网络连接跟踪表 (<code>conntrack</code>) 也可能在高并发连接下耗尽。</li>
<li><strong>故障域扩大 (Increased Blast Radius):</strong> 单个大型集群意味着<strong>任何控制平面的故障、核心组件（如 CNI、CoreDNS）的全局性问题或安全漏洞，其影响范围会波及整个集群的所有应用</strong>，可能导致大规模服务中断。</li>
<li><strong>升级与维护困难:</strong> 对大型集群进行版本升级或重大变更风险更高，耗时更长，回滚也更复杂。协调数千个节点的维护窗口是一项艰巨的任务。</li>
<li><strong>资源公平性与隔离:</strong> 在超大集群中，确保不同团队或应用之间的资源公平分配、避免“吵闹邻居”问题（一个行为不当的应用影响其他应用）变得更加困难，需要更精细的 ResourceQuota、LimitRange 和可能的 NetworkPolicy 配置。</li>
</ul>
</li>
<li><p><strong>更多还是更少？如何权衡？</strong></p>
<ul>
<li><strong>倾向于更少、更大的集群:</strong><ul>
<li><strong>优点:</strong> 管理开销相对较低（管理一个控制平面 vs 多个），集群内服务间通信简单直接（无需跨集群服务发现和网络连接）。</li>
<li><strong>缺点:</strong> 如上所述，故障域大，升级风险高，可能存在扩展瓶颈。</li>
</ul>
</li>
<li><strong>倾向于更多、更小的集群:</strong><ul>
<li><strong>优点:</strong> <strong>显著减小故障域 (Reduced Blast Radius)</strong>，单个集群故障影响范围有限。更容易实现基于业务线、环境或团队的隔离。升级和维护可以分批进行，降低风险。可能更容易满足特定的合规性或数据本地性要求。</li>
<li><strong>缺点:</strong> 运维开销增加（需要管理多个控制平面、etcd 集群、监控系统等）。需要解决<strong>跨集群的服务发现、网络连接和身份认证</strong>问题，增加了架构复杂性（可能需要 Service Mesh 或多集群管理平台如 Karmada, Open Cluster Management）。</li>
</ul>
</li>
</ul>
<p><strong>权衡决策通常基于：</strong> 组织的运维能力、应用架构特点、对故障域的容忍度、安全和合规要求、地理分布需求等。<strong>没有绝对的“最佳”规模，需要根据具体场景定制</strong>。</p>
</li>
</ul>
<h4 id="如何根据地域划分集群-Dividing-Clusters-by-Region"><a href="#如何根据地域划分集群-Dividing-Clusters-by-Region" class="headerlink" title="如何根据地域划分集群 (Dividing Clusters by Region)"></a>如何根据地域划分集群 (Dividing Clusters by Region)</h4><p>这关系到如何处理跨地理位置的节点部署，核心在于<strong>网络延迟</strong>对 Kubernetes 控制平面（尤其是 <code>etcd</code>）的影响。</p>
<ul>
<li><p><strong>不同地域的计算节点划分到同一集群 (Anti-Pattern):</strong> <strong>强烈不推荐将跨越广域网（WAN）的不同地理区域（例如北京和上海）的节点加入到同一个 Kubernetes 集群</strong>。原因在于：</p>
<ul>
<li><strong>高延迟和不稳定性:</strong> <code>etcd</code> 依赖 <strong>Raft 协议</strong>进行数据同步和领导者选举，该协议对网络延迟非常敏感。跨地域的高延迟（通常几十到几百毫秒）会导致 Raft 超时、频繁的领导者选举失败，最终<strong>破坏 <code>etcd</code> 集群的稳定性和数据一致性</strong>。</li>
<li><strong><code>kubelet</code> 心跳问题:</strong> <code>kubelet</code> 需要定期向 <code>kube-apiserver</code> 发送心跳并更新节点状态。高延迟可能导致心跳超时，Node Controller 会将节点标记为 <code>NotReady</code>，导致 Pod 被错误地驱逐。</li>
<li><strong>API 访问缓慢:</strong> 远端节点访问中心化的 <code>kube-apiserver</code> 会非常缓慢，影响 Pod 启动、配置更新等操作效率。</li>
<li><strong>CNI 性能问题:</strong> 跨地域的 Pod 间通信性能会急剧下降，且许多 CNI 插件的设计并未考虑或优化 WAN 场景。</li>
</ul>
</li>
<li><p><strong>将同一地域的节点划分到同一集群 (Best Practice):</strong> <strong>这是标准的、推荐的做法</strong>。应该<strong>为每个需要部署资源的地理区域（Region）创建一个独立的 Kubernetes 集群</strong>。在同一个地理区域内，节点应部署在<strong>不同的可用区 (Availability Zones - AZs)</strong> 以实现高可用，因为 AZ 之间的网络延迟通常足够低（个位数毫秒），能够满足 <code>etcd</code> 和其他控制平面组件的要求。如果业务需要在多个地理区域存在，就应该部署和管理<strong>多个独立的 Kubernetes 集群</strong>。</p>
</li>
</ul>
<h4 id="如何规划集群的网络-Planning-Cluster-Networking"><a href="#如何规划集群的网络-Planning-Cluster-Networking" class="headerlink" title="如何规划集群的网络 (Planning Cluster Networking)"></a>如何规划集群的网络 (Planning Cluster Networking)</h4><p>网络规划是生产集群管理中的复杂环节，直接关系到安全、性能和隔离性。</p>
<ul>
<li><p><strong>企业办公环境、测试环境、预生产环境和生产环境应如何进行网络分离:</strong></p>
<ul>
<li><strong>目标:</strong> 防止开发&#x2F;测试活动影响生产环境，保障生产环境的安全性和稳定性。</li>
<li><strong>实现方式:</strong><ul>
<li><strong>物理&#x2F;逻辑隔离:</strong> 最彻底的方式是为不同环境使用<strong>完全独立的 Kubernetes 集群</strong>，部署在不同的网络段（VLANs&#x2F;Subnets），并通过防火墙进行严格访问控制。</li>
<li><strong>集群内隔离 (如果必须混合部署):</strong> 如果资源限制或管理需求导致必须在同一集群内容纳不同环境（通常不推荐用于生产环境与其他环境混合），则需要利用 Kubernetes 的内置机制：<ul>
<li><strong>Namespaces:</strong> 为不同环境（如 <code>dev</code>, <code>test</code>, <code>prod</code>）创建独立的命名空间。</li>
<li><strong>NetworkPolicy:</strong> <strong>使用 NetworkPolicy 对象来定义严格的网络访问规则</strong>。例如，可以配置 NetworkPolicy 默认拒绝所有跨命名空间的流量，然后显式允许必要的通信。这需要 CNI 插件（如 Calico, Cilium）支持 NetworkPolicy。<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># 示例：默认拒绝所有进入 &#x27;prod&#x27; 命名空间的流量</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">networking.k8s.io/v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">NetworkPolicy</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">default-deny-ingress</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">prod</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">podSelector:</span> &#123;&#125; <span class="hljs-comment"># 选择命名空间下的所有 Pod</span><br>  <span class="hljs-attr">policyTypes:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">Ingress</span><br><span class="hljs-meta">---</span><br><span class="hljs-comment"># 示例：允许 &#x27;monitoring&#x27; 命名空间的 Pod 访问 &#x27;prod&#x27; 命名空间的 Pod</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">networking.k8s.io/v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">NetworkPolicy</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">allow-monitoring-ingress</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">prod</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">podSelector:</span> &#123;&#125; <span class="hljs-comment"># 应用到 prod 下的所有 Pod</span><br>  <span class="hljs-attr">ingress:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">from:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-attr">namespaceSelector:</span><br>        <span class="hljs-attr">matchLabels:</span><br>          <span class="hljs-attr">kubernetes.io/metadata.name:</span> <span class="hljs-string">monitoring</span> <span class="hljs-comment"># 允许来自 monitoring 命名空间的流量</span><br>    <span class="hljs-comment"># 可以进一步根据 Pod Label 或端口进行限制</span><br>  <span class="hljs-attr">policyTypes:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">Ingress</span><br></code></pre></td></tr></table></figure></li>
<li><strong>RBAC:</strong> 结合基于角色的访问控制（RBAC）限制用户或服务账号只能访问其被授权的命名空间和资源。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>不同租户之间应如何进行网络隔离 (Multi-tenancy Network Isolation):</strong></p>
<ul>
<li><strong>目标:</strong> 在共享集群中为不同用户、团队或客户（租户）提供安全的网络边界，防止他们相互干扰或访问对方的应用。</li>
<li><strong>实现方式:</strong> 与环境隔离类似，主要依赖 <strong>Namespaces + NetworkPolicy + RBAC</strong>。每个租户分配一个或多个 Namespace。NetworkPolicy 用于强制执行租户间的网络隔离，确保一个租户的 Pod 默认无法访问另一个租户的 Pod。RBAC 则限制租户只能管理自己 Namespace 内的资源。此外，还需要配合 <strong>ResourceQuota 和 LimitRange</strong> 来限制每个租户的资源使用，防止资源滥用。对于需要更强隔离保证的场景，可以考虑使用<strong>虚拟集群 (Virtual Clusters)</strong> 技术（如 vCluster）。</li>
</ul>
</li>
</ul>
<h4 id="如何自动化搭建集群-Automating-Cluster-Setup"><a href="#如何自动化搭建集群-Automating-Cluster-Setup" class="headerlink" title="如何自动化搭建集群 (Automating Cluster Setup)"></a>如何自动化搭建集群 (Automating Cluster Setup)</h4><p>在生产环境中，<strong>手动搭建和管理 Kubernetes 集群是不可行且极易出错的</strong>。自动化是必须的。</p>
<ul>
<li><strong>如何自动化搭建和升级集群:</strong><ul>
<li><strong>目标:</strong> 实现集群部署、配置、扩缩容和升级的一致性、可重复性和高效性。</li>
<li><strong>实现方式:</strong><ul>
<li><strong>基础设施即代码 (Infrastructure as Code - IaC):</strong> 使用 Terraform, Pulumi 等工具自动化创建底层基础设施（虚拟机、网络、负载均衡器、存储等）。</li>
<li><strong>集群部署工具:</strong> 使用 <code>kubeadm</code> (结合脚本或 Ansible 等配置管理工具)、<code>kubespray</code> (基于 Ansible)、<code>RKE</code> (Rancher Kubernetes Engine)、<code>k3s</code> (轻量级发行版，易于自动化) 或云厂商提供的托管服务 (EKS, GKE, AKS) 的 API&#x2F;CLI 来自动化部署 Kubernetes 控制平面和数据平面核心组件。</li>
<li><strong>配置管理:</strong> 使用 Ansible, Chef, Puppet 或 SaltStack 自动化配置节点操作系统、安装依赖、设置内核参数等。</li>
<li><strong>GitOps:</strong> <strong>采用 GitOps 理念</strong>，将集群的期望状态（包括 Kubernetes 版本、组件配置、部署的应用等）存储在 Git 仓库中，使用 <strong>Argo CD</strong> 或 <strong>Flux</strong> 等工具自动同步 Git 仓库的状态到集群中。升级集群也通过修改 Git 中的配置并由工具自动执行完成。<strong>这是目前自动化管理集群状态和应用部署的主流最佳实践</strong>。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="与企业认证平台集成-Integration-with-Enterprise-Authentication-Platform"><a href="#与企业认证平台集成-Integration-with-Enterprise-Authentication-Platform" class="headerlink" title="与企业认证平台集成 (Integration with Enterprise Authentication Platform)"></a>与企业认证平台集成 (Integration with Enterprise Authentication Platform)</h3><p>在企业环境中，通常已经存在一套成熟的<strong>统一认证平台 (Identity Provider, IdP)</strong>，例如基于 LDAP、Active Directory (AD)、SAML 或 OpenID Connect (OIDC) 的系统（如 Keycloak, Okta, Azure AD 等）。Kubernetes 集群不应再独立维护一套用户账号体系。<strong>与企业认证平台集成，可以让企业用户使用他们熟悉的账号密码或单点登录 (SSO) 方式访问 Kubernetes 集群资源</strong>（例如通过 <code>kubectl</code>、Dashboard 或其他依赖 K8s API 的应用）。</p>
<ul>
<li><strong>解决了什么问题？</strong> 避免了账号管理的碎片化，简化了用户管理流程，提高了安全性（密码策略、多因素认证等由统一平台管理），并且方便了权限审计。</li>
<li><strong>是什么？</strong> Kubernetes API Server 支持通过 OIDC 或 Webhook Token Authentication 等方式对接外部认证系统。管理员可以配置 API Server 启动参数，指定 IdP 的信息。</li>
<li><strong>原理：</strong><ol>
<li>用户尝试访问 K8s API（例如执行 <code>kubectl get pods</code>）。</li>
<li>如果用户未认证，<code>kubectl</code> 或客户端库会根据 <code>kubeconfig</code> 中的 OIDC 配置，将用户重定向到企业的 IdP 进行登录。</li>
<li>用户在 IdP 成功登录后，IdP 会签发一个 ID Token (JWT)。</li>
<li>客户端将 ID Token 发送给 <code>kube-apiserver</code>。</li>
<li><code>kube-apiserver</code> 使用配置的 OIDC 信息（Issuer URL, Client ID）验证 ID Token 的签名和声明 (Claims)。</li>
<li>验证通过后，API Server 从 Token 中提取用户信息（用户名、用户组），然后结合 Kubernetes 的 <strong>RBAC (Role-Based Access Control)</strong> 规则进行授权判断。</li>
</ol>
</li>
<li><strong>配置示例 (API Server 启动参数):</strong><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">kube-apiserver --oidc-issuer-url=https://idp.example.com/auth/realms/myrealm \<br>               --oidc-client-id=kubernetes \<br>               --oidc-username-claim=email \<br>               --oidc-groups-claim=<span class="hljs-built_in">groups</span> \<br>               ... <span class="hljs-comment"># 其他 OIDC 相关参数</span><br></code></pre></td></tr></table></figure>
或者使用像 <strong>Dex</strong> 或 <strong>Pinniped</strong> 这样的中间件来简化多种 IdP 的集成。</li>
</ul>
<h4 id="集成企业域名服务-DNS-与负载均衡-ELB"><a href="#集成企业域名服务-DNS-与负载均衡-ELB" class="headerlink" title="集成企业域名服务 (DNS) 与负载均衡 (ELB)"></a>集成企业域名服务 (DNS) 与负载均衡 (ELB)</h4><p>Kubernetes 集群内部的服务发现（如 CoreDNS）通常只解析集群内部的 Service 名称。但集群内的应用可能需要访问集群外部的企业服务，同时，集群内发布的服务也需要通过企业级的 DNS 和负载均衡器暴露给外部用户。</p>
<ul>
<li><strong>集成 DNS:</strong><ul>
<li><strong>场景:</strong> Pod 需要解析企业内网的域名（如 <code>database.internal.mycorp.com</code>）。</li>
<li><strong>原理:</strong> 配置 Kubernetes 的 <strong>CoreDNS</strong>，使其将对特定域（如 <code>internal.mycorp.com</code>）的查询<strong>转发 (forward)</strong> 给企业的 DNS 服务器。这可以通过修改 CoreDNS 的 ConfigMap 实现。</li>
<li><strong>配置示例 (CoreDNS Corefile):</strong><figure class="highlight roboconf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs roboconf">internal.mycorp.com:53 &#123;<br>    <span class="hljs-attribute">errors</span><br><span class="hljs-attribute">    cache 30</span><br><span class="hljs-attribute">    forward . 10.100.0.5 10.100.0.6 # 转发给企业 DNS 服务器地址</span><br><span class="hljs-attribute">&#125;</span><br><span class="hljs-attribute">.</span>:53 &#123;<br>    errors<br>    cache 30<br>    kubernetes cluster<span class="hljs-variable">.local</span> in-addr<span class="hljs-variable">.arpa</span> ip6<span class="hljs-variable">.arpa</span> &#123;<br>       pods insecure<br>       upstream # 使用宿主机的 /etc/resolv<span class="hljs-variable">.conf</span> 作为上游<br>       fallthrough in-addr<span class="hljs-variable">.arpa</span> ip6<span class="hljs-variable">.arpa</span><br>    &#125;<br>    prometheus :9153<br>    forward . /etc/resolv<span class="hljs-variable">.conf</span> # 默认转发<br>    loop<br>    reload 5s<br>&#125;<br></code></pre></td></tr></table></figure></li>
</ul>
</li>
<li><strong>集成负载均衡 (ELB):</strong><ul>
<li><strong>场景:</strong> 将 Kubernetes 中运行的服务（通常通过 Ingress 或 Service type&#x3D;LoadBalancer）安全、可靠地暴露给企业网络或公网用户。</li>
<li><strong>原理:</strong> 使用企业现有的硬件负载均衡器 (如 F5) 或软件负载均衡器 (如 HAProxy, Nginx)，或者云厂商提供的 ELB 服务。这些负载均衡器将流量导向 Kubernetes Ingress Controller 的 Pod 所在节点的 NodePort，或者直接与 Service type&#x3D;LoadBalancer 创建的云厂商 LB 对接。<strong>企业 ELB 提供了统一的流量入口、SSL 卸载、更复杂的路由规则和高可用保障</strong>。</li>
<li><strong>配置:</strong> 这通常涉及在企业 ELB 上配置 VIP (Virtual IP Address)、后端服务器池（指向 K8s Node 或 Ingress Controller 暴露的端口），以及健康检查。</li>
</ul>
</li>
</ul>
<h4 id="依赖服务的可靠性考量"><a href="#依赖服务的可靠性考量" class="headerlink" title="依赖服务的可靠性考量"></a>依赖服务的可靠性考量</h4><p>当 Kubernetes 集群中的应用依赖于外部的企业公共服务（如数据库、消息队列、支付网关、遗留系统 API 等）时，<strong>这些外部服务的可靠性会直接影响到 K8s 应用的整体可用性</strong>。</p>
<ul>
<li><strong>问题:</strong> 外部服务可能发生故障、性能下降或维护。如果 K8s 应用没有做好容错处理，外部服务的抖动会传递到 K8s 应用，导致应用本身不可用或性能严重下降。</li>
<li><strong>应对策略:</strong><ul>
<li><strong>明确依赖关系和 SLA:</strong> 清晰梳理应用对外部服务的依赖，了解这些服务的服务水平协议 (SLA)。</li>
<li><strong>设计容错模式:</strong> 在应用代码层面实现<strong>断路器 (Circuit Breaker)</strong> 模式。当检测到对某个外部服务的调用连续失败或超时达到阈值时，断路器“跳闸”，暂时停止调用该服务，可以直接返回错误或返回预设的降级响应 (Fallback)，避免雪崩效应。断路器会定期尝试恢复（半开状态）。在 Golang 中，可以使用如 <code>sony/gobreaker</code> 或 <code>mercari/go-circuitbreaker</code> 等库。</li>
<li><strong>异步化与解耦:</strong> 对于非核心、可延迟处理的依赖，尽量采用<strong>异步消息队列</strong>（如 Kafka, RabbitMQ）进行解耦。</li>
<li><strong>监控与告警:</strong> 对外部服务的调用进行细致的监控（延迟、错误率），并设置告警。</li>
</ul>
</li>
</ul>
<h4 id="同步调用与超时管理"><a href="#同步调用与超时管理" class="headerlink" title="同步调用与超时管理"></a>同步调用与超时管理</h4><p>对于必须<strong>同步调用 (Synchronous Call)</strong> 的外部服务请求（即发起请求后必须等待响应才能继续执行），<strong>设置合理的超时时间至关重要</strong>。</p>
<ul>
<li><strong>问题:</strong> 如果不设置超时，或者超时时间设置过长，当外部服务响应缓慢或卡死时，调用方的线程（或 Goroutine）会被长时间阻塞，耗尽资源（如连接池、内存、线程数），最终导致调用方服务自身也变得不可用，无法处理新的请求。<strong>过长的等待时间还会显著增加整个请求链路的端到端延迟，从而降低系统的吞吐量 (TPS - Transactions Per Second)</strong>。</li>
<li><strong>解决方案:</strong><ul>
<li><strong>客户端超时:</strong> <strong>必须在调用方（客户端）设置超时</strong>。不要依赖于被调用方（服务端）的超时。在 Golang 中，进行 HTTP 调用时，可以在 <code>http.Client</code> 中设置 <code>Timeout</code> 字段，它涵盖了从连接、请求发送到接收响应头的整个过程。对于更细粒度的控制（如单独设置连接超时、读超时、写超时），可以配置 <code>http.Transport</code>。对于 gRPC 调用，可以通过 <code>context.WithTimeout</code> 传递带超时的上下文。<figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs golang"><span class="hljs-keyword">import</span> (<br>	<span class="hljs-string">&quot;net/http&quot;</span><br>	<span class="hljs-string">&quot;time&quot;</span><br>    <span class="hljs-string">&quot;context&quot;</span><br>)<br><br><span class="hljs-comment">// 示例 1: 设置整体超时</span><br>client := http.Client&#123;<br>	Timeout: <span class="hljs-number">500</span> * time.Millisecond, <span class="hljs-comment">// 设置 500ms 超时</span><br>&#125;<br>resp, err := client.Get(<span class="hljs-string">&quot;http://external.service/api&quot;</span>)<br><br><span class="hljs-comment">// 示例 2: 使用带超时的 Context (更推荐，尤其在复杂调用链中)</span><br>ctx, cancel := context.WithTimeout(context.Background(), <span class="hljs-number">500</span>*time.Millisecond)<br><span class="hljs-keyword">defer</span> cancel() <span class="hljs-comment">// 确保资源释放</span><br><br>req, _ := http.NewRequestWithContext(ctx, <span class="hljs-string">&quot;GET&quot;</span>, <span class="hljs-string">&quot;http://external.service/api&quot;</span>, <span class="hljs-literal">nil</span>)<br>resp, err := http.DefaultClient.Do(req)<br></code></pre></td></tr></table></figure></li>
<li><strong>合理设置:</strong> 超时时间应根据对外部服务 P99 响应时间的了解来设定，并留有少量 buffer。<strong>宁可超时失败，快速返回错误，也不能无限期等待</strong>。</li>
</ul>
</li>
</ul>
<h4 id="智能重试策略-Intelligent-Retry-Strategy"><a href="#智能重试策略-Intelligent-Retry-Strategy" class="headerlink" title="智能重试策略 (Intelligent Retry Strategy)"></a>智能重试策略 (Intelligent Retry Strategy)</h4><p>网络是不可靠的，<strong>瞬时性的网络抖动 (Network Jitter)</strong> 或服务临时过载可能导致调用失败。对于这类<strong>短暂的、偶然的 (Transient) 失败</strong>，进行重试是合理的，可以提高系统的韧性。但是，<strong>盲目重试所有失败，尤其是那些必然 (Definitive) 的失败（如认证失败、请求参数错误、资源不存在），或者在下游服务已经崩溃时持续重试，会适得其反</strong>。</p>
<ul>
<li><strong>问题:</strong><ul>
<li><strong>重试风暴 (Retry Storm):</strong> 如果大量客户端同时对一个已经有问题的服务进行重试，会进一步放大请求量，加剧服务的负载，甚至导致服务彻底崩溃或恢复时间延长。</li>
<li><strong>无效重试:</strong> 对逻辑错误（如 <code>4xx</code> 错误码）或永久性错误进行重试是无效的，只会浪费资源。</li>
</ul>
</li>
<li><strong>解决方案:</strong><ul>
<li><strong>区分错误类型:</strong> <strong>只对明确可重试的错误进行重试</strong>。通常包括：<ul>
<li>网络错误（连接超时、读取超时等）。</li>
<li>特定的 HTTP 状态码，如 <code>503 Service Unavailable</code>, <code>504 Gateway Timeout</code>, <code>429 Too Many Requests</code> (有时需要配合 <code>Retry-After</code> 头部)，以及某些情况下的 <code>500 Internal Server Error</code>（如果 API 明确约定某些 500 是临时的）。</li>
<li><strong>绝不重试</strong> <code>4xx</code> 客户端错误（除了 <code>429</code>）。</li>
</ul>
</li>
<li><strong>实现指数退避和抖动 (Exponential Backoff and Jitter):</strong> 重试时不应立即进行，而是等待一段时间。每次重试的等待时间应指数级增长（如 100ms, 200ms, 400ms…），以避免短时间内大量重试。同时，在等待时间上增加一个随机的<strong>抖动 (Jitter)</strong>，防止多个客户端在完全相同的时间点重试，打散重试请求。<figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs golang"><span class="hljs-comment">// 简化的指数退避+抖动逻辑示意</span><br><span class="hljs-keyword">import</span> (<br>	<span class="hljs-string">&quot;math&quot;</span><br>	<span class="hljs-string">&quot;math/rand&quot;</span><br>	<span class="hljs-string">&quot;time&quot;</span><br>)<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">calculateBackoff</span><span class="hljs-params">(attempt <span class="hljs-type">int</span>)</span></span> time.Duration &#123;<br>	baseDelay := <span class="hljs-number">100</span> * time.Millisecond<br>	maxDelay := <span class="hljs-number">5</span> * time.Second<br>	backoff := time.Duration(<span class="hljs-type">float64</span>(baseDelay) * math.Pow(<span class="hljs-number">2</span>, <span class="hljs-type">float64</span>(attempt)))<br>	<span class="hljs-keyword">if</span> backoff &gt; maxDelay &#123;<br>		backoff = maxDelay<br>	&#125;<br>	<span class="hljs-comment">// 添加 Jitter (例如，+/- 10% 的随机抖动)</span><br>	jitter := time.Duration(rand.Float64()*<span class="hljs-type">float64</span>(backoff)*<span class="hljs-number">0.2</span>) - (backoff / <span class="hljs-number">10</span>)<br>	<span class="hljs-keyword">return</span> backoff + jitter<br>&#125;<br><br><span class="hljs-comment">// 在重试循环中使用</span><br><span class="hljs-comment">// for attempt := 0; attempt &lt; maxRetries; attempt++ &#123;</span><br><span class="hljs-comment">//     err := callExternalService()</span><br><span class="hljs-comment">//     if err == nil &#123; break &#125;</span><br><span class="hljs-comment">//     if !isRetryable(err) &#123; break &#125;</span><br><span class="hljs-comment">//     time.Sleep(calculateBackoff(attempt))</span><br><span class="hljs-comment">// &#125;</span><br></code></pre></td></tr></table></figure></li>
<li><strong>限制重试次数:</strong> 设置一个最大重试次数或总重试时间上限，避免无限重试。</li>
<li><strong>利用库或框架:</strong> 使用成熟的库（如 Golang 的 <code>hashicorp/go-retryablehttp</code>）或 Service Mesh (如 Istio, Linkerd) 提供的重试功能，它们通常内置了健壮的重试逻辑。</li>
</ul>
</li>
</ul>
<h3 id="控制平面高可用"><a href="#控制平面高可用" class="headerlink" title="控制平面高可用"></a>控制平面高可用</h3><img src="/2025/04/24/kubernetes%E7%94%9F%E4%BA%A7%E5%8C%96%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/image-20250429104305577.png" srcset="/img/loading.gif" lazyload class="" title="image-20250429104305577">

<h4 id="理解控制平面及其重要性"><a href="#理解控制平面及其重要性" class="headerlink" title="理解控制平面及其重要性"></a>理解控制平面及其重要性</h4><p>在深入探讨高可用策略之前，我们首先要明确 Kubernetes 控制平面包含哪些核心组件以及它们的作用。主要组件通常包括：</p>
<ol>
<li><strong>kube-apiserver</strong>: 集群的统一入口，处理所有 REST API 请求，负责认证、授权、准入控制以及 API 注册和发现。它是无状态的，可以水平扩展。</li>
<li><strong>etcd</strong>: 一个一致性、高可用的键值存储，作为 Kubernetes 所有集群数据的后台数据库（例如 Pods, Services, Secrets 的状态）。<strong>etcd 的可用性和数据一致性是整个集群可用性的基石</strong>。</li>
<li><strong>kube-scheduler</strong>: 监视新创建的、未指定运行节点的 Pods，并根据资源需求、策略限制、亲和性&#x2F;反亲和性规则、数据局部性等因素，选择一个最佳节点来运行它们。</li>
<li><strong>kube-controller-manager</strong>: 运行控制器进程。逻辑上，每个控制器是一个独立的进程，但为了降低复杂性，它们都被编译到同一个可执行文件 <code>kube-controller-manager</code> 中，并在一个进程中运行。这些控制器包括节点控制器、副本控制器（ReplicaSet）、端点控制器（Endpoints）、服务账户和令牌控制器等。它们负责驱动集群状态从当前状态趋向期望状态。</li>
<li><strong>cloud-controller-manager</strong>: （可选，在云环境中）运行与底层云提供商交互的控制器。例如，管理云路由、负载均衡器、存储卷等。将这部分逻辑分离出来，使得 Kubernetes 核心与特定云平台的耦合度降低。</li>
</ol>
<p>控制平面的高可用性意味着即使部分组件或其所在的节点发生故障，整个集群的管理功能仍然可用，或者只有短暂中断。</p>
<h4 id="多-Master-节点与物理分布"><a href="#多-Master-节点与物理分布" class="headerlink" title="多 Master 节点与物理分布"></a>多 Master 节点与物理分布</h4><p><strong>解决的问题</strong>：单点故障。如果控制平面所有组件都运行在单一节点上，该节点的任何硬件故障（CPU、内存、磁盘、网络）、操作系统崩溃或维护都会导致整个控制平面瘫痪。</p>
<p><strong>是什么</strong>：高可用控制平面部署的核心思想是运行<strong>多个控制平面实例</strong>，并将它们分布在不同的物理基础设施上。通常这意味着至少有 3 个节点专门用于运行控制平面组件（kube-apiserver, etcd, kube-scheduler, kube-controller-manager）。</p>
<p><strong>原理</strong>：</p>
<ul>
<li><strong>etcd 集群</strong>：etcd 本身设计为分布式系统，使用 <strong>Raft 一致性算法</strong>来保证数据一致性和容错。一个 etcd 集群通常需要奇数个成员（例如 3 或 5 个），因为它需要**多数派（Quorum）**才能进行写操作和选举 Leader。对于 N 个成员的集群，它可以容忍 (N-1)&#x2F;2 个成员的故障。例如，一个 3 节点的 etcd 集群可以容忍 1 个节点故障，一个 5 节点的集群可以容忍 2 个节点故障。<strong>etcd 的高可用是整个控制平面 HA 的基础和关键</strong>。</li>
<li><strong>API Server 负载均衡</strong>：多个 <code>kube-apiserver</code> 实例是无状态的，它们都连接到同一个 etcd 集群。在这些 API Server 前端需要放置一个<strong>负载均衡器（Load Balancer）</strong>。集群内部组件（如 kubelet, kube-proxy）以及外部用户（如 <code>kubectl</code>）都通过这个负载均衡器的虚拟 IP (VIP) 或 DNS 名称访问 API Server。如果某个 API Server 实例失败，负载均衡器会将其从可用池中移除，请求会被路由到其他健康的实例。</li>
<li><strong>Scheduler 和 Controller Manager 的 Leader Election</strong>：<code>kube-scheduler</code> 和 <code>kube-controller-manager</code> 内部实现了**领导者选举（Leader Election）**机制。虽然你可以运行多个实例，但在任何时候只有一个实例是 active (leader)，负责执行实际的调度或控制循环。其他实例处于 standby 状态。如果当前 leader 失败，其中一个 standby 实例会通过竞争成为新的 leader，接管工作。这个机制通常利用 Kubernetes API（创建&#x2F;更新 Lease 或 Endpoint 对象）来实现分布式锁。你可以通过查看 <code>kube-system</code> 命名空间下的 <code>lease</code> 对象来观察 leader 选举情况，例如 <code>kubectl get lease -n kube-system</code>。</li>
</ul>
<p><strong>物理分布</strong>：为了防止单一物理故障（如整个机架断电、交换机故障）影响所有控制平面节点，这些节点应分布在<strong>不同的故障域（Failure Domains）<strong>中。在本地数据中心，这可能意味着不同的物理服务器、不同的机架、甚至不同的房间。在云环境中，这通常指</strong>不同的可用区（Availability Zones, AZs）</strong>。云提供商的 AZ 设计就是为了隔离故障。</p>
<h4 id="专用节点与资源保障"><a href="#专用节点与资源保障" class="headerlink" title="专用节点与资源保障"></a>专用节点与资源保障</h4><p><strong>解决的问题</strong>：资源争抢和干扰。如果控制平面组件与普通业务应用 Pod 混合部署在同一节点上，业务应用可能消耗过多资源（CPU、内存、磁盘 I&#x2F;O），导致控制平面组件性能下降、响应缓慢甚至 OOM (Out Of Memory) 被杀死。反之，控制平面组件的峰值负载也可能影响业务应用。此外，业务应用的潜在安全漏洞也可能威胁到控制平面组件。</p>
<p><strong>是什么</strong>：为控制平面组件<strong>划分专用的节点</strong>，不让普通用户的业务 Pod 调度到这些节点上。同时，确保这些专用节点拥有<strong>充足且有保障的资源</strong>。</p>
<p><strong>原理与实践</strong>：</p>
<ul>
<li><strong>节点污点（Taints）和容忍（Tolerations）</strong>：这是 Kubernetes 实现节点隔离的主要机制。可以给控制平面节点打上<strong>污点（Taint）</strong>，表明这些节点不接受不能“容忍”该污点的 Pod。<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 给 master-node1 添加一个 Taint，效果是 NoSchedule (不允许新 Pod 调度上来，除非有对应 Toleration)</span><br>kubectl taint nodes master-node1 node-role.kubernetes.io/master:NoSchedule<br><span class="hljs-comment"># 或者使用更通用的 control-plane role</span><br>kubectl taint nodes master-node1 node-role.kubernetes.io/control-plane:NoSchedule<br></code></pre></td></tr></table></figure>
控制平面组件的 Pod 定义中需要包含对这个 Taint 的<strong>容忍（Toleration）</strong>，这样它们才能被调度到这些专用节点上。例如，<code>kube-apiserver</code> 的 Pod YAML 中会有类似配置：<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">tolerations:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">&quot;node-role.kubernetes.io/master&quot;</span><br>    <span class="hljs-attr">operator:</span> <span class="hljs-string">&quot;Exists&quot;</span><br>    <span class="hljs-attr">effect:</span> <span class="hljs-string">&quot;NoSchedule&quot;</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">&quot;node-role.kubernetes.io/control-plane&quot;</span><br>    <span class="hljs-attr">operator:</span> <span class="hljs-string">&quot;Exists&quot;</span><br>    <span class="hljs-attr">effect:</span> <span class="hljs-string">&quot;NoSchedule&quot;</span><br></code></pre></td></tr></table></figure></li>
<li><strong>资源请求（Requests）和限制（Limits）</strong>：为控制平面组件的 Pod 设置合理的<strong>资源请求（<code>requests</code>）和限制（<code>limits</code>）<strong>至关重要。<code>requests</code> 保证了 Pod 至少能获得这些资源，影响调度决策和节点的资源预留。<code>limits</code> 则限制了 Pod 能使用的资源上限，防止其过度消耗。对于关键的控制平面组件，特别是 <code>etcd</code> 和 <code>kube-apiserver</code>，通常建议将 <code>requests</code> 和 <code>limits</code> 设置为相同的值，使其获得</strong>Guaranteed QoS Class</strong>，这样它们在节点资源紧张时被 OOM Kill 的优先级最低。<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">containers:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">kube-apiserver</span><br>    <span class="hljs-attr">resources:</span><br>      <span class="hljs-attr">requests:</span><br>        <span class="hljs-attr">cpu:</span> <span class="hljs-string">&quot;1&quot;</span> <span class="hljs-comment"># 请求 1 个 CPU 核心</span><br>        <span class="hljs-attr">memory:</span> <span class="hljs-string">&quot;2Gi&quot;</span> <span class="hljs-comment"># 请求 2 GiB 内存</span><br>      <span class="hljs-attr">limits:</span><br>        <span class="hljs-attr">cpu:</span> <span class="hljs-string">&quot;1&quot;</span> <span class="hljs-comment"># 限制最多使用 1 个 CPU 核心</span><br>        <span class="hljs-attr">memory:</span> <span class="hljs-string">&quot;2Gi&quot;</span> <span class="hljs-comment"># 限制最多使用 2 GiB 内存</span><br></code></pre></td></tr></table></figure>
<strong>过于严苛的资源限制会导致系统效率低下，降低集群可用性</strong>。例如，API Server 内存不足会频繁重启，etcd 磁盘 I&#x2F;O 不足会导致请求超时和集群不稳定。</li>
</ul>
<h4 id="减少外部依赖与解耦"><a href="#减少外部依赖与解耦" class="headerlink" title="减少外部依赖与解耦"></a>减少外部依赖与解耦</h4><p><strong>解决的问题</strong>：外部系统故障导致 Kubernetes 控制平面不可用。在早期 Kubernetes 版本中，一些核心功能（如云存储卷管理、负载均衡器创建）直接内置在 <code>kube-controller-manager</code> 或 <code>kube-apiserver</code> 中，并直接调用 Cloud Provider API。如果 Cloud Provider API 出现故障或响应缓慢，会直接影响 Kubernetes 控制平面的稳定性和性能。</p>
<p><strong>是什么</strong>：将与特定环境（尤其是云平台）相关的逻辑从 Kubernetes 核心组件中<strong>剥离</strong>出来，通过**标准接口（如 CSI, CNI, CPI）**与外部组件或驱动进行交互。</p>
<p><strong>原理与实践</strong>：</p>
<ul>
<li><strong>Cloud Provider Interface (CPI)</strong>: <code>cloud-controller-manager</code> 的引入就是为了解耦。它独立运行，负责与云平台 API 交互。如果云 API 故障，影响主要局限在与云资源相关的操作，而不是整个 <code>kube-controller-manager</code>。</li>
<li><strong>Container Storage Interface (CSI)</strong>: 定义了一套标准的存储卷管理接口。存储提供商（云厂商或存储供应商）可以开发符合 CSI 规范的驱动程序，作为独立的 Pod 部署在集群中。Kubernetes 核心组件通过调用 CSI 接口来管理存储卷，不再需要内置特定存储的逻辑。这使得存储功能的更新和维护独立于 Kubernetes 版本。</li>
<li><strong>Container Network Interface (CNI)</strong>: 定义了容器网络配置的标准接口。网络插件（如 Calico, Flannel, Cilium）作为独立组件部署，负责 Pod 网络设置和策略。<code>kubelet</code> 在创建和销毁 Pod 时调用 CNI 插件。</li>
</ul>
<p>通过这种解耦，<strong>减少了 Kubernetes 控制平面核心组件对外部系统的直接强依赖</strong>。即使某个外部系统（如云存储 API）暂时不可用，只要不涉及相关操作，控制平面的核心功能（如 Pod 调度、状态同步）仍然可以工作。</p>
<h4 id="控制平面与数据平面解耦"><a href="#控制平面与数据平面解耦" class="headerlink" title="控制平面与数据平面解耦"></a>控制平面与数据平面解耦</h4><p><strong>解决的问题</strong>：控制平面故障是否会导致运行中的业务中断。</p>
<p><strong>是什么</strong>：Kubernetes 架构设计上<strong>天然地将控制平面（管理）和数据平面（运行业务负载）进行了分离</strong>。</p>
<p><strong>原理</strong>：</p>
<ul>
<li><strong>数据平面</strong>主要由集群中的**工作节点（Worker Nodes）**以及运行在这些节点上的 <code>kubelet</code>、<code>kube-proxy</code> 和容器运行时（如 Docker, containerd）组成。业务 Pod 运行在数据平面上。</li>
<li><code>kubelet</code> 负责管理本节点上的 Pod 生命周期，与容器运行时交互。<code>kube-proxy</code> 负责维护网络规则（如 iptables 或 IPVS），实现 Service 的负载均衡。</li>
<li>当控制平面（主要是 <code>kube-apiserver</code>）不可用时：<ul>
<li><strong>现有 Pod 会继续运行</strong>：<code>kubelet</code> 会继续保持当前运行 Pod 的状态，容器运行时也会继续工作。</li>
<li><strong>Service 网络通常继续工作</strong>：<code>kube-proxy</code> 基于它最后一次从 <code>kube-apiserver</code> 获取的信息来维护网络规则。只要 <code>kube-proxy</code> 自身和底层网络设施（如 CNI 插件管理的部分）正常，现有的 Service 访问通常不受影响。</li>
<li><strong>无法进行管理操作</strong>：无法创建、删除、更新 Pod、Service、Deployment 等资源。自动伸缩（HPA&#x2F;VPA&#x2F;Cluster Autoscaler）会停止工作。</li>
<li><strong>节点状态更新和心跳停止</strong>：<code>kubelet</code> 无法向 <code>kube-apiserver</code> 报告节点和 Pod 状态。长时间失联后，<code>kube-controller-manager</code>（如果它还能运行并访问 etcd）可能会将失联节点标记为 <code>NotReady</code>，并可能触发 Pod 驱逐（如果配置了）。</li>
<li><strong>新 Pod 无法调度</strong>：<code>kube-scheduler</code> 无法工作。</li>
</ul>
</li>
</ul>
<p><strong>这意味着控制平面的短暂故障通常不会立即影响正在运行的业务</strong>，为修复控制平面赢得了时间。然而，长时间的控制平面故障会逐渐影响集群的健康和管理能力。<strong>确保控制平面组件出现故障时，将业务影响降到最低</strong>是 HA 设计的重要目标。</p>
<h4 id="核心插件的可用性"><a href="#核心插件的可用性" class="headerlink" title="核心插件的可用性"></a>核心插件的可用性</h4><p><strong>解决的问题</strong>：一些以普通 Pod 形式运行的、对集群功能至关重要的<strong>核心插件（Add-ons）</strong>，如 DNS（CoreDNS）、网络插件（CNI daemons）、Ingress 控制器等，它们的故障也会严重影响集群的可用性。</p>
<p><strong>是什么</strong>：这些插件虽然不是狭义上的控制平面组件，但它们提供了基础服务，其可用性同样关键。它们通常作为 <code>Deployment</code> 或 <code>DaemonSet</code> 部署在集群中。</p>
<p><strong>原理与实践</strong>：</p>
<ul>
<li><strong>副本和调度策略</strong>：对于像 CoreDNS 或 Ingress 控制器这样的服务，应使用 <code>Deployment</code> 并配置<strong>多个副本（replicas）</strong>。利用**Pod 反亲和性（Pod Anti-Affinity）**规则，确保这些副本分布在不同的节点甚至不同的故障域上，避免单点故障。<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">replicas:</span> <span class="hljs-number">3</span><br>  <span class="hljs-attr">affinity:</span><br>    <span class="hljs-attr">podAntiAffinity:</span><br>      <span class="hljs-attr">requiredDuringSchedulingIgnoredDuringExecution:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">labelSelector:</span><br>          <span class="hljs-attr">matchExpressions:</span><br>          <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">k8s-app</span><br>            <span class="hljs-attr">operator:</span> <span class="hljs-string">In</span><br>            <span class="hljs-attr">values:</span><br>            <span class="hljs-bullet">-</span> <span class="hljs-string">kube-dns</span> <span class="hljs-comment"># For CoreDNS example</span><br>        <span class="hljs-attr">topologyKey:</span> <span class="hljs-string">&quot;kubernetes.io/hostname&quot;</span> <span class="hljs-comment"># Ensure pods are on different nodes</span><br>      <span class="hljs-attr">preferredDuringSchedulingIgnoredDuringExecution:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">weight:</span> <span class="hljs-number">100</span><br>        <span class="hljs-attr">podAffinityTerm:</span><br>          <span class="hljs-attr">labelSelector:</span><br>            <span class="hljs-attr">matchExpressions:</span><br>            <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">k8s-app</span><br>              <span class="hljs-attr">operator:</span> <span class="hljs-string">In</span><br>              <span class="hljs-attr">values:</span><br>              <span class="hljs-bullet">-</span> <span class="hljs-string">kube-dns</span><br>          <span class="hljs-attr">topologyKey:</span> <span class="hljs-string">&quot;topology.kubernetes.io/zone&quot;</span> <span class="hljs-comment"># Prefer pods in different AZs</span><br></code></pre></td></tr></table></figure></li>
<li><strong>资源保障</strong>：同样，为这些核心插件 Pod 配置合理的<strong>资源请求和限制</strong>，确保它们有足够的资源稳定运行。</li>
<li><strong>调度位置</strong>：这些插件 Pod 默认可能会被调度到任何工作节点上，与普通应用竞争资源。在某些情况下，可能需要考虑将它们调度到特定的节点池，或者确保它们有足够的优先级（PriorityClass）来抢占资源。<strong>这些插件是否正常运行也决定了集群的可用性</strong>。例如，如果 CoreDNS 全部实例失败，集群内部的服务发现就会瘫痪。</li>
</ul>
<h3 id="集群安装方法比较"><a href="#集群安装方法比较" class="headerlink" title="集群安装方法比较"></a>集群安装方法比较</h3><img src="/2025/04/24/kubernetes%E7%94%9F%E4%BA%A7%E5%8C%96%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/image-20250429105221429.png" srcset="/img/loading.gif" lazyload class="" title="image-20250429105221429">

<h4 id="二进制安装-“The-Hard-Way”-从零构建"><a href="#二进制安装-“The-Hard-Way”-从零构建" class="headerlink" title="二进制安装 (“The Hard Way” &#x2F; 从零构建)"></a>二进制安装 (“The Hard Way” &#x2F; 从零构建)</h4><p><strong>是什么</strong>: 这是最基础、最原始的安装方式。它涉及到<strong>手动下载</strong> Kubernetes 各个核心组件的二进制文件（例如 <code>kube-apiserver</code>, <code>kube-controller-manager</code>, <code>kube-scheduler</code>, <code>kubelet</code>, <code>kube-proxy</code>）以及 <code>etcd</code> 的二进制文件，然后<strong>手动配置</strong>每一个组件，并通过 Linux 的服务管理系统（通常是 <strong>systemd</strong>）来启动和管理这些进程。</p>
<p><strong>原理与工作方式</strong>: 你需要从头开始构建整个集群的控制平面和数据平面。这要求你对每个组件的角色、配置选项、依赖关系以及它们之间的交互有深刻的理解。具体步骤通常包括：</p>
<ol>
<li><p><strong>准备基础设施</strong>: 配置 Linux 虚拟机或物理服务器，设置网络。</p>
</li>
<li><p><strong>生成证书</strong>: 手动创建所有必要的 <strong>TLS 证书和密钥</strong>，用于保障组件之间（如 API Server 与 etcd、kubelet 与 API Server）的安全通信。这通常使用 <code>openssl</code> 或 <code>cfssl</code> 等工具完成，这个过程能让你深刻理解 <strong>PKI（公钥基础设施）</strong> 在分布式系统安全中的核心作用。</p>
</li>
<li><p><strong>部署 etcd 集群</strong>: 手动配置并启动一个高可用的 etcd 集群（通常需要 3 或 5 个节点）。这涉及到配置节点间发现、TLS 加密通信，并且需要理解 etcd 依赖的 <strong>Raft 一致性算法</strong> 来保证数据可靠性。</p>
</li>
<li><p><strong>配置 Kubernetes 组件</strong>: 为 <code>kube-apiserver</code>, <code>kube-controller-manager</code>, <code>kube-scheduler</code> 等控制平面组件编写配置文件（通常是 YAML 格式或传递给 systemd 单元的环境变量文件），指定诸如 etcd 服务器地址、证书路径、特性门控 (feature gates)、网络参数（Pod CIDR, Service CIDR）、云服务商集成配置（如果需要）等大量参数。</p>
</li>
<li><p><strong>创建 systemd Unit 文件</strong>: 为每个组件编写 <code>.service</code> 文件，以便 systemd 能够管理它们的生命周期（启动、停止、重启、开机自启）。你需要理解 <strong>systemd 的 target、依赖关系（如 <code>Wants</code>, <code>After</code>）以及执行参数（<code>ExecStart</code>）</strong>。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs systemd"># /etc/systemd/system/kube-apiserver.service 文件示例片段<br>[Unit]<br>Description=Kubernetes API Server<br>Documentation=https://github.com/kubernetes/kubernetes<br># 确保网络和 etcd 服务先启动<br>After=network.target etcd.service<br>Wants=etcd.service<br><br>[Service]<br>ExecStart=/usr/local/bin/kube-apiserver \\<br>  --advertise-address=$&#123;INTERNAL_IP&#125; \\<br>  --allow-privileged=true \\<br>  --apiserver-count=3 \\ # HA 配置需要<br>  --audit-log-maxage=30 \\<br>  --audit-log-maxbackup=3 \\<br>  --audit-log-maxsize=100 \\<br>  --audit-log-path=/var/log/audit.log \\<br>  --authorization-mode=Node,RBAC \\<br>  --bind-address=0.0.0.0 \\<br>  --client-ca-file=/var/lib/kubernetes/ca.pem \\<br>  --enable-admission-plugins=... \\<br>  --etcd-cafile=/var/lib/kubernetes/ca.pem \\<br>  --etcd-certfile=/var/lib/kubernetes/etcd-server.pem \\<br>  --etcd-keyfile=/var/lib/kubernetes/etcd-server-key.pem \\<br>  # 连接到 etcd 集群<br>  --etcd-servers=https://ETCD1_IP:2379,https://ETCD2_IP:2379,https://ETCD3_IP:2379 \\<br>  # ... 此处省略大量其他必要参数 ...<br>  --service-account-key-file=/var/lib/kubernetes/service-account.pem \\<br>  --service-cluster-ip-range=10.32.0.0/24 \\<br>  --tls-cert-file=/var/lib/kubernetes/kube-apiserver.pem \\<br>  --tls-private-key-file=/var/lib/kubernetes/kube-apiserver-key.pem \\<br>  --v=2<br>Restart=on-failure<br>RestartSec=5<br><br>[Install]<br>WantedBy=multi-user.target<br></code></pre></td></tr></table></figure></li>
<li><p><strong>配置工作节点</strong>: 在工作节点上配置和启动 <code>kubelet</code> 和 <code>kube-proxy</code>，确保它们能通过 bootstrap token 或证书向 API Server 注册。</p>
</li>
<li><p><strong>安装网络插件 (CNI)</strong>: 在核心组件运行起来之后，手动安装和配置一个 CNI 网络插件（如 Calico, Flannel, Cilium），这是 Pod 之间通信的基础。</p>
</li>
</ol>
<p><strong>优点</strong>:</p>
<ul>
<li><strong>极高的灵活性和控制力</strong>: 你可以完全控制每个组件的版本、每一个配置参数，适合进行深度定制或集成。</li>
<li><strong>最深入的理解</strong>: 这种方式强迫你学习 Kubernetes 各组件之间错综复杂的依赖和交互、证书管理、Linux 服务管理以及网络知识。<strong>这对于后续排查复杂问题非常有价值</strong>。</li>
</ul>
<p><strong>缺点</strong>:</p>
<ul>
<li><strong>极其复杂</strong>: 整个过程非常繁琐、耗时，且极易出错。需要对细节有极高的关注度。正如图片所说，<strong>复杂，需要关心每一个组件的配置</strong>。</li>
<li><strong>高昂的维护成本</strong>: 集群的升级、证书轮换、配置变更等操作完全是手动的，非常复杂且难以保证一致性。</li>
<li><strong>重复造轮子</strong>: 你在手动执行大量已被其他工具自动化的任务。<strong>对系统服务的依赖性过多</strong>，因为你需要手动确保所有底层依赖（如时间同步、内核模块）都已就绪。</li>
</ul>
<h4 id="Kubeadm"><a href="#Kubeadm" class="headerlink" title="Kubeadm"></a>Kubeadm</h4><p><strong>是什么</strong>: Kubeadm 是 Kubernetes 官方（SIG Cluster Lifecycle 小组）维护的一个命令行工具，旨在 <strong>简化创建符合最佳实践的 Kubernetes 集群的引导（bootstrap）过程</strong>。它自动化了二进制安装中的许多复杂步骤，但仍将一些基础设施准备和附加组件（Add-ons）的安装留给用户。</p>
<p><strong>原理与工作方式</strong>:</p>
<ul>
<li><p><strong><code>kubeadm init</code></strong>: 在第一个控制平面节点上运行。</p>
<ul>
<li>执行一系列<strong>预检检查</strong>（验证系统状态、内核模块、端口可用性等）。</li>
<li><strong>生成必要的 TLS 证书和密钥</strong>（默认存储在 <code>/etc/kubernetes/pki</code> 目录下）。</li>
<li>可以选择性地设置一个 <strong>stacked etcd</strong> 实例（即 etcd 作为静态 Pod 运行在控制平面节点上），或者配置连接到一个外部的 etcd 集群。</li>
<li>在 <code>/etc/kubernetes/manifests/</code> 目录下为控制平面组件（<code>kube-apiserver</code>, <code>kube-controller-manager</code>, <code>kube-scheduler</code>）生成 <strong>静态 Pod (Static Pods)</strong> 的 YAML 清单文件。这些 Pod 由节点上的 <code>kubelet</code> 直接管理，而不是通过 API Server，这是 Kubernetes 自举的一个关键机制。</li>
<li>生成用于集群管理的 <code>kubeconfig</code> 文件 (<code>admin.conf</code>) 以及供内部组件使用的配置文件 (<code>controller-manager.conf</code>, <code>scheduler.conf</code>)。</li>
<li>部署一些基础的插件，如 <code>kube-proxy</code>（以 DaemonSet 形式）和 <code>CoreDNS</code>（以 Deployment 形式）。</li>
<li>输出一个 <strong><code>kubeadm join</code> 命令</strong>，其中包含用于添加其他节点的<strong>引导令牌 (bootstrap token)</strong>。</li>
</ul>
</li>
<li><p><strong><code>kubeadm join</code></strong>: 在其他节点（可以是控制平面节点或工作节点）上运行。</p>
<ul>
<li>使用 <code>init</code> 命令提供的引导令牌和发现信息（API Server 地址和 CA 证书哈希）安全地连接到集群。</li>
<li>执行 TLS 引导流程，为本节点的 <code>kubelet</code> 获取唯一的凭证。</li>
<li>配置本地 <code>kubelet</code> 与 API Server 通信。如果使用 <code>--control-plane</code> 参数加入作为控制平面节点，它也会在本节点上设置控制平面组件的静态 Pod。</li>
</ul>
</li>
<li><p><strong>配置</strong>: 通过 <code>--config</code> 参数传递一个 <code>ClusterConfiguration</code> YAML 文件来进行定制（例如，指定 Pod&#x2F;Service 的 CIDR 网段、Kubernetes 版本、控制平面端点、特性门控等）。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 使用配置文件执行 init 命令示例</span><br><span class="hljs-built_in">sudo</span> kubeadm init --config kubeadm-config.yaml --upload-certs<br><br><span class="hljs-comment"># kubeadm-config.yaml 文件示例片段</span><br>apiVersion: kubeadm.k8s.io/v1beta3<br>kind: InitConfiguration<br>bootstrapTokens:<br>- <span class="hljs-built_in">groups</span>:<br>  - system:bootstrappers:kubeadm:default-node-token<br>  token: abcdef.0123456789abcdef <span class="hljs-comment"># 示例 Token</span><br>  ttl: 24h0m0s<br>  usages:<br>  - signing<br>  - authentication<br>localAPIEndpoint:<br>  advertiseAddress: <span class="hljs-string">&quot;192.168.1.100&quot;</span> <span class="hljs-comment"># 节点 IP</span><br>  bindPort: 6443<br>nodeRegistration:<br>  criSocket: unix:///var/run/containerd/containerd.sock <span class="hljs-comment"># 或 Docker shim</span><br>  imagePullPolicy: IfNotPresent<br>  name: master-1<br>  taints: <span class="hljs-comment"># 给控制平面节点打上污点</span><br>  - effect: NoSchedule<br>    key: node-role.kubernetes.io/master<br>  - effect: NoSchedule<br>    key: node-role.kubernetes.io/control-plane<br>---<br>apiVersion: kubeadm.k8s.io/v1beta3<br>kind: ClusterConfiguration<br>apiServer:<br>  timeoutForControlPlane: 4m0s<br>  certSANs: <span class="hljs-comment"># 如果需要，添加额外的证书主题备用名 (SANs)</span><br>  - <span class="hljs-string">&quot;my-loadbalancer-dns.com&quot;</span><br>certificatesDir: /etc/kubernetes/pki<br>clusterName: my-cluster<br><span class="hljs-comment"># 为 HA 设置控制平面端点 (VIP 或 DNS)</span><br>controlPlaneEndpoint: <span class="hljs-string">&quot;my-loadbalancer-dns.com:6443&quot;</span><br>controllerManager: &#123;&#125;<br>dns: &#123;&#125;<br>etcd:<br>  <span class="hljs-built_in">local</span>: <span class="hljs-comment"># 使用内置的 stacked etcd</span><br>    dataDir: /var/lib/etcd<br>kubernetesVersion: v1.28.2<br>networking:<br>  dnsDomain: cluster.local<br>  podSubnet: <span class="hljs-string">&quot;10.244.0.0/16&quot;</span> <span class="hljs-comment"># Pod 网络 CIDR</span><br>  serviceSubnet: 10.96.0.0/12 <span class="hljs-comment"># Service 网络 CIDR</span><br>scheduler: &#123;&#125;<br>---<br><span class="hljs-comment"># 可选: KubeletConfiguration, KubeProxyConfiguration 等</span><br></code></pre></td></tr></table></figure></li>
</ul>
<p><strong>优点</strong>:</p>
<ul>
<li><strong>简化引导过程</strong>: 相比二进制安装，<strong>极大地降低了初始搭建的复杂度</strong>，自动化了证书生成、etcd 基本设置（可选）、控制平面静态 Pod 创建等繁琐任务。正如图片所说，<strong>控制面板组件的安装和配置被封装起来了</strong>。</li>
<li><strong>封装了最佳实践</strong>: 遵循社区推荐的标准方式来配置集群。</li>
<li><strong>生命周期管理</strong>: 提供了用于<strong>集群升级 (<code>kubeadm upgrade</code>)</strong>、证书续期 (<code>kubeadm certs renew</code>)、令牌管理等命令。</li>
</ul>
<p><strong>缺点</strong>:</p>
<ul>
<li><strong>自动化不完整</strong>: <strong>它不会自动安装 CNI 网络插件</strong>。这是在 <code>kubeadm init</code> 成功后必须手动完成的一步（例如 <code>kubectl apply -f calico.yaml</code>）。它通常也<strong>不处理操作系统层面的配置自动化</strong>（如安装 <code>containerd</code>&#x2F;<code>docker</code>、禁用 swap、配置内核参数如 <code>net.bridge.bridge-nf-call-iptables=1</code>）。</li>
<li><strong>高可用（HA）需要额外步骤</strong>: 搭建多主高可用集群需要手动设置外部负载均衡器（或使用带内建 keepalived&#x2F;haproxy 的实验性 <code>--control-plane-endpoint</code> 功能），可能需要管理外部 etcd 集群，并在 <code>init</code> 时使用 <code>--upload-certs</code> 参数、<code>join</code> 时使用 <code>--control-plane</code> 参数。<strong>运行时安装配置等复杂步骤依然是必须的</strong>。</li>
</ul>
<h4 id="Kubespray"><a href="#Kubespray" class="headerlink" title="Kubespray"></a>Kubespray</h4><p><strong>是什么</strong>: Kubespray 是一个使用 <strong>Ansible playbooks</strong> 来部署和管理 Kubernetes 集群的工具。它旨在提供比 Kubeadm 更“开箱即用”的体验，能够处理操作系统准备、依赖安装以及可选附加组件的部署。</p>
<p><strong>原理与工作方式</strong>:</p>
<ul>
<li><p>它依赖于 <strong>Ansible</strong>，一个无代理的配置管理和自动化工具。你需要在 Ansible 的 <strong>inventory 文件</strong> 和 <strong>变量文件 (<code>group_vars</code>)</strong> 中定义你的集群拓扑和配置。</p>
</li>
<li><p>Ansible playbooks（特别是 Kubespray 中的 <code>cluster.yml</code> playbook）通过 SSH 连接到你的目标 Linux 主机，并执行一系列预定义的任务：</p>
<ul>
<li>操作系统准备（安装软件包、调整内核参数）。</li>
<li>安装和配置容器运行时（Docker, containerd）。</li>
<li>下载 Kubernetes 二进制文件或容器镜像。</li>
<li>生成证书。</li>
<li>设置 etcd 集群。</li>
<li>配置并启动 Kubernetes 组件（注意：它<strong>可能在内部利用了 kubeadm</strong> 来完成部分引导工作，但它负责编排整个过程）。</li>
<li>部署你选择的 CNI 网络插件。</li>
<li>可选地部署其他附加组件（如 metrics-server, ingress controller 等）。</li>
</ul>
</li>
<li><p>整个过程由变量文件中定义的配置驱动，允许进行大量的定制（选择 CNI 插件、Kubernetes 版本、启用特性等）。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># Ansible inventory 文件示例片段 (inventory/mycluster/inventory.ini)</span><br>[<span class="hljs-string">all</span>]<br><span class="hljs-string">master1</span> <span class="hljs-string">ansible_host=192.168.1.101</span> <span class="hljs-string">ip=192.168.1.101</span><br><span class="hljs-string">node1</span> <span class="hljs-string">ansible_host=192.168.1.102</span> <span class="hljs-string">ip=192.168.1.102</span><br><span class="hljs-string">node2</span> <span class="hljs-string">ansible_host=192.168.1.103</span> <span class="hljs-string">ip=192.168.1.103</span><br><br>[<span class="hljs-string">kube_control_plane</span>]<br><span class="hljs-string">master1</span><br><br>[<span class="hljs-string">etcd</span>]<br><span class="hljs-string">master1</span><br><br>[<span class="hljs-string">kube_node</span>]<br><span class="hljs-string">node1</span><br><span class="hljs-string">node2</span><br><br>[<span class="hljs-string">k8s_cluster:children</span>]<br><span class="hljs-string">kube_control_plane</span><br><span class="hljs-string">kube_node</span><br></code></pre></td></tr></table></figure>

<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># 变量文件示例片段 (inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml)</span><br><span class="hljs-attr">kube_version:</span> <span class="hljs-string">v1.28.2</span><br><span class="hljs-attr">kube_network_plugin:</span> <span class="hljs-string">calico</span> <span class="hljs-comment"># 可选 flannel, weave, cilium 等</span><br><span class="hljs-comment"># 设置 Pod 和 Service CIDR</span><br><span class="hljs-attr">kube_pods_subnet:</span> <span class="hljs-number">10.233</span><span class="hljs-number">.64</span><span class="hljs-number">.0</span><span class="hljs-string">/18</span><br><span class="hljs-attr">kube_service_addresses:</span> <span class="hljs-number">10.233</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span><span class="hljs-string">/18</span><br><span class="hljs-comment"># 启用附加组件</span><br><span class="hljs-attr">metrics_server_enabled:</span> <span class="hljs-literal">true</span><br><span class="hljs-attr">ingress_nginx_enabled:</span> <span class="hljs-literal">true</span><br></code></pre></td></tr></table></figure></li>
</ul>
<p><strong>优点</strong>:</p>
<ul>
<li><strong>高度自动化</strong>: <strong>自动化了从操作系统准备到依赖安装、Kubernetes 核心组件、CNI 插件乃至附加组件的整个部署流程</strong>。</li>
<li><strong>高度可定制</strong>: 通过丰富的变量配置，支持多种 Linux 发行版、容器运行时、CNI 插件和 Kubernetes 版本。</li>
<li><strong>幂等性 (Idempotent)</strong>: Ansible 的设计保证了重复运行 playbook 会收敛到期望的状态，这对于更新和保持配置一致性很有用。</li>
<li><strong>基础设施无关性</strong>: 只要 Ansible 能通过 SSH 连接到主机，就可以在裸金属、虚拟机、公有云或私有云上部署。</li>
</ul>
<p><strong>缺点</strong>:</p>
<ul>
<li><strong>需要 Ansible 知识</strong>: 使用者需要理解 Ansible 的基本概念（inventory, playbooks, variables, roles）。</li>
<li><strong>复杂性</strong>: 大量的配置变量和层层嵌套的 playbook 结构，在出现问题时可能难以调试。</li>
<li><strong>命令式本质</strong>: 虽然 Ansible 定义了期望状态，但其执行过程是一系列的任务步骤（命令式）。这与 KOPS 或 Cluster API 的完全声明式方法不同。图片中提到<strong>缺少基于声明式 API 的支持</strong>，指的就是它不像 Cluster API 那样提供一个 K8s CRD 风格的 API 来管理集群生命周期。</li>
</ul>
<h4 id="KOPS-Kubernetes-Operations"><a href="#KOPS-Kubernetes-Operations" class="headerlink" title="KOPS (Kubernetes Operations)"></a>KOPS (Kubernetes Operations)</h4><p><strong>是什么</strong>: KOPS 是一个 Kubernetes 官方 SIG 项目，用于创建、销毁、升级和维护<strong>生产级别、高可用的</strong> Kubernetes 集群。它主要<strong>面向云环境</strong>（对 AWS 的支持最好，对 GCP、Azure、OpenStack 等也有不同程度的支持），并采用<strong>声明式方法</strong>。</p>
<p><strong>原理与工作方式</strong>:</p>
<ul>
<li><p><strong>声明式状态</strong>: 你在一个 <strong>YAML 清单文件</strong>中定义集群的<strong>期望状态</strong>。这包括 Kubernetes 版本、节点实例类型和数量、网络配置（VPC&#x2F;子网选择、CNI）、高可用设置（跨可用区的多主节点）、IAM 角色、负载均衡器等。这些在 KOPS 内部被表示为 <code>Cluster</code> 和 <code>InstanceGroup</code> 等自定义资源。</p>
</li>
<li><p><strong>云 API 集成</strong>: KOPS 直接与目标<strong>云服务商的 API</strong> 交互，来<strong>自动置备所需的基础设施</strong>（如虚拟机、VPC、子网、安全组、IAM 角色、自动伸缩组 Auto Scaling Groups、负载均衡器 Load Balancers 等）。</p>
</li>
<li><p><strong><code>kops create cluster</code></strong>: 读取你的定义（或命令行参数）并生成详细的配置清单。</p>
</li>
<li><p><strong><code>kops update cluster</code></strong>: 预览为了使云端基础设施与清单匹配所需做的变更。</p>
</li>
<li><p><strong><code>kops update cluster --yes</code></strong>: <strong>应用这些变更</strong>，在云上创建或修改资源，并在实例上安装和配置 Kubernetes 组件。它会自动处理 HA 设置、etcd 配置（通常使用云厂商管理的 etcd 或自动设置 etcd 集群）以及基础的集群引导。</p>
</li>
<li><p><strong>生命周期管理</strong>: 提供用于<strong>集群升级 (<code>kops upgrade cluster</code>)</strong>、伸缩节点组 (<code>kops edit ig nodes</code>)、管理附加组件等的命令。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 在 AWS 上定义并创建集群的示例命令</span><br><span class="hljs-built_in">export</span> NAME=mycluster.k8s.local<br><span class="hljs-comment"># 将 KOPS 状态存储在 S3 bucket 中</span><br><span class="hljs-built_in">export</span> KOPS_STATE_STORE=s3://my-kops-state-bucket<br><br>kops create cluster \<br>  --zones=us-west-2a,us-west-2b,us-west-2c \<br>  --node-count=3 \<br>  --node-size=t3.medium \<br>  --master-zones=us-west-2a,us-west-2b,us-west-2c \<br>  --master-size=t3.medium \<br>  --dns-zone=my-hosted-zone-id \ <span class="hljs-comment"># 或者使用 --dns private 进行基于 gossip 的发现</span><br>  --networking=calico \<br>  <span class="hljs-variable">$&#123;NAME&#125;</span><br><br><span class="hljs-comment"># 预览变更计划</span><br>kops update cluster <span class="hljs-variable">$&#123;NAME&#125;</span><br><br><span class="hljs-comment"># 应用变更 (置备基础设施并安装 K8s)</span><br>kops update cluster <span class="hljs-variable">$&#123;NAME&#125;</span> --<span class="hljs-built_in">yes</span><br></code></pre></td></tr></table></figure></li>
</ul>
<p><strong>优点</strong>:</p>
<ul>
<li><strong>声明式与云原生</strong>: 与 Kubernetes 的声明式模型高度一致。<strong>与云服务商的深度集成</strong>简化了基础设施管理，<strong>非常适合利用云平台的能力</strong>。</li>
<li><strong>生产级 HA</strong>: 设计时就将高可用作为核心目标，自动化了多主节点、跨可用区（AZ）的部署。</li>
<li><strong>强大的生命周期管理</strong>: 通过更新清单文件，对集群升级、伸缩和配置变更提供稳健的支持。</li>
<li><strong>遵循社区标准 (Cluster API)</strong>: 尽管有自己的历史 API，KOPS 正在逐步向 <strong>Cluster API</strong> 标准靠拢。Cluster API 旨在提供一个跨云服务商的、统一的声明式 API 来管理集群生命周期。正如图片所说，<strong>基于社区标准的 Cluster API 进行集群管理</strong>，<strong>节点的操作系统安装等全自动化</strong>。</li>
</ul>
<p><strong>缺点</strong>:</p>
<ul>
<li><strong>侧重于云服务商&#x2F;存在锁定风险</strong>: <strong>与云环境深度集成</strong>意味着它在特定的云平台上（尤其是 AWS）表现最好。对于本地（on-premise）部署或支持较少的云平台，其<strong>灵活性</strong>不如 Kubespray 或二进制安装。</li>
<li><strong>主观性强 (Opinionated)</strong>: KOPS 对基础设施的设置（如 VPC 布局、实例配置）做了很多默认决策，可能不适用于所有场景。在基础设施层面的灵活性低于 Kubespray。</li>
<li><strong>抽象泄漏</strong>: 调试问题时，可能既需要理解 KOPS 的内部工作原理，也需要了解它所管理的底层云服务商资源。</li>
</ul>
<p><strong>总结</strong>:</p>
<p>选择哪种安装方法取决于你的目标和环境：</p>
<ul>
<li><strong>二进制安装</strong>: 追求最极致的控制和最深入的学习，不介意极高的复杂性和维护成本。</li>
<li><strong>Kubeadm</strong>: 想要一个相对简单的方式来引导符合最佳实践的核心集群，同时愿意手动处理 OS 配置和 CNI 安装，是在学习和生产之间的一个良好平衡点。</li>
<li><strong>Kubespray</strong>: 需要在包括本地环境在内的各种基础设施上实现高度自动化的、可定制的集群部署，并且熟悉或愿意学习 Ansible。</li>
<li><strong>KOPS</strong>: 主要在主流公有云上部署生产级、高可用的集群，并希望利用声明式方法和云集成来简化基础设施和集群生命周期管理。</li>
</ul>
<p>对于系统性学习，从 Kubeadm 开始可以让你理解核心的引导过程，同时避免二进制安装的过度复杂。之后，根据你的目标环境（云或本地）和对自动化程度的需求，可以进一步学习 KOPS 或 Kubespray。</p>
<h3 id="Kubespray：解决-Kubernetes-集群部署的复杂性"><a href="#Kubespray：解决-Kubernetes-集群部署的复杂性" class="headerlink" title="Kubespray：解决 Kubernetes 集群部署的复杂性"></a>Kubespray：解决 Kubernetes 集群部署的复杂性</h3><p>部署一个生产级别的 Kubernetes 集群，尤其是高可用集群，是一项复杂的任务。你需要考虑诸多因素：</p>
<ul>
<li><strong>多节点配置</strong>：控制平面节点、工作节点、etcd 节点的角色分配和初始化。</li>
<li><strong>组件安装与版本管理</strong>：选择合适的容器运行时 (Docker&#x2F;Containerd)、kubelet、kubeadm、apiserver、scheduler、controller-manager、etcd 等核心组件，并确保它们的版本兼容性。</li>
<li><strong>网络配置</strong>：选择和部署 CNI (Container Network Interface) 插件，如 Calico、Flannel 等，以实现 Pod 间的通信。</li>
<li><strong>高可用性 (HA)</strong>：确保控制平面和 etcd 集群的冗余和故障切换能力。</li>
<li><strong>证书管理</strong>：生成和分发集群所需的各种 TLS 证书。</li>
<li><strong>可扩展性和可升级性</strong>：集群后续的节点增删和版本升级。</li>
</ul>
<p>手动执行这些步骤不仅耗时，而且极易出错。<strong>Kubespray 的核心价值在于它通过 Ansible 自动化了整个 Kubernetes 集群的部署、配置、扩展和升级过程</strong>。它提供了一套经过社区验证和广泛测试的 Ansible Playbook 和 Role，允许用户通过声明式配置来定义集群的期望状态，然后由 Ansible 负责在目标机器上执行所有必要的任务，以达到这个状态。</p>
<h4 id="Kubespray-的核心原理与工作流程"><a href="#Kubespray-的核心原理与工作流程" class="headerlink" title="Kubespray 的核心原理与工作流程"></a>Kubespray 的核心原理与工作流程</h4><p>Kubespray 本质上是一系列 <strong>Ansible Playbook 和 Role</strong> 的集合。Ansible 是一款强大的自动化工具，它通过 SSH（默认）连接到目标主机，执行预定义的任务，无需在目标主机上安装 Agent。</p>
<p>从图中我们可以看到 Kubespray 的主要工作流程：</p>
<ol>
<li><p><strong>定义集群清单 (Inventory)</strong>：</p>
<ul>
<li>用户首先需要定义集群的拓扑结构。这通常通过一个 <strong><code>hosts.yml</code></strong> 文件（或者直接是 Ansible 的 <code>inventory.ini</code> 格式文件）来完成。</li>
<li><code>hosts.yml</code> 中会定义主机组，例如：<ul>
<li><code>all</code>: 包含所有参与集群的主机。</li>
<li><code>kube_control_plane</code>: 指定哪些主机将作为 Kubernetes 的控制平面节点（运行 apiserver, scheduler, controller-manager）。</li>
<li><code>kube_node</code>: 指定哪些主机将作为 Kubernetes 的工作节点（运行 kubelet 和容器运行时，承载 Pod）。</li>
<li><code>etcd</code>: 指定哪些主机将组成 etcd 集群（存储 Kubernetes 的所有状态数据）。通常建议 etcd 节点与控制平面节点部署在一起，或者作为独立的集群。对于高可用，etcd 至少需要3个节点。</li>
<li><code>k8s_cluster</code>: 这是一个组合组，通常包含了 <code>kube_control_plane</code> 和 <code>kube_node</code>。</li>
<li><code>calico_rr</code>: (可选) 如果使用 Calico CNI 并且需要 BGP Route Reflector，会定义这个组。</li>
</ul>
</li>
<li>图中显示的 <strong><code>inventory_builder</code></strong> 是一个辅助脚本，它可以基于更简洁的 <code>hosts.yml</code> (如示例中只列出 node1, node2, node3) 来动态生成符合 Ansible 要求的 <code>inventory.ini</code> 文件。这个 <code>inventory.ini</code> 文件会更详细地描述每个主机属于哪些组。</li>
</ul>
</li>
<li><p><strong>配置集群变量 (Variables)</strong>：</p>
<ul>
<li>Kubespray 提供了大量的可配置变量，允许用户定制化集群的各个方面。这些变量通常定义在 <code>group_vars</code> 目录下的文件中，例如 <code>all/all.yml</code>, <code>k8s_cluster/k8s-cluster.yml</code>, <code>etcd.yml</code> 等。</li>
<li>图中特别指出了 <strong><code>vars.yml</code></strong> (这可能是一个概括性的说法，实际上变量会分散在多个文件中)。关键的配置项包括：<ul>
<li><strong><code>gcr_image_repo</code> &#x2F; <code>kube_image_repo</code></strong>: Kubernetes 的核心组件镜像默认存储在 <code>k8s.gcr.io</code> (Google Container Registry)。在国内或其他访问 Google 服务受限的区域，需要将其替换为国内的镜像仓库地址，例如图中的 <code>registry.aliyuncs.com/google_containers</code>。<strong>这是确保在国内顺利部署的关键配置</strong>。</li>
<li><strong><code>etcd_download_url</code> &#x2F; <code>cni_download_url</code></strong>: 类似地，etcd 的二进制文件和 CNI 插件（如 Calico, Flannel）的二进制文件或镜像可能也需要从可访问的源下载。</li>
<li><strong><code>ghproxy.com</code> (Github 代理)</strong>: Kubespray 在部署过程中可能会从 Github 下载一些资源（例如 CNI 插件的 manifest 文件）。如果直接访问 Github 速度慢或不稳定，可以通过配置代理来加速下载。</li>
</ul>
</li>
<li>其他重要变量还包括：Kubernetes 版本 (<code>kube_version</code>)、CNI 插件选择 (<code>kube_network_plugin</code>)、容器运行时选择 (<code>container_manager</code>，可以是 <code>docker</code> 或 <code>containerd</code>)、高可用模式配置等。</li>
</ul>
</li>
<li><p><strong>执行 Ansible Playbook</strong>:</p>
<ul>
<li>一旦清单和变量配置完毕，用户就可以通过 <strong><code>ansible-playbook</code></strong> 命令来执行 Kubespray 的主 playbook (通常是 <code>cluster.yml</code>)。</li>
<li><code>ansible-playbook -i inventory/mycluster/hosts.yml cluster.yml -b -v --private-key=~/.ssh/id_rsa</code><ul>
<li><code>-i inventory/mycluster/hosts.yml</code>: 指定清单文件路径。</li>
<li><code>cluster.yml</code>: Kubespray 的主 playbook 文件，它会调用一系列 roles 来完成集群部署。</li>
<li><code>-b</code> (<code>--become</code>): 表示以特权用户 (通常是 root) 执行任务，因为安装系统组件需要高权限。</li>
<li><code>-v</code>: 详细输出模式。</li>
<li><code>--private-key</code>: 指定用于 SSH 免密登录的私钥。<strong>Kubespray 要求部署节点能够免密登录到所有集群节点</strong>。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>在目标节点上安装和配置组件</strong>:</p>
<ul>
<li>Ansible Playbook 会在清单中定义的各个节点上执行一系列任务，包括：<ul>
<li>操作系统预配置：安装依赖包、配置内核参数 (如 <code>net.bridge.bridge-nf-call-iptables=1</code>)、禁用 SELinux (如果需要)、配置防火墙规则等。</li>
<li>安装容器运行时：根据 <code>container_manager</code> 的配置安装 Docker 或 Containerd。</li>
<li>下载和分发 Kubernetes 二进制文件和镜像。</li>
<li>配置和启动 <strong>etcd</strong> 集群。</li>
<li>使用 <strong>kubeadm</strong> (Kubespray 在底层会利用 kubeadm 的一些功能) 初始化控制平面节点，生成证书，配置 <strong>kube-apiserver, kube-scheduler, kube-controller-manager</strong>。</li>
<li>在工作节点上配置和启动 <strong>kubelet</strong> 和 <strong>kube-proxy</strong>。</li>
<li>部署 CNI 插件，使得 Pod 网络能够正常工作。</li>
<li>配置高可用：例如，如果部署了多个控制平面节点，Kubespray 会配置负载均衡器（可以是外部的，也可以是基于 keepalived+haproxy&#x2F;nginx 的内部负载均衡方案）来分发对 API Server 的请求。</li>
</ul>
</li>
</ul>
</li>
</ol>
<p>图中右侧的“计算节点集群”展示了最终在每个 Node (包括控制平面节点和工作节点) 上运行的核心组件。例如，<strong>Docker&#x2F;Containerd</strong> 作为容器运行时，<strong>kubelet</strong> 作为节点代理与控制平面通信并管理 Pod 生命周期，<strong>kubeadm</strong> 在初始化阶段被使用，<strong>Etcd</strong> 存储集群状态，<strong>Apiserver</strong> 提供 API 接口，<strong>Scheduler</strong> 负责 Pod 调度。</p>
<h4 id="Kubespray-的高可用性-HA-实现"><a href="#Kubespray-的高可用性-HA-实现" class="headerlink" title="Kubespray 的高可用性 (HA) 实现"></a>Kubespray 的高可用性 (HA) 实现</h4><p>Kubespray 对高可用性的支持是其核心特性之一。</p>
<ul>
<li><strong>Etcd HA</strong>: 通过部署奇数个 (通常是 3 或 5 个) etcd 节点组成集群。Raft 协议保证了数据的一致性和容错性。</li>
<li><strong>Control Plane HA</strong>:<ul>
<li>部署多个控制平面节点 (apiserver, scheduler, controller-manager)。</li>
<li><strong>API Server 负载均衡</strong>: 需要一个负载均衡器（LB）在多个 API Server 实例前。这个 LB 可以是云提供商的 LB、硬件 LB，或者 Kubespray 可以帮你部署一个基于 <strong>nginx&#x2F;haproxy + keepalived</strong> 的本地高可用负载均衡方案。Keepalived 通过 VRRP 协议提供一个虚拟 IP (VIP)，当主 LB 节点故障时，VIP 会漂移到备用节点。</li>
<li><strong>Scheduler 和 Controller Manager</strong>: Kubernetes 内部通过 leader election 机制确保在任何时候只有一个 scheduler 和一个 controller-manager 实例处于活动状态，其他实例处于备用状态。</li>
</ul>
</li>
</ul>
<h4 id="配置示例-Inventory-片段"><a href="#配置示例-Inventory-片段" class="headerlink" title="配置示例 (Inventory 片段)"></a>配置示例 (Inventory 片段)</h4><p>一个简化的 <code>inventory/mycluster/hosts.ini</code> (或由 <code>hosts.yml</code> 生成) 可能如下所示：</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-section">[all]</span><br>master01 <span class="hljs-attr">ansible_host</span>=<span class="hljs-number">192.168</span>.<span class="hljs-number">10.11</span> ip=<span class="hljs-number">192.168</span>.<span class="hljs-number">10.11</span> etcd_member_name=etcd1<br>master02 <span class="hljs-attr">ansible_host</span>=<span class="hljs-number">192.168</span>.<span class="hljs-number">10.12</span> ip=<span class="hljs-number">192.168</span>.<span class="hljs-number">10.12</span> etcd_member_name=etcd2<br>master03 <span class="hljs-attr">ansible_host</span>=<span class="hljs-number">192.168</span>.<span class="hljs-number">10.13</span> ip=<span class="hljs-number">192.168</span>.<span class="hljs-number">10.13</span> etcd_member_name=etcd3<br>worker01 <span class="hljs-attr">ansible_host</span>=<span class="hljs-number">192.168</span>.<span class="hljs-number">10.21</span> ip=<span class="hljs-number">192.168</span>.<span class="hljs-number">10.21</span><br>worker02 <span class="hljs-attr">ansible_host</span>=<span class="hljs-number">192.168</span>.<span class="hljs-number">10.22</span> ip=<span class="hljs-number">192.168</span>.<span class="hljs-number">10.22</span><br><br><span class="hljs-section">[kube_control_plane]</span><br>master01<br>master02<br>master03<br><br><span class="hljs-section">[etcd]</span><br>master01<br>master02<br>master03<br><br><span class="hljs-section">[kube_node]</span><br>worker01<br>worker02<br><br><span class="hljs-section">[k8s_cluster:children]</span><br>kube_control_plane<br>kube_node<br></code></pre></td></tr></table></figure>

<p>在 <code>group_vars/all/all.yml</code> 中配置镜像仓库：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># Kubernetes GCR Image Registry</span><br><span class="hljs-attr">gcr_image_repo:</span> <span class="hljs-string">&quot;registry.aliyuncs.com/google_containers&quot;</span><br><span class="hljs-attr">kube_image_repo:</span> <span class="hljs-string">&quot;registry.aliyuncs.com/google_containers&quot;</span><br><span class="hljs-comment"># Quay Image Registry</span><br><span class="hljs-attr">quay_image_repo:</span> <span class="hljs-string">&quot;quay.mirrors.ustc.edu.cn&quot;</span> <span class="hljs-comment"># 例如，如果Calico镜像在quay.io</span><br><span class="hljs-comment"># Docker Hub Registry</span><br><span class="hljs-attr">docker_image_repo:</span> <span class="hljs-string">&quot;dockerhub.azk8s.cn&quot;</span> <span class="hljs-comment"># 例如，一些辅助镜像可能在Docker Hub</span><br></code></pre></td></tr></table></figure>

<p>在 <code>group_vars/k8s_cluster/k8s-cluster.yml</code> 中配置 CNI：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">kube_network_plugin:</span> <span class="hljs-string">calico</span><br><span class="hljs-comment"># 或者</span><br><span class="hljs-comment"># kube_network_plugin: flannel</span><br></code></pre></td></tr></table></figure>

<h3 id="声明式-API-与集群管理"><a href="#声明式-API-与集群管理" class="headerlink" title="声明式 API 与集群管理"></a>声明式 API 与集群管理</h3><img src="/2025/04/24/kubernetes%E7%94%9F%E4%BA%A7%E5%8C%96%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/image-20250509131354765.png" srcset="/img/loading.gif" lazyload class="" title="image-20250509131354765">

<p>在云原生场景中，我们追求的目标是让系统按照我们 <strong>期望的状态 (Desired State)</strong> 运行。Kubernetes 本身就是基于声明式 API 构建的：你通过 YAML 文件描述你想要的应用部署、服务配置等，然后 Kubernetes 的控制器会不断地工作，将 <strong>当前状态 (Current State)</strong> 调整到你声明的期望状态。</p>
<p>然而，在 Cluster API 出现之前，Kubernetes 集群本身的创建、升级、扩缩容等生命周期管理操作，往往依赖于各种不同的工具（如 <code>kubeadm</code>, <code>kops</code>, <code>kubespray</code>，或云厂商的控制台&#x2F;CLI），这些工具很多是 <strong>命令式 (Imperative)</strong> 的。这意味着你需要执行一系列步骤来达到某个状态，而不是声明一个最终状态让系统去实现。这种方式在自动化、一致性和跨平台管理上存在挑战。</p>
<p><strong>Cluster API 项目的核心目标就是将 Kubernetes 的声明式 API 模型扩展到集群自身的生命周期管理上。</strong> 这意味着你可以像管理应用一样，用 Kubernetes API（通过自定义资源 CRD）来创建、配置和管理 Kubernetes 集群。</p>
<h4 id="Cluster-API-详解"><a href="#Cluster-API-详解" class="headerlink" title="Cluster API 详解"></a>Cluster API 详解</h4><p>Cluster API 允许用户通过在“管理集群 (Management Cluster)”上创建 Kubernetes 自定义资源 (CRDs) 来声明“工作负载集群 (Workload Clusters)”的期望状态。然后，一系列的控制器会监听这些资源的变化，并驱动底层的云服务商或基础设施提供商来创建和调整实际的集群资源。</p>
<h4 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h4><ol>
<li><strong>标准化集群部署</strong>：提供一套标准的 API 来定义和管理集群，无论底层是 AWS、Azure、GCP、vSphere 还是裸金属。</li>
<li><strong>自动化集群生命周期管理</strong>：通过控制器模式，自动化集群的创建、扩缩容、升级和删除。</li>
<li><strong>GitOps for Clusters</strong>：集群的定义可以存储在 Git 仓库中，通过 GitOps 工作流来管理集群的变更，实现可追溯和可审计的集群运维。</li>
<li><strong>可插拔的基础设施提供商</strong>：通过定义良好的接口，允许不同的基础设施提供商（Infrastructure Providers）接入，实现跨云和本地环境的统一管理。</li>
<li><strong>提升开发者和运维体验</strong>：对于熟悉 Kubernetes API 的用户来说，学习成本更低，管理方式更统一。</li>
</ol>
<h4 id="核心架构与组件"><a href="#核心架构与组件" class="headerlink" title="核心架构与组件"></a>核心架构与组件</h4><ol>
<li><p><strong>管理集群 (Management Cluster)</strong>：</p>
<ul>
<li>这是一个预先存在的 Kubernetes 集群，用于运行 Cluster API 的控制器。</li>
<li>你在这个集群上安装 Cluster API 的 CRDs 和相关的控制器。</li>
<li>你通过 <code>kubectl</code> 与这个管理集群交互，来定义和控制你的工作负载集群。</li>
</ul>
</li>
<li><p><strong>工作负载集群 (Workload Cluster &#x2F; Target Cluster)</strong>：</p>
<ul>
<li>这些是你希望通过 Cluster API 创建和管理的 Kubernetes 集群。它们是真正运行你的应用程序的地方。</li>
<li>这些集群可以是运行在任何受支持的基础设施上。</li>
</ul>
</li>
<li><p><strong>提供商 (Providers)</strong>：</p>
<ul>
<li><strong>Cluster API Controller (Core Provider)</strong>：这是 Cluster API 的核心，提供了与基础设施无关的集群生命周期管理逻辑，例如 <code>Machine</code>, <code>MachineSet</code>, <code>MachineDeployment</code> 等 CRD 和对应的控制器。</li>
<li><strong>Infrastructure Provider (基础设施提供商)</strong>：负责与特定的基础设施（如 AWS, Azure, GCP, vSphere, OpenStack, Bare Metal 等）交互，以创建和管理该基础设施上的资源（如虚拟机、网络、负载均衡器等）。例如，<code>cluster-api-provider-aws</code> (CAPA), <code>cluster-api-provider-azure</code> (CAPZ)。每个基础设施提供商都会定义自己的 CRD，如 <code>AWSCluster</code>, <code>AWSMachine</code>。<ul>
<li>例如，当你想在 AWS 上创建一个集群，你需要安装 CAPA。CAPA 会提供 <code>AWSCluster</code> CRD 来描述 AWS 相关的集群配置 (如 VPC, Region) 和 <code>AWSMachine</code> CRD 来描述 EC2 实例的配置 (如实例类型, AMI ID)。</li>
</ul>
</li>
<li><strong>Bootstrap Provider (引导提供商)</strong>：负责将一个基础设施上的机器（如 VM）转变为一个 Kubernetes 节点。它通常使用 <code>cloud-init</code> 或类似的机制来安装 <code>kubelet</code>, <code>kubeadm</code>, <code>containerd</code> 等，并执行 <code>kubeadm init</code> 或 <code>kubeadm join</code>。默认且最常用的是 <code>cluster-api-bootstrap-provider-kubeadm</code> (CABPK)。它会提供如 <code>KubeadmConfig</code> 这样的 CRD。</li>
<li><strong>Control Plane Provider (控制平面提供商)</strong>：负责管理工作负载集群的控制平面。它通常与引导提供商协同工作，确保控制平面的高可用性、升级等。默认且最常用的是 <code>cluster-api-control-plane-provider-kubeadm</code> (KCP)。它会提供如 <code>KubeadmControlPlane</code> 这样的 CRD。</li>
</ul>
</li>
</ol>
<h4 id="关键自定义资源-CRDs"><a href="#关键自定义资源-CRDs" class="headerlink" title="关键自定义资源 (CRDs)"></a>关键自定义资源 (CRDs)</h4><p>这些 CRDs 是你用来声明集群状态的 “API 对象”：</p>
<ul>
<li><p><strong><code>Cluster</code></strong>: 顶层资源，代表一个 Kubernetes 集群。它通常引用一个基础设施提供商的集群资源 (如 <code>AWSCluster</code>) 和一个控制平面提供商的资源 (如 <code>KubeadmControlPlane</code>)。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">cluster.x-k8s.io/v1beta1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Cluster</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">my-cluster</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">default</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">clusterNetwork:</span><br>    <span class="hljs-attr">pods:</span><br>      <span class="hljs-attr">cidrBlocks:</span> [<span class="hljs-string">&quot;192.168.0.0/16&quot;</span>]<br>  <span class="hljs-attr">controlPlaneRef:</span><br>    <span class="hljs-attr">apiVersion:</span> <span class="hljs-string">controlplane.cluster.x-k8s.io/v1beta1</span><br>    <span class="hljs-attr">kind:</span> <span class="hljs-string">KubeadmControlPlane</span><br>    <span class="hljs-attr">name:</span> <span class="hljs-string">my-cluster-control-plane</span><br>  <span class="hljs-attr">infrastructureRef:</span><br>    <span class="hljs-attr">apiVersion:</span> <span class="hljs-string">infrastructure.cluster.x-k8s.io/v1beta1</span> <span class="hljs-comment"># 假设是 AWS</span><br>    <span class="hljs-attr">kind:</span> <span class="hljs-string">AWSCluster</span><br>    <span class="hljs-attr">name:</span> <span class="hljs-string">my-cluster-aws</span><br></code></pre></td></tr></table></figure>
</li>
<li><p><strong><code>InfrastructureCluster</code> (例如 <code>AWSCluster</code>, <code>AzureCluster</code>, <code>VSphereCluster</code>)</strong>: 特定于基础设施的集群级配置。由相应的基础设施提供商定义。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">infrastructure.cluster.x-k8s.io/v1beta1</span> <span class="hljs-comment"># 示例为 AWS</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">AWSCluster</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">my-cluster-aws</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">default</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">region:</span> <span class="hljs-string">&quot;us-east-1&quot;</span><br>  <span class="hljs-attr">sshKeyName:</span> <span class="hljs-string">&quot;my-ssh-key&quot;</span><br>  <span class="hljs-comment"># ...其他 AWS 特定配置，如 VPC ID, Subnet IDs</span><br></code></pre></td></tr></table></figure>
</li>
<li><p><strong><code>ControlPlane</code> (例如 <code>KubeadmControlPlane</code>)</strong>: 定义工作负载集群控制平面的期望状态，如副本数、Kubernetes 版本。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">controlplane.cluster.x-k8s.io/v1beta1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">KubeadmControlPlane</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">my-cluster-control-plane</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">default</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">replicas:</span> <span class="hljs-number">3</span> <span class="hljs-comment"># 期望3个控制平面节点</span><br>  <span class="hljs-attr">version:</span> <span class="hljs-string">&quot;v1.28.3&quot;</span> <span class="hljs-comment"># 期望的 Kubernetes 版本</span><br>  <span class="hljs-attr">machineTemplate:</span><br>    <span class="hljs-attr">infrastructureRef:</span><br>      <span class="hljs-attr">apiVersion:</span> <span class="hljs-string">infrastructure.cluster.x-k8s.io/v1beta1</span> <span class="hljs-comment"># 假设是 AWS</span><br>      <span class="hljs-attr">kind:</span> <span class="hljs-string">AWSMachineTemplate</span><br>      <span class="hljs-attr">name:</span> <span class="hljs-string">my-cluster-control-plane-template</span><br>  <span class="hljs-attr">kubeadmConfigSpec:</span><br>    <span class="hljs-comment"># kubeadm init 和 join 的配置</span><br>    <span class="hljs-attr">initConfiguration:</span><br>      <span class="hljs-attr">nodeRegistration:</span><br>        <span class="hljs-attr">kubeletExtraArgs:</span><br>          <span class="hljs-attr">cloud-provider:</span> <span class="hljs-string">&quot;aws&quot;</span><br>    <span class="hljs-attr">joinConfiguration:</span><br>      <span class="hljs-attr">nodeRegistration:</span><br>        <span class="hljs-attr">kubeletExtraArgs:</span><br>          <span class="hljs-attr">cloud-provider:</span> <span class="hljs-string">&quot;aws&quot;</span><br></code></pre></td></tr></table></figure>
</li>
<li><p><strong><code>Machine</code></strong>: 代表一个单独的节点（物理机或虚拟机）。它通常引用一个引导配置 (如 <code>KubeadmConfig</code>) 和一个基础设施机器配置 (如 <code>AWSMachine</code>)。<strong>通常你不会直接创建 <code>Machine</code>，而是通过 <code>MachineSet</code> 或 <code>MachineDeployment</code>。</strong></p>
</li>
<li><p><strong><code>MachineSet</code></strong>: 确保指定数量的 <code>Machine</code> 副本正在运行，类似于 Kubernetes 的 <code>ReplicaSet</code> 对于 <code>Pod</code>。</p>
</li>
<li><p><strong><code>MachineDeployment</code></strong>: 为 <code>MachineSet</code> 和 <code>Machine</code> 提供声明式的更新能力，类似于 Kubernetes 的 <code>Deployment</code> 对于 <code>Pod</code>。<strong>这是管理工作节点 (worker nodes) 的推荐方式。</strong></p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">cluster.x-k8s.io/v1beta1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">MachineDeployment</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">my-cluster-md-0</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">default</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">clusterName:</span> <span class="hljs-string">my-cluster</span><br>  <span class="hljs-attr">replicas:</span> <span class="hljs-number">3</span> <span class="hljs-comment"># 期望3个工作节点</span><br>  <span class="hljs-attr">selector:</span><br>    <span class="hljs-attr">matchLabels:</span><br>      <span class="hljs-attr">cluster.x-k8s.io/cluster-name:</span> <span class="hljs-string">my-cluster</span><br>      <span class="hljs-comment"># ...</span><br>  <span class="hljs-attr">template:</span><br>    <span class="hljs-attr">spec:</span><br>      <span class="hljs-attr">clusterName:</span> <span class="hljs-string">my-cluster</span><br>      <span class="hljs-attr">version:</span> <span class="hljs-string">&quot;v1.28.3&quot;</span> <span class="hljs-comment"># 期望的 Kubernetes 版本</span><br>      <span class="hljs-attr">bootstrap:</span><br>        <span class="hljs-attr">configRef:</span><br>          <span class="hljs-attr">apiVersion:</span> <span class="hljs-string">bootstrap.cluster.x-k8s.io/v1beta1</span><br>          <span class="hljs-attr">kind:</span> <span class="hljs-string">KubeadmConfigTemplate</span><br>          <span class="hljs-attr">name:</span> <span class="hljs-string">my-cluster-md-0-bootstrap-template</span><br>      <span class="hljs-attr">infrastructureRef:</span><br>        <span class="hljs-attr">apiVersion:</span> <span class="hljs-string">infrastructure.cluster.x-k8s.io/v1beta1</span> <span class="hljs-comment"># 假设是 AWS</span><br>        <span class="hljs-attr">kind:</span> <span class="hljs-string">AWSMachineTemplate</span><br>        <span class="hljs-attr">name:</span> <span class="hljs-string">my-cluster-md-0-infra-template</span><br></code></pre></td></tr></table></figure>
</li>
<li><p><strong><code>BootstrapConfigTemplate</code> (例如 <code>KubeadmConfigTemplate</code>)</strong>: 为 <code>Machine</code> 定义引导配置的模板。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">bootstrap.cluster.x-k8s.io/v1beta1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">KubeadmConfigTemplate</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">my-cluster-md-0-bootstrap-template</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">default</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">template:</span><br>    <span class="hljs-attr">spec:</span><br>      <span class="hljs-comment"># kubeadm join 的配置</span><br>      <span class="hljs-attr">joinConfiguration:</span><br>        <span class="hljs-attr">nodeRegistration:</span><br>          <span class="hljs-attr">kubeletExtraArgs:</span><br>            <span class="hljs-attr">cloud-provider:</span> <span class="hljs-string">&quot;aws&quot;</span><br>          <span class="hljs-comment"># ...</span><br></code></pre></td></tr></table></figure>
</li>
<li><p><strong><code>InfrastructureMachineTemplate</code> (例如 <code>AWSMachineTemplate</code>, <code>AzureMachineTemplate</code>)</strong>: 为 <code>Machine</code> 定义基础设施特定配置的模板。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">infrastructure.cluster.x-k8s.io/v1beta1</span> <span class="hljs-comment"># 示例为 AWS</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">AWSMachineTemplate</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">my-cluster-md-0-infra-template</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">default</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">template:</span><br>    <span class="hljs-attr">spec:</span><br>      <span class="hljs-attr">instanceType:</span> <span class="hljs-string">&quot;t3.medium&quot;</span><br>      <span class="hljs-attr">ami:</span><br>        <span class="hljs-attr">id:</span> <span class="hljs-string">&quot;ami-xxxxxxxxxxxxxxxxx&quot;</span> <span class="hljs-comment"># AWS AMI ID</span><br>      <span class="hljs-attr">sshKeyName:</span> <span class="hljs-string">&quot;my-ssh-key&quot;</span><br>      <span class="hljs-comment"># ... 其他 AWS 实例配置</span><br></code></pre></td></tr></table></figure>
</li>
<li><p><strong><code>MachineHealthCheck</code></strong>: 定义节点健康检查的条件和修复策略（例如，当节点状态为 <code>NotReady</code> 超过一定时间，则删除并重建该 <code>Machine</code>）。</p>
</li>
</ul>
<h4 id="工作流程（以创建集群为例）"><a href="#工作流程（以创建集群为例）" class="headerlink" title="工作流程（以创建集群为例）"></a>工作流程（以创建集群为例）</h4><ol>
<li><p><strong>初始化管理集群</strong>: 使用 <code>clusterctl init --infrastructure aws</code> (以 AWS 为例) 在你的管理集群上安装 Cluster API 核心组件、AWS 基础设施提供商、Kubeadm 引导提供商和 Kubeadm 控制平面提供商。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 示例：初始化管理集群以支持 AWS</span><br>clusterctl init --infrastructure aws<br></code></pre></td></tr></table></figure>

<p>这个命令会下载并应用所需的 CRDs 和部署对应的控制器 Pods。</p>
</li>
<li><p><strong>定义集群</strong>: 创建上述 YAML 文件（<code>Cluster</code>, <code>AWSCluster</code>, <code>KubeadmControlPlane</code>, <code>MachineDeployment</code>, <code>AWSMachineTemplate</code>, <code>KubeadmConfigTemplate</code> 等）。<br>你可以使用 <code>clusterctl generate cluster</code> 命令来帮助生成这些 YAML 文件的模板。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 示例：生成一个名为 my-cluster 的集群配置文件模板</span><br><span class="hljs-comment"># 你需要设置一些环境变量，如 AWS_REGION, AWS_SSH_KEY_NAME 等</span><br><span class="hljs-comment"># KUBERNETES_VERSION, CONTROL_PLANE_MACHINE_COUNT, WORKER_MACHINE_COUNT</span><br>clusterctl generate cluster my-cluster \<br>  --kubernetes-version v1.28.3 \<br>  --control-plane-machine-count=3 \<br>  --worker-machine-count=3 \<br>  &gt; my-cluster.yaml<br></code></pre></td></tr></table></figure>
</li>
<li><p><strong>应用集群定义</strong>: 将这些 YAML 文件 <code>kubectl apply -f &lt;your-cluster-definition&gt;.yaml</code> 到管理集群。</p>
</li>
<li><p><strong>控制器协同工作</strong>:</p>
<ul>
<li><code>Cluster</code> 控制器监听到新的 <code>Cluster</code> 对象。</li>
<li>它会根据 <code>infrastructureRef</code> 找到对应的 <code>AWSCluster</code> 对象。AWS 基础设施提供商的控制器会开始创建 VPC、子网、安全组等（如果需要）。</li>
<li>它会根据 <code>controlPlaneRef</code> 找到对应的 <code>KubeadmControlPlane</code> 对象。Kubeadm 控制平面提供商的控制器开始协调控制平面节点的创建。</li>
<li><code>KubeadmControlPlane</code> 控制器会为其定义的 <code>replicas</code> 创建相应数量的 <code>Machine</code> 对象，并关联 <code>AWSMachineTemplate</code> 和 <code>KubeadmConfigTemplate</code>。</li>
<li>对于每个 <code>Machine</code> 对象：<ul>
<li>引导提供商 (CABPK) 的控制器会根据 <code>KubeadmConfigTemplate</code> 生成 <code>cloud-init</code> 脚本（包含 <code>kubeadm init</code> 或 <code>kubeadm join</code> 命令及证书等）。</li>
<li>基础设施提供商 (CAPA) 的控制器会根据 <code>AWSMachineTemplate</code> 在 AWS 上创建 EC2 实例，并将 <code>cloud-init</code> 脚本作为用户数据传递给实例。</li>
<li>EC2 实例启动后，执行 <code>cloud-init</code> 脚本，安装 Kubernetes 组件，然后初始化控制平面或加入现有控制平面。</li>
</ul>
</li>
<li><code>MachineDeployment</code> 控制器会类似地为其定义的 <code>replicas</code> 创建工作节点的 <code>Machine</code> 对象。</li>
<li>所有控制器会持续监控其管理的资源，确保实际状态与期望状态一致。例如，如果一个 EC2 实例意外终止，相应的控制器会尝试创建一个新的实例来替代它。</li>
</ul>
</li>
</ol>
<h4 id="集群生命周期管理示例"><a href="#集群生命周期管理示例" class="headerlink" title="集群生命周期管理示例"></a>集群生命周期管理示例</h4><ol>
<li><p><strong>集群扩缩容</strong>:</p>
<ul>
<li><p><strong>控制平面扩容</strong>: 修改 <code>KubeadmControlPlane</code> 对象的 <code>spec.replicas</code> 字段。KCP 控制器会自动创建或删除控制平面节点。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">kubectl patch kcp my-cluster-control-plane --<span class="hljs-built_in">type</span>=<span class="hljs-string">&#x27;json&#x27;</span> -p=<span class="hljs-string">&#x27;[&#123;&quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/spec/replicas&quot;, &quot;value&quot;: 5&#125;]&#x27;</span><br></code></pre></td></tr></table></figure>
</li>
<li><p><strong>工作节点扩容</strong>: 修改 <code>MachineDeployment</code> 对象的 <code>spec.replicas</code> 字段。<code>MachineDeployment</code> 控制器会自动创建或删除工作节点。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">kubectl scale machinedeployment my-cluster-md-0 --replicas=5<br></code></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p><strong>节点健康检查和自动修复</strong>:<br>创建一个 <code>MachineHealthCheck</code> 对象，指定不健康条件（例如，NodeStatus 为 <code>Unknown</code> 超过 5 分钟）和对应的 <code>MachineDeployment</code> 或 <code>KubeadmControlPlane</code>。如果节点满足不健康条件，<code>MachineHealthCheck</code> 控制器会删除对应的 <code>Machine</code> 对象，然后其所属的 <code>MachineSet</code> (由 <code>MachineDeployment</code> 或 <code>KubeadmControlPlane</code> 管理) 会创建一个新的 <code>Machine</code> 来替换它。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">cluster.x-k8s.io/v1beta1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">MachineHealthCheck</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">my-cluster-mhc</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">clusterName:</span> <span class="hljs-string">my-cluster</span><br>  <span class="hljs-attr">selector:</span> <span class="hljs-comment"># 选择要监控的机器</span><br>    <span class="hljs-attr">matchLabels:</span><br>      <span class="hljs-attr">cluster.x-k8s.io/cluster-name:</span> <span class="hljs-string">my-cluster</span> <span class="hljs-comment"># 通常会监控所有 worker</span><br>  <span class="hljs-attr">unhealthyConditions:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-attr">type:</span> <span class="hljs-string">Ready</span><br>      <span class="hljs-attr">status:</span> <span class="hljs-string">Unknown</span><br>      <span class="hljs-attr">timeout:</span> <span class="hljs-string">300s</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-attr">type:</span> <span class="hljs-string">Ready</span><br>      <span class="hljs-attr">status:</span> <span class="hljs-string">&quot;False&quot;</span><br>      <span class="hljs-attr">timeout:</span> <span class="hljs-string">300s</span><br>  <span class="hljs-comment"># remediationTemplate: (可选，定义修复操作，默认为删除Machine)</span><br></code></pre></td></tr></table></figure>
</li>
<li><p><strong>Kubernetes 升级</strong>:</p>
<ul>
<li><p><strong>控制平面升级</strong>: 修改 <code>KubeadmControlPlane</code> 对象的 <code>spec.version</code> 字段。KCP 控制器会执行滚动升级，一次替换一个控制平面节点。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">kubectl patch kcp my-cluster-control-plane --<span class="hljs-built_in">type</span>=<span class="hljs-string">&#x27;json&#x27;</span> -p=<span class="hljs-string">&#x27;[&#123;&quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/spec/version&quot;, &quot;value&quot;: &quot;v1.29.0&quot;&#125;]&#x27;</span><br></code></pre></td></tr></table></figure>
</li>
<li><p><strong>工作节点升级</strong>: 修改 <code>MachineDeployment</code> 对象的 <code>spec.template.spec.version</code> 字段。<code>MachineDeployment</code> 控制器会执行滚动升级，根据其更新策略（如 <code>RollingUpdate</code>）逐个替换工作节点。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 获取 MachineDeployment 的名称</span><br>MD_NAME=$(kubectl get machinedeployment -l cluster.x-k8s.io/cluster-name=my-cluster -o jsonpath=<span class="hljs-string">&#x27;&#123;.items[0].metadata.name&#125;&#x27;</span>)<br><span class="hljs-comment"># Patch MachineDeployment 的版本</span><br>kubectl patch machinedeployment <span class="hljs-variable">$MD_NAME</span> --<span class="hljs-built_in">type</span>=<span class="hljs-string">&#x27;json&#x27;</span> -p=<span class="hljs-string">&#x27;[&#123;&quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/spec/template/spec/version&quot;, &quot;value&quot;: &quot;v1.29.0&quot;&#125;]&#x27;</span><br></code></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p><strong>操作系统升级</strong>:<br>这通常通过更新 <code>InfrastructureMachineTemplate</code> (例如 <code>AWSMachineTemplate</code>) 中的镜像 ID (如 <code>spec.template.spec.ami.id</code> 对于 AWS) 来实现。然后，<code>MachineDeployment</code> (对于工作节点) 或 <code>KubeadmControlPlane</code> (对于控制平面节点) 会检测到其关联模板的变更（这取决于提供商的实现，通常需要手动触发或等待下一次协调），并执行滚动更新，用新的操作系统镜像创建新的节点来替换旧节点。<br>例如，更新 <code>AWSMachineTemplate</code> 中的 <code>ami</code>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 假设你的 AWSMachineTemplate 名为 my-cluster-md-0-infra-template</span><br>NEW_AMI_ID=<span class="hljs-string">&quot;ami-newxxxxxxxxxxxxxx&quot;</span><br>kubectl patch awsmachinetemplate my-cluster-md-0-infra-template --<span class="hljs-built_in">type</span>=<span class="hljs-string">&#x27;json&#x27;</span> -p=<span class="hljs-string">&quot;[&#123;&#x27;op&#x27;: &#x27;replace&#x27;, &#x27;path&#x27;: &#x27;/spec/template/spec/ami/id&#x27;, &#x27;value&#x27;: &#x27;<span class="hljs-variable">$&#123;NEW_AMI_ID&#125;</span>&#x27;&#125;]&quot;</span><br></code></pre></td></tr></table></figure>

<p>之后，相关的 <code>MachineDeployment</code> 会开始滚动更新其管理的 <code>Machine</code>。</p>
</li>
</ol>
<h3 id="Cluster-API-核心角色"><a href="#Cluster-API-核心角色" class="headerlink" title="Cluster API 核心角色"></a>Cluster API 核心角色</h3><p>Cluster API (CAPI) 是一个 Kubernetes 子项目，它<strong>通过声明式 API 来创建、配置和管理 Kubernetes 集群的生命周期</strong>。它的核心思想是将 Kubernetes 集群本身也作为一种资源，使用 Kubernetes 的方式来管理 Kubernetes (Kubernetes-ception)。</p>
<p>Cluster API 的运作依赖于几个关键的角色，它们协同工作以实现集群的自动化管理。</p>
<ol>
<li><p><strong>管理集群 (Management Cluster)</strong></p>
<ul>
<li><strong>核心职责</strong>：这是 Cluster API 控制器运行的地方，<strong>它负责托管和管理一个或多个 Workload 集群的生命周期</strong>。</li>
<li><strong>关键特征</strong>：管理集群本身是一个已经存在的 Kubernetes 集群。<strong>所有描述 Workload 集群的 Cluster API 对象（自定义资源，CRDs）都存储在管理集群的 etcd 中</strong>。例如，<code>Cluster</code>、<code>Machine</code>、<code>MachineDeployment</code> 等 CRD 对象都存在于此。</li>
<li><strong>运行机制</strong>：管理集群中的 Cluster API 控制器会监听这些 CRD 对象的变化，并根据其声明的状态来驱动 Workload 集群的创建、更新和删除。</li>
</ul>
</li>
<li><p><strong>Workload 集群 (Workload Cluster)</strong></p>
<ul>
<li><strong>核心职责</strong>：这是<strong>最终用户用来部署和运行其应用程序的 Kubernetes 集群</strong>。</li>
<li><strong>关键特征</strong>：Workload 集群的整个生命周期，从基础设施的准备、Kubernetes 组件的安装与配置，到节点的加入和集群的伸缩，都<strong>由管理集群通过 Cluster API 的声明式 API 进行管理</strong>。</li>
<li><strong>用户视角</strong>：对于最终用户而言，Workload 集群就是一个标准的 Kubernetes 集群，他们可以直接使用 <code>kubectl</code> 与之交互，部署应用。</li>
</ul>
</li>
<li><p><strong>Infrastructure Provider (基础设施提供者)</strong></p>
<ul>
<li><strong>核心职责</strong>：<strong>负责与特定的底层基础设施（如 AWS, Azure, GCP, vSphere, Bare Metal 等）进行交互，以管理 Workload 集群所需的计算、网络和存储资源</strong>。</li>
<li><strong>关键特征</strong>：每个 Infrastructure Provider 都实现了一套 Cluster API 定义的接口，将 Cluster API 中通用的声明（如“我需要一个拥有 3 个节点的集群”）转化为特定云平台的 API 调用（如在 AWS 创建 3 个 EC2 实例，配置 VPC、安全组等）。</li>
<li><strong>实现机制</strong>：通常以一组自定义控制器和 CRD 的形式存在，例如 <code>AWSMachine</code>、<code>AzureCluster</code> 等。当一个通用的 <code>Machine</code> 对象被创建时，对应的 Infrastructure Provider 控制器会监听到，并创建其特定平台的资源。</li>
<li><strong>例如</strong>：<code>cluster-api-provider-aws</code> (CAPA), <code>cluster-api-provider-azure</code> (CAPZ)。</li>
</ul>
</li>
<li><p><strong>Bootstrap Provider (引导提供者)</strong></p>
<ul>
<li><strong>核心职责</strong>：<strong>负责在新创建的基础设施节点上进行 Kubernetes 的初始化和引导工作</strong>。</li>
<li><strong>关键特征</strong>：一旦 Infrastructure Provider 创建了计算节点（如虚拟机），Bootstrap Provider 就接管后续工作。</li>
<li><strong>具体任务包括</strong>：<ul>
<li><strong>证书生成</strong>：为新的 Kubernetes 集群生成所需的 PKI 证书和密钥。</li>
<li><strong>控制面组件安装和初始化</strong>：在指定的节点上安装和配置 Kubernetes 控制平面组件（如 etcd, kube-apiserver, kube-controller-manager, kube-scheduler）。这通常通过云初始化脚本（cloud-init）或类似机制，结合 <code>kubeadm</code> 来完成。</li>
<li><strong>节点加入</strong>：生成 <code>kubeadm join</code> 命令或配置，使新的控制平面节点和工作节点能够正确加入到 Workload 集群中。</li>
</ul>
</li>
<li><strong>常用实现</strong>：最常见的 Bootstrap Provider 是基于 <code>kubeadm</code> 的 (<code>CABPK</code> - Cluster API Bootstrap Provider Kubeadm)。</li>
</ul>
</li>
<li><p><strong>Control Plane Provider (控制平面提供者)</strong></p>
<ul>
<li><strong>核心职责</strong>：<strong>负责管理 Workload 集群控制平面的生命周期和配置</strong>。</li>
<li><strong>关键特征</strong>：它与 Bootstrap Provider 紧密协作，但更侧重于控制平面作为一个整体的编排和升级。</li>
<li><strong>例如</strong>：<code>KubeadmControlPlaneProvider</code> (KCP) 是一个常见的实现，它使用 <code>kubeadm</code> 来管理控制平面节点的集合，支持声明式的控制平面升级、伸缩等操作。它通常会创建一组 <code>Machine</code> 对象来代表控制平面节点。</li>
</ul>
</li>
</ol>
<h3 id="Cluster-API-核心模型-CRDs"><a href="#Cluster-API-核心模型-CRDs" class="headerlink" title="Cluster API 核心模型 (CRDs)"></a>Cluster API 核心模型 (CRDs)</h3><p>Cluster API 通过一系列自定义资源定义 (CRDs) 来描述和管理 Kubernetes 集群。这些模型是声明式 API 的基础。</p>
<ol>
<li><p><strong>Machine</strong></p>
<ul>
<li><strong>定义</strong>：<strong><code>Machine</code> 是 Cluster API 中的一个核心 CRD，它代表一个计算资源单元，通常是一个虚拟机或物理机</strong>。这个计算资源将被用来运行 Kubernetes 组件，并最终成为 Workload 集群中的一个 Kubernetes <code>Node</code>。</li>
<li><strong>与 Kubernetes <code>Node</code> 的差异</strong>：<ul>
<li><strong><code>Machine</code> 是 Cluster API 的抽象，存在于管理集群中，描述的是基础设施层面的“机器”或“实例”的期望状态</strong>。</li>
<li><strong>Kubernetes <code>Node</code> 是 Kubernetes 内部的逻辑概念，存在于 Workload 集群中，代表一个已经成功加入集群并准备好运行 Pod 的工作单元</strong>。</li>
<li>一个 <code>Machine</code> 对象最终会通过 Infrastructure Provider 和 Bootstrap Provider 的协作，在 Workload 集群中具化为一个 <code>Node</code> 对象。</li>
</ul>
</li>
<li><strong>生命周期</strong>：<ul>
<li><strong>创建</strong>：当一个新的 <code>Machine</code> 对象被创建时，Cluster API 的相关控制器（通常是 Infrastructure Provider 的控制器）会与底层基础设施（如 AWS）交互，<strong>创建一个实际的计算实例（如 EC2 实例）</strong>。随后，Bootstrap Provider 会在该实例上<strong>安装操作系统和 Kubernetes 组件，并使其加入 Workload 集群</strong>。<code>Machine</code> 对象的状态会随之更新以反映其实际状态。</li>
<li><strong>删除</strong>：当一个 <code>Machine</code> 对象被删除时，对应的控制器会<strong>从 Workload 集群中优雅地移除对应的 <code>Node</code></strong>（驱逐 Pod），然后<strong>调用 Infrastructure Provider 回收底层的计算资源</strong>。</li>
<li><strong>更新与不可变性 (Machine Immutability)</strong>：<ul>
<li><strong>核心理念</strong>：Cluster API 强烈推荐并默认采用<strong>不可变基础设施 (Immutable Infrastructure)</strong> 的模式来管理 <code>Machine</code>。</li>
<li><strong>In-place Upgrade vs. Replace</strong>：当 <code>Machine</code> 的某些属性需要更新时（例如，指定新的 Kubernetes 版本、更改实例类型或 AMI），<strong>Cluster API 不会尝试在现有机器上进行“原地升级”(In-place Upgrade)，而是采用“替换”(Replace) 策略</strong>。</li>
<li><strong>替换流程</strong>：一个新的 <code>Machine</code> 实例会根据新的配置被创建出来。一旦新 <code>Machine</code> 准备就绪并加入集群，旧的 <code>Machine</code> 实例才会被删除。</li>
<li><strong>优势</strong>：这种不可变性大大<strong>简化了升级和回滚的复杂性，提高了系统的可预测性和可靠性，避免了因原地修改引入的配置漂移或潜在的失败状态</strong>。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>MachineDeployment</strong></p>
<ul>
<li><strong>定义</strong>：<strong><code>MachineDeployment</code> 类似于 Kubernetes 原生的 <code>Deployment</code> 对象，但它管理的是 <code>MachineSet</code> 和 <code>Machine</code> 的集合</strong>。</li>
<li><strong>功能</strong>：它为一组相同的 <code>Machine</code> (通常是 Workload 集群的工作节点) <strong>提供了声明式的更新和管理能力</strong>。你可以定义期望的 <code>Machine</code> 模板、副本数量以及更新策略（如滚动更新 <code>RollingUpdate</code>）。</li>
<li><strong>例如</strong>：当你想升级 Workload 集群工作节点的 Kubernetes 版本时，你会修改 <code>MachineDeployment</code> 中定义的 <code>Machine</code> 模板（如更新 <code>version</code> 字段）。<code>MachineDeployment</code> 控制器会按照指定的策略（例如，一次替换一个节点）逐步创建新的 <code>Machine</code> (使用新版本) 并删除旧的 <code>Machine</code>。</li>
</ul>
</li>
<li><p><strong>MachineSet</strong></p>
<ul>
<li><strong>定义</strong>：<strong><code>MachineSet</code> 类似于 Kubernetes 原生的 <code>ReplicaSet</code> 对象，但它管理的是 <code>Machine</code></strong>。</li>
<li><strong>功能</strong>：它的主要目标是<strong>确保在任何给定时间都有指定数量的 <code>Machine</code> 副本在运行</strong>。如果实际运行的 <code>Machine</code> 数量少于期望数量，<code>MachineSet</code> 控制器会根据其模板创建新的 <code>Machine</code>；如果数量过多，则会删除多余的 <code>Machine</code>。</li>
<li><strong>关系</strong>：<code>MachineDeployment</code> 通过管理 <code>MachineSet</code> 来实现复杂的部署和更新策略。通常用户直接操作 <code>MachineDeployment</code>，而不是 <code>MachineSet</code>。</li>
</ul>
</li>
<li><p><strong>MachineHealthCheck</strong></p>
<ul>
<li><strong>定义</strong>：<code>MachineHealthCheck</code> 是一种机制，用于<strong>监控 <code>Machine</code> (及其对应的 Kubernetes <code>Node</code>) 的健康状况</strong>。</li>
<li><strong>功能</strong>：用户可以<strong>定义一组条件，当 <code>Machine</code> 或 <code>Node</code> 满足这些条件时，该 <code>Machine</code> 就会被标记为不健康</strong>。例如，<code>Node</code> 状态长时间处于 <code>NotReady</code>。</li>
<li><strong>自动修复</strong>：当 <code>MachineHealthCheck</code> 控制器检测到一个 <code>Machine</code> 不健康时，它可以触发自动修复流程。最常见的修复动作是<strong>删除这个不健康的 <code>Machine</code> 对象。随后，拥有该 <code>Machine</code> 的 <code>MachineSet</code> (或由 <code>MachineDeployment</code> 管理的 <code>MachineSet</code>) 会检测到副本数量不足，并自动创建一个新的、健康的 <code>Machine</code> 来替代它</strong>，从而实现节点的自愈。</li>
</ul>
</li>
</ol>
<p>通过这些角色和模型的协同工作，Cluster API 提供了一个强大且灵活的框架，用于以声明式、Kubernetes 原生的方式管理 Kubernetes 集群的整个生命周期，极大地简化了多集群管理和自动化运维的复杂性。</p>
<h3 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h3><h4 id="Create-host-cluster"><a href="#Create-host-cluster" class="headerlink" title="Create host cluster"></a>Create host cluster</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">./create_cluster.sh<br></code></pre></td></tr></table></figure>

<h4 id="Generate-cluster-specs"><a href="#Generate-cluster-specs" class="headerlink" title="Generate cluster specs"></a>Generate cluster specs</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-built_in">cd</span> cluster-api<br>./init_docker_provider.sh<br>./generate_workload_cluster.sh<br>kubectl apply -f capi-quickstart.yaml<br></code></pre></td></tr></table></figure>

<h4 id="Check"><a href="#Check" class="headerlink" title="Check"></a>Check</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs sh">docker ps|grep control-plane<br><br>b107b11771e5        kindest/haproxy:v20210715-a6da3463   <span class="hljs-string">&quot;haproxy -sf 7 -W -d…&quot;</span>   4 minutes ago       Up 4 minutes        40295/tcp, 0.0.0.0:40295-&gt;6443/tcp     capi-quickstart-lb<br>clusterctl get kubeconfig capi-quickstart &gt; capi-quickstart.kubeconfig<br>kubectl get no --kubeconfig capi-quickstart.kubeconfig --server https://127.0.0.1:40295<br>NAME                                    STATUS     ROLES                  AGE     VERSION<br>capi-quickstart-control-plane-6slwd     NotReady   control-plane,master   4m19s   v1.22.0<br>capi-quickstart-md-0-765cf784c5-6klwr   NotReady   &lt;none&gt;                 3m41s   v1.22.0<br></code></pre></td></tr></table></figure>

<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sh">kubectl get cluster<br>kubectl get machineset<br></code></pre></td></tr></table></figure>

<img src="/2025/04/24/kubernetes%E7%94%9F%E4%BA%A7%E5%8C%96%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/image-20250509150422859.png" srcset="/img/loading.gif" lazyload class="" title="image-20250509150422859">

<img src="/2025/04/24/kubernetes%E7%94%9F%E4%BA%A7%E5%8C%96%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/image-20250509150436143.png" srcset="/img/loading.gif" lazyload class="" title="image-20250509150436143">

<h2 id="8-Cluster-Autoscaler-CA"><a href="#8-Cluster-Autoscaler-CA" class="headerlink" title="8. Cluster Autoscaler (CA)"></a>8. Cluster Autoscaler (CA)</h2><p><strong>Cluster Autoscaler (CA)</strong> 是 Kubernetes 中用于 <strong>自动调整集群节点数量</strong> 的核心组件。</p>
<ul>
<li><p><strong>解决了什么问题</strong>:</p>
<ul>
<li><strong>资源浪费</strong>: 静态配置的集群规模如果过大，在负载较低时会造成大量节点资源闲置，增加成本。</li>
<li><strong>资源不足</strong>: 如果集群规模过小，当应用负载突增或部署新应用时，可能因资源不足导致 Pod 处于 <code>Pending</code> 状态无法调度，影响业务。</li>
<li><strong>手动调整效率低且易错</strong>: 手动监控集群负载并增删节点费时费力，且响应不及时。</li>
</ul>
</li>
<li><p><strong>是什么</strong>: 一个运行在集群内的独立程序（通常是一个 Deployment），它监控集群状态，并根据需要与 <strong>云提供商（Cloud Provider）</strong> 的 API（如 AWS Auto Scaling Groups, GCP Managed Instance Groups, Azure Virtual Machine Scale Sets）交互，自动增加或减少集群中的工作节点数量。</p>
</li>
<li><p><strong>原理</strong>:</p>
<ul>
<li><strong>监控</strong>: CA 定期（通过 <code>--scan-interval</code> 参数配置，默认 10 秒）扫描集群中处于 <code>Pending</code> 状态的 Pod。它检查这些 Pod 无法被调度的原因是否是 <strong>资源不足</strong>（CPU、内存、GPU 或其他自定义资源）。同时，它也监控 <strong>节点利用率</strong>。</li>
<li><strong>Scale-Up (扩容)</strong>: 如果发现有 Pod 因资源不足而 <code>Pending</code>，并且 CA <strong>模拟</strong> 发现如果增加一个来自某个 <strong>节点组（Node Group）</strong> 的新节点就能让这些 Pod 成功调度，CA 就会向关联的云提供商发出请求，要求增加该节点组的实例数量（通常是增加 1 个）。CA 会选择能够满足最多 <code>Pending</code> Pod 需求的节点组进行扩容。</li>
<li><strong>Scale-Down (缩容)</strong>: CA 会检查哪些节点 <strong>持续一段时间</strong>（通过 <code>--scale-down-delay-after-add</code>, <code>--scale-down-unneeded-time</code> 等参数配置）处于 <strong>低利用率</strong> 状态（CPU 和内存请求低于某个阈值，通过 <code>--scale-down-utilization-threshold</code> 配置，默认 0.5 即 50%）。对于满足缩容条件的节点，CA 会 <strong>模拟</strong> 如果将该节点上的所有 Pod（除了系统关键 Pod 和配置了不能驱逐的 Pod）<strong>重新调度</strong> 到其他节点上是否可行。这个模拟过程会严格遵守 <strong>PodDisruptionBudgets (PDBs)</strong>、<strong>亲和性&#x2F;反亲和性规则</strong>、<strong>节点选择器 (Node Selectors)</strong>、<strong>污点和容忍 (Taints and Tolerations)</strong> 等调度约束。如果模拟成功，CA 会首先 <strong>驱逐 (Drain)</strong> 该节点上的 Pod（将 Pod 安全地迁移到其他节点），然后向云提供商发出请求，要求 <strong>终止 (Terminate)</strong> 该节点实例并将其从节点组中移除。</li>
<li><strong>节点组 (Node Groups)</strong>: CA 的操作是基于节点组的。你需要为 CA 配置集群中可自动伸缩的节点组，通常对应云提供商的 Auto Scaling Group 或 VM Scale Set。每个节点组需要定义 <strong>最小节点数 (<code>minSize</code>)</strong> 和 <strong>最大节点数 (<code>maxSize</code>)</strong>。CA 的伸缩活动会被限制在这个范围内。</li>
</ul>
</li>
<li><p><strong>关键配置</strong>:</p>
<ul>
<li><strong>部署</strong>: 通常以 Deployment 方式部署在 <code>kube-system</code> 命名空间。</li>
<li><strong>云提供商集成</strong>: 需要为 CA Pod 配置访问云提供商 API 的权限（例如通过 IAM Role for Service Accounts on AWS, Workload Identity on GCP）。</li>
<li><strong>启动参数</strong>: 通过 ConfigMap 或命令行参数配置，关键参数包括：<ul>
<li><code>--cloud-provider</code>: 指定云提供商 (e.g., <code>aws</code>, <code>gce</code>, <code>azure</code>)。</li>
<li><code>--nodes=&lt;min&gt;:&lt;max&gt;:&lt;asg_name&gt;</code>: 定义节点组及其规模限制 (可指定多个)。</li>
<li><code>--scan-interval</code>: 扫描周期。</li>
<li><code>--scale-down-enabled</code>: 是否启用缩容 (默认 <code>true</code>)。</li>
<li><code>--scale-down-utilization-threshold</code>: 节点利用率低于此阈值才可能被缩容。</li>
<li><code>--scale-down-unneeded-time</code>: 节点需要持续低于阈值多长时间才会被考虑缩容。</li>
<li><code>--skip-nodes-with-local-storage</code>: 不缩容包含本地存储 Pod 的节点 (防止数据丢失)。</li>
<li><code>--skip-nodes-with-system-pods</code>: 不缩容包含 <code>kube-system</code> 命名空间下非 DaemonSet Pod 的节点。</li>
</ul>
</li>
<li><strong>示例 (概念性启动命令)</strong>:<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash">./cluster-autoscaler \<br>  --v=4 \<br>  --stderrthreshold=info \<br>  --cloud-provider=aws \<br>  --skip-nodes-with-local-storage=<span class="hljs-literal">false</span> \<br>  --expander=least-waste \<br>  --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/my-cluster-name \<br>  --balance-similar-node-groups \<br>  --skip-nodes-with-system-pods=<span class="hljs-literal">false</span> \<br>  --scale-down-utilization-threshold=0.5 \<br>  --scale-down-unneeded-time=10m \<br>  --scale-down-delay-after-add=10m<br></code></pre></td></tr></table></figure>
<ul>
<li><code>--node-group-auto-discovery</code>: 使用 AWS Tag 自动发现 ASG。</li>
<li><code>--expander</code>: 选择扩容策略（如 <code>least-waste</code> 优先选择浪费资源最少的节点组）。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Golang 源码视角</strong>: Cluster Autoscaler 的核心逻辑在 <code>cluster-autoscaler/core</code> 包中。<code>static_autoscaler.go</code> 文件中的 <code>RunOnce</code> 函数是主循环，它调用 <code>ScaleUp</code> 和 <code>ScaleDown</code> 函数。<code>ScaleUp</code> 会检查 <code>unschedulablePods</code>，并使用 <code>provisioning_simulator.go</code> 中的逻辑来模拟节点添加。<code>ScaleDown</code> 则会识别 <code>candidates</code> (可能被缩容的节点)，并通过 <code>drain.go</code> 中的函数来执行驱逐，最后与具体的 <code>cloudprovider</code> 接口交互来删除节点。它大量使用了 Kubernetes <code>client-go</code> 库来与 API Server 交互，获取 Pods、Nodes 等资源的状态，并依赖 <code>informers</code> 来高效地监听资源变化。</p>
</li>
</ul>
<h2 id="9-集群管理实践案例分析"><a href="#9-集群管理实践案例分析" class="headerlink" title="9. 集群管理实践案例分析"></a>9. 集群管理实践案例分析</h2><p>从提供的 PDF 片段（如 CI&#x2F;CD 流程图、SaltStack 相关图示、CRD 示例 <code>Cluster</code>, <code>ComputeNode</code>, <code>ClusterDeployment</code>）来看，这描绘了一个 <strong>自动化、声明式的集群生命周期管理</strong> 实践方案。这种方案通常结合了 <strong>GitOps</strong>、<strong>Operator Pattern</strong> 和 <strong>配置管理工具</strong>。</p>
<ul>
<li><strong>解决了什么问题</strong>: 手动创建、升级和管理 Kubernetes 集群（尤其是大规模集群）既复杂又容易出错。需要一个标准化、可重复、可审计的流程来管理从集群创建、节点配置、版本升级到最终退役的整个生命周期。</li>
<li><strong>是什么</strong>: 这是一套集成的工具链和工作流，旨在实现 Kubernetes 集群的自动化管理。</li>
<li><strong>核心组件与流程</strong>:<ol>
<li><strong>Git Repository (Source of Truth)</strong>: 所有集群的 <strong>期望状态 (Desired State)</strong>，包括集群配置（版本、网络、区域等）、节点配置、控制平面参数、附加组件（如 CNI, CoreDNS, Ingress Controller）等，都以声明式文件（如 YAML）的形式存储在 Git 仓库中。</li>
<li><strong>CI&#x2F;CD Pipeline</strong>:<ul>
<li>当 Git 仓库中的配置发生变更（如 Pull Request 合并）时，触发 CI&#x2F;CD 流水线。</li>
<li><strong>CI (Continuous Integration)</strong>: 阶段包括代码风格检查、YAML 语法验证、配置策略检查（如使用 OPA Gatekeeper）、单元&#x2F;集成测试（如果涉及自定义代码）。可能会构建自定义的 Kubernetes 组件镜像或 Operator 镜像。</li>
<li><strong>CD (Continuous Deployment&#x2F;Delivery)</strong>: 负责将验证过的配置变更应用到目标环境。这通常不是直接调用 <code>kubectl apply</code> 或配置管理工具，而是更新代表集群状态的 <strong>Custom Resource (CR)</strong> 对象。</li>
</ul>
</li>
<li><strong>Kubernetes Operator &#x2F; Custom Controller</strong>:<ul>
<li>集群中运行着一个（或多个）自定义的 <strong>Operator</strong>。这个 Operator 负责 <strong>监听 (Watch)</strong> 特定类型的 CRD（如示例中的 <code>Cluster</code>, <code>ComputeNode</code>, <code>ClusterDeployment</code>）。</li>
<li><strong>Reconciliation Loop</strong>: 当 Operator 检测到其管理的 CR 对象发生变化（创建、更新、删除）时，会触发 <strong>调和循环 (Reconciliation Loop)</strong>。</li>
<li><strong>执行操作</strong>: 在调和循环中，Operator 读取 CR 中定义的期望状态，并与集群的 <strong>实际状态 (Actual State)</strong> 进行比较。如果两者不一致，Operator 会执行必要的操作来使实际状态趋向于期望状态。这些操作可能包括：<ul>
<li>调用 <strong>云提供商 API</strong> 来创建&#x2F;删除虚拟机、负载均衡器、网络等基础设施资源。</li>
<li>使用 <strong>配置管理工具 (如 SaltStack、Ansible)</strong> (如 <code>salt highstate</code> 命令所示) 来配置节点操作系统、安装 Docker&#x2F;Containerd、Kubelet、Kube-proxy 等。<code>salt-master</code> &#x2F; <code>salt-minion</code> 的架构表明使用 SaltStack 进行节点级别的配置管理。</li>
<li>执行 <code>kubeadm</code> 命令或调用 Kubernetes API 来初始化控制平面、加入节点、执行版本升级。</li>
<li>部署或更新集群附加组件。</li>
</ul>
</li>
<li><strong>CRD 定义</strong>:<ul>
<li><code>Cluster</code>: 可能定义了集群的元数据（名称、区域）、Kubernetes 版本、网络配置（CNI 类型、Pod&#x2F;Service CIDR）等。</li>
<li><code>ComputeNode</code>: 定义了单个工作节点或控制平面节点的规格（实例类型&#x2F;flavor、操作系统镜像、磁盘大小等）。</li>
<li><code>ClusterDeployment</code>: 可能用于管理 <strong>集群升级</strong> 过程，定义了目标版本、升级策略（如滚动升级的批次大小和百分比 <code>strategy: &quot;10,30,30,30&quot;</code> 表示分四批，分别升级 10%, 30%, 30%, 30% 的节点）、SaltStack 或其他部署工具所需资源的 URL (<code>saltTarUrl</code>, <code>serverTarUrl</code>) 等。</li>
</ul>
</li>
</ul>
</li>
<li><strong>管理工具</strong>:<ul>
<li><code>kubectl</code>: 标准的 Kubernetes 命令行工具，用于与 API Server 交互，查看 CR 状态、Pod 日志等。</li>
<li><code>tm-cli</code> (推测): 一个 <strong>自定义的命令行工具</strong>，可能封装了对上述 CRD 的操作（创建、获取、更新、删除 Cluster&#x2F;Node&#x2F;Deployment 等），提供更友好的用户接口，或者执行一些 Operator 无法覆盖的特定管理任务。</li>
<li><code>kube-up</code> &#x2F; <code>kube-down</code> (推测): 可能是用于集群 <strong>初始引导 (Bootstrap)</strong> 或 <strong>销毁 (Teardown)</strong> 的脚本或工具，在 Operator 接管之前或之后使用。</li>
</ul>
</li>
<li><strong>运维界面 (Operator Interface)</strong>: 可能提供了一个 Web UI 或 API，用于可视化集群状态、监控调和过程、触发特定操作等。</li>
</ol>
</li>
<li><strong>优势</strong>:<ul>
<li><strong>自动化</strong>: 大幅减少手动操作，提高效率，降低人为错误。</li>
<li><strong>声明式</strong>: 只需定义期望状态，具体实现由 Operator 负责，易于理解和管理。</li>
<li><strong>版本控制与审计</strong>: 所有变更记录在 Git 中，可追溯、可回滚。</li>
<li><strong>一致性</strong>: 确保所有集群都按照相同的标准和流程进行管理。</li>
<li><strong>可扩展性</strong>: 可以通过增加新的 CRD 和 Controller 来扩展管理能力。</li>
</ul>
</li>
<li><strong>Linux&#x2F;Golang 关联</strong>: Operator 通常使用 <strong>Go 语言</strong> 编写，利用 <strong><code>client-go</code></strong> 库与 Kubernetes API 交互，并使用 <strong><code>controller-runtime</code></strong> 框架来简化 Operator 的开发。配置管理工具（如 SaltStack 的 Minion）运行在 <strong>Linux 节点</strong> 上，执行系统级的配置命令。整个流程涉及 Linux 系统管理、网络配置、容器运行时（Docker&#x2F;Containerd）管理等底层操作。</li>
</ul>
<h2 id="10-多租户（Multi-Tenancy）集群管理"><a href="#10-多租户（Multi-Tenancy）集群管理" class="headerlink" title="10. 多租户（Multi-Tenancy）集群管理"></a>10. 多租户（Multi-Tenancy）集群管理</h2><p>在许多场景下，需要让多个不同的用户、团队或客户（即 <strong>租户 Tenants</strong>）共享同一个 Kubernetes 集群资源。<strong>多租户</strong> 的目标是在共享基础设施的同时，提供足够的 <strong>隔离性 (Isolation)</strong>、<strong>安全性 (Security)</strong> 和 <strong>资源公平性 (Fairness)</strong>。</p>
<ul>
<li><p><strong>解决了什么问题</strong>:</p>
<ul>
<li><strong>资源利用率</strong>: 共享集群通常比为每个租户单独部署集群更节省成本和资源。</li>
<li><strong>管理开销</strong>: 集中管理一个（或少量）大型集群比管理大量小型集群更高效。</li>
<li><strong>快速环境提供</strong>: 可以快速为新团队或项目分配隔离的环境。</li>
</ul>
</li>
<li><p><strong>是什么</strong>: 在单个 Kubernetes 集群内部署和运行属于不同租户的应用程序，同时确保它们之间互不干扰、资源使用受控、访问权限严格分离。</p>
</li>
<li><p><strong>核心机制与实践</strong>: Kubernetes 本身提供了一些构建块来实现（通常是 <strong>软性 Soft</strong>）多租户：</p>
<ul>
<li><p><strong>Namespaces</strong>:</p>
<ul>
<li><strong>提供逻辑隔离</strong>: Namespace 是 Kubernetes 中资源（如 Pods, Services, Deployments, Secrets）的基本作用域。不同 Namespace 中的资源名称可以重复。这是实现多租户的 <strong>第一道屏障</strong>。</li>
<li><strong>不提供安全隔离</strong>: <strong>关键点：Namespace 本身并不阻止跨 Namespace 的网络访问，也不限制资源使用</strong>。默认情况下，一个 Namespace 中的 Pod 可以访问其他 Namespace 中的 Service (如果知道其 DNS 名称或 IP)。</li>
<li><strong>配置示例</strong>:<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 创建两个租户的 Namespace</span><br>kubectl create namespace tenant-a<br>kubectl create namespace tenant-b<br></code></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p><strong>RBAC (Role-Based Access Control)</strong>:</p>
<ul>
<li><strong>提供访问权限控制</strong>: RBAC 用于精细化地控制 <strong>谁 (Subject: User, Group, ServiceAccount)</strong> 可以对 <strong>什么资源 (Resource: Pods, Services, Nodes)</strong> 执行 <strong>什么操作 (Verb: get, list, watch, create, update, patch, delete)</strong>。</li>
<li><strong>Namespace 作用域</strong>: 可以创建 <code>Role</code> 和 <code>RoleBinding</code>，其权限仅限于特定的 Namespace。这是限制租户用户只能管理自己 Namespace 资源的关键。</li>
<li><strong>Cluster 作用域</strong>: <code>ClusterRole</code> 和 <code>ClusterRoleBinding</code> 具有集群范围的权限，通常用于集群管理员或需要访问集群级别资源（如 Nodes, Namespaces）的组件。</li>
<li><strong>配置示例</strong>:<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># role-tenant-a-admin.yaml: 定义租户 A 管理员角色</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Role</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">tenant-a</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">tenant-a-admin</span><br><span class="hljs-attr">rules:</span><br><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> [<span class="hljs-string">&quot;&quot;</span>, <span class="hljs-string">&quot;apps&quot;</span>, <span class="hljs-string">&quot;extensions&quot;</span>, <span class="hljs-string">&quot;batch&quot;</span>, <span class="hljs-string">&quot;networking.k8s.io&quot;</span>] <span class="hljs-comment"># &quot;&quot; indicates core API group</span><br>  <span class="hljs-attr">resources:</span> [<span class="hljs-string">&quot;*&quot;</span>] <span class="hljs-comment"># Allow access to all resources within the namespace</span><br>  <span class="hljs-attr">verbs:</span> [<span class="hljs-string">&quot;*&quot;</span>] <span class="hljs-comment"># Allow all verbs</span><br><br><span class="hljs-meta">---</span><br><span class="hljs-comment"># rolebinding-tenant-a-admin.yaml: 将用户 &quot;user-a&quot; 绑定到租户 A 管理员角色</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">RoleBinding</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">tenant-a-admin-binding</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">tenant-a</span><br><span class="hljs-attr">subjects:</span><br><span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">User</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">user-a</span> <span class="hljs-comment"># Name is case sensitive</span><br>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span><br><span class="hljs-attr">roleRef:</span><br>  <span class="hljs-attr">kind:</span> <span class="hljs-string">Role</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">tenant-a-admin</span><br>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span><br></code></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">kubectl apply -f role-tenant-a-admin.yaml<br>kubectl apply -f rolebinding-tenant-a-admin.yaml<br></code></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p><strong>Network Policies</strong>:</p>
<ul>
<li><strong>提供网络隔离</strong>: NetworkPolicy 允许定义 Pod 之间以及 Pod 与外部网络端点之间的 <strong>网络访问规则</strong>。这是实现租户间网络隔离的 <strong>核心机制</strong>。</li>
<li><strong>依赖 CNI</strong>: 需要使用支持 NetworkPolicy 的 CNI 插件（如 Calico, Cilium, Weave Net）。</li>
<li><strong>默认策略</strong>: 最佳实践是首先应用一个 <strong>默认拒绝 (Default Deny)</strong> 策略到每个租户 Namespace，阻止所有入站 (Ingress) 和&#x2F;或出站 (Egress) 流量，然后根据需要显式地允许特定流量。</li>
<li><strong>Linux 内核关联</strong>: NetworkPolicy 的实现通常依赖于 Linux 内核的网络过滤机制，如 <strong>iptables</strong> 或更现代的 <strong>eBPF (Extended Berkeley Packet Filter)</strong>。CNI 插件会监听 NetworkPolicy 对象，并将规则转换为相应的 iptables 规则链或 eBPF 程序加载到内核中，从而在数据包路径上进行过滤。</li>
<li><strong>配置示例</strong>:<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># netpol-default-deny-ingress.yaml: 拒绝所有进入 tenant-a 的流量</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">networking.k8s.io/v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">NetworkPolicy</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">default-deny-ingress</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">tenant-a</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">podSelector:</span> &#123;&#125; <span class="hljs-comment"># An empty podSelector selects all pods in the namespace</span><br>  <span class="hljs-attr">policyTypes:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">Ingress</span> <span class="hljs-comment"># Apply policy to Ingress traffic</span><br><span class="hljs-meta">---</span><br><span class="hljs-comment"># netpol-allow-nginx-ingress.yaml: 允许来自特定 namespace (e.g., monitoring) 的流量访问 nginx Pod</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">networking.k8s.io/v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">NetworkPolicy</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">allow-nginx-ingress</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">tenant-a</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">podSelector:</span><br>    <span class="hljs-attr">matchLabels:</span><br>      <span class="hljs-attr">app:</span> <span class="hljs-string">nginx</span><br>  <span class="hljs-attr">policyTypes:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">Ingress</span><br>  <span class="hljs-attr">ingress:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">from:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-attr">namespaceSelector:</span><br>        <span class="hljs-attr">matchLabels:</span><br>          <span class="hljs-attr">team:</span> <span class="hljs-string">monitoring</span> <span class="hljs-comment"># Allow from pods in namespaces labeled &#x27;team=monitoring&#x27;</span><br>    <span class="hljs-attr">ports:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span><br>      <span class="hljs-attr">port:</span> <span class="hljs-number">80</span><br></code></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">kubectl apply -f netpol-default-deny-ingress.yaml -n tenant-a<br>kubectl apply -f netpol-allow-nginx-ingress.yaml -n tenant-a<br></code></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p><strong>Resource Quotas</strong>:</p>
<ul>
<li><strong>提供资源总量限制</strong>: ResourceQuota 用于限制一个 Namespace <strong>能够消耗的资源总量</strong>，包括计算资源（CPU 请求&#x2F;限制, 内存 请求&#x2F;限制）、存储资源（持久卷声明数量, 总存储容量）以及对象数量（Pods, Services, Secrets, ConfigMaps 等）。</li>
<li><strong>防止资源滥用</strong>: 确保单个租户不会耗尽整个集群的资源，影响其他租户。</li>
<li><strong>配置示例</strong>:<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># quota-tenant-a.yaml: 限制 tenant-a 的资源</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">ResourceQuota</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">tenant-a-quota</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">tenant-a</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">hard:</span><br>    <span class="hljs-attr">requests.cpu:</span> <span class="hljs-string">&quot;10&quot;</span>       <span class="hljs-comment"># Total requested CPU across all pods cannot exceed 10 cores</span><br>    <span class="hljs-attr">requests.memory:</span> <span class="hljs-string">20Gi</span>    <span class="hljs-comment"># Total requested Memory cannot exceed 20 GiB</span><br>    <span class="hljs-attr">limits.cpu:</span> <span class="hljs-string">&quot;20&quot;</span>         <span class="hljs-comment"># Total limited CPU cannot exceed 20 cores</span><br>    <span class="hljs-attr">limits.memory:</span> <span class="hljs-string">40Gi</span>      <span class="hljs-comment"># Total limited Memory cannot exceed 40 GiB</span><br>    <span class="hljs-attr">pods:</span> <span class="hljs-string">&quot;50&quot;</span>               <span class="hljs-comment"># Maximum number of pods</span><br>    <span class="hljs-attr">services:</span> <span class="hljs-string">&quot;20&quot;</span>           <span class="hljs-comment"># Maximum number of services</span><br>    <span class="hljs-attr">persistentvolumeclaims:</span> <span class="hljs-string">&quot;10&quot;</span> <span class="hljs-comment"># Maximum number of PVCs</span><br>    <span class="hljs-attr">requests.storage:</span> <span class="hljs-string">&quot;100Gi&quot;</span> <span class="hljs-comment"># Maximum total requested storage by PVCs</span><br></code></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">kubectl apply -f quota-tenant-a.yaml -n tenant-a<br></code></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p><strong>Limit Ranges</strong>:</p>
<ul>
<li><strong>提供容器级别的资源约束</strong>: LimitRange 用于为一个 Namespace 中的 <strong>每个 Pod 或 Container</strong> 设置默认、最小或最大的资源请求（requests）和限制（limits）。</li>
<li><strong>规范资源使用</strong>: 确保 Pod 不会请求过大或过小的资源，同时强制所有 Pod 都设置资源请求&#x2F;限制（这是 ResourceQuota 正常工作的前提）。</li>
<li><strong>配置示例</strong>:<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># limits-tenant-a.yaml: 设置 tenant-a 中容器的默认和最大资源</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">LimitRange</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">tenant-a-limits</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">tenant-a</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">limits:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">type:</span> <span class="hljs-string">Container</span><br>    <span class="hljs-attr">max:</span> <span class="hljs-comment"># Maximum limits per container</span><br>      <span class="hljs-attr">cpu:</span> <span class="hljs-string">&quot;2&quot;</span><br>      <span class="hljs-attr">memory:</span> <span class="hljs-string">&quot;4Gi&quot;</span><br>    <span class="hljs-attr">min:</span> <span class="hljs-comment"># Minimum requests per container</span><br>      <span class="hljs-attr">cpu:</span> <span class="hljs-string">&quot;100m&quot;</span><br>      <span class="hljs-attr">memory:</span> <span class="hljs-string">&quot;100Mi&quot;</span><br>    <span class="hljs-attr">default:</span> <span class="hljs-comment"># Default limits if not specified by container</span><br>      <span class="hljs-attr">cpu:</span> <span class="hljs-string">&quot;500m&quot;</span><br>      <span class="hljs-attr">memory:</span> <span class="hljs-string">&quot;1Gi&quot;</span><br>    <span class="hljs-attr">defaultRequest:</span> <span class="hljs-comment"># Default requests if not specified by container</span><br>      <span class="hljs-attr">cpu:</span> <span class="hljs-string">&quot;200m&quot;</span><br>      <span class="hljs-attr">memory:</span> <span class="hljs-string">&quot;256Mi&quot;</span><br></code></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">kubectl apply -f limits-tenant-a.yaml -n tenant-a<br></code></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p><strong>Pod Security Admission (PSA) &#x2F; Pod Security Policies (PSP - Deprecated)</strong>:</p>
<ul>
<li><strong>提供运行时安全控制</strong>: 限制 Pod 的安全相关行为，如是否能以 root 用户运行、是否能访问宿主机文件系统、是否能使用特权模式、允许加载哪些内核能力 (Capabilities) 等。PSA 是内置的 Admission Controller，提供了 <code>privileged</code>, <code>baseline</code>, <code>restricted</code> 三种策略级别，可以通过 Namespace Label 来强制实施。</li>
<li><strong>增强安全性</strong>: 防止租户 Pod 破坏节点或其他租户的 Pod。</li>
</ul>
</li>
<li><p><strong>Admission Controllers (尤其是 Validating&#x2F;Mutating Webhooks)</strong>:</p>
<ul>
<li><strong>提供自定义策略实施</strong>: 可以部署自定义的 Admission Webhook（通常结合 OPA Gatekeeper 或 Kyverno）来实施更复杂的、组织特定的策略，例如强制所有资源打上特定标签、禁止使用 <code>latest</code> 镜像标签、限制可以使用的 Ingress Hostname 等。</li>
</ul>
</li>
<li><p><strong>(高级) 运行时隔离</strong>: 使用如 <strong>gVisor</strong> 或 <strong>Kata Containers</strong> 等沙箱技术，为 Pod 提供更强的内核级隔离，但会带来一定的性能开销。</p>
</li>
</ul>
</li>
<li><p><strong>挑战</strong>:</p>
<ul>
<li><strong>复杂性</strong>: 正确配置所有隔离机制需要深入的 Kubernetes 知识。</li>
<li><strong>控制平面共享</strong>: 所有租户共享 API Server, etcd 等控制平面组件，一个租户的恶意或错误行为（如 API Flood）可能影响整个集群（需要 API 优先级和公平性 APF）。</li>
<li><strong>内核共享</strong>: 容器共享宿主机内核，内核漏洞可能导致隔离被破坏（除非使用沙箱技术）。</li>
<li><strong>“吵闹的邻居”问题</strong>: 即使有 Quota，一个租户的网络流量、磁盘 I&#x2F;O 也可能间接影响其他租户的性能。</li>
</ul>
</li>
<li><p><strong>设计模式</strong>:</p>
<ul>
<li><strong>Namespace as a Service</strong>: 最常见的多租户模式，使用上述 K8s 原生机制在共享集群中隔离租户。适用于信任度较高或对隔离性要求不是极高的场景。</li>
<li><strong>Cluster as a Service</strong>: 为每个租户提供独立的 Kubernetes 集群。隔离性最好，但成本和管理开销最高。</li>
<li><strong>Control Plane as a Service</strong>: 多个租户共享基础设施（节点），但拥有独立的控制平面（如使用 Virtual Kubelet 或 vCluster 等技术）。在隔离性和成本之间取得平衡。</li>
</ul>
</li>
</ul>
<p>多租户是一个复杂的主题，没有一刀切的解决方案。选择哪种模式和哪些隔离机制取决于具体的安全要求、信任模型、成本预算和管理能力。</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/kubernetes/" class="print-no-link">#kubernetes</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>kubernetes生产化集群管理</div>
      <div>https://mfzzf.github.io/2025/04/24/kubernetes生产化集群管理/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Mzzf</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年4月24日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/04/27/kubernetes%E7%94%9F%E4%BA%A7%E5%8C%96%E8%BF%90%E7%BB%B4/" title="kubernetes生产化运维">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">kubernetes生产化运维</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/04/22/Ingress/" title="Kubernetes 网络服务暴露：从 Service 到 Ingress 与 BGP/DSR">
                        <span class="hidden-mobile">Kubernetes 网络服务暴露：从 Service 到 Ingress 与 BGP/DSR</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
